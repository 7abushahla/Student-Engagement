{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 00:10:10.179283: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-22 00:10:10.471870: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-22 00:10:10.471920: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-22 00:10:10.472916: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-22 00:10:10.604486: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.applications import MobileNetV2, MobileNetV3Small, MobileNet\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Input, Activation\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Directory Setup\n",
    "os.makedirs('./models/', exist_ok=True)\n",
    "os.makedirs('./plots/loss/', exist_ok=True)\n",
    "os.makedirs('./plots/accuracy/', exist_ok=True)\n",
    "os.makedirs('./logs/', exist_ok=True)  # For TensorBoard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name = \"MobileNet_E_New\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_dir = \"./Dataset\"\n",
    "img_width, img_height = 128, 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gaussian_noise(x, mean=0, std=0.1):\n",
    "    noise = np.random.normal(mean, std, x.shape)\n",
    "    noisy_x = x + noise\n",
    "    return np.clip(noisy_x, 0, 255)\n",
    "\n",
    "def apply_color_channel_changes(x, delta_hue=10, delta_saturation=0.2, delta_brightness=0.2):\n",
    "    x = np.uint8(x)  \n",
    "    hsv_x = cv2.cvtColor(x, cv2.COLOR_RGB2HSV)\n",
    "    \n",
    "    h, s, v = cv2.split(hsv_x)\n",
    "    h = (h + delta_hue) % 180\n",
    "    s = np.clip(s + delta_saturation, 0, 255)\n",
    "    v = np.clip(v + delta_brightness, 0, 255)\n",
    "    \n",
    "    h = h.astype(np.uint8)\n",
    "    s = s.astype(np.uint8)\n",
    "    v = v.astype(np.uint8)\n",
    "    \n",
    "    hsv_augmented = cv2.merge((h, s, v))\n",
    "    augmented_x = cv2.cvtColor(hsv_augmented, cv2.COLOR_HSV2RGB)\n",
    "    \n",
    "    return augmented_x\n",
    "\n",
    "def apply_random_crop(x, crop_size=(120, 120)):\n",
    "    h, w, _ = x.shape\n",
    "    \n",
    "    if h < crop_size[0] or w < crop_size[1]:\n",
    "        raise ValueError(\"Crop size is larger than image dimensions\")\n",
    "    \n",
    "    top = np.random.randint(0, h - crop_size[0])\n",
    "    left = np.random.randint(0, w - crop_size[1])\n",
    "    \n",
    "    cropped_x = x[top:top + crop_size[0], left:left + crop_size[1], :]\n",
    "    \n",
    "    resized_cropped_x = cv2.resize(cropped_x, (img_width, img_height))\n",
    "    \n",
    "    return resized_cropped_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataGenerator(ImageDataGenerator):\n",
    "    def standardize(self, x):\n",
    "        x = apply_gaussian_noise(x)\n",
    "        x = apply_color_channel_changes(x)\n",
    "        x = apply_random_crop(x)\n",
    "        x = tf.keras.applications.mobilenet.preprocess_input(x)\n",
    "        return x\n",
    "\n",
    "train_datagen = CustomImageDataGenerator(\n",
    "    channel_shift_range=0.1,\n",
    ")\n",
    "\n",
    "test_datagen = CustomImageDataGenerator()\n",
    "\n",
    "class_subdirs = os.listdir(root_data_dir)\n",
    "num_classes = len(class_subdirs)\n",
    "\n",
    "num_samples_per_class = [len(os.listdir(os.path.join(root_data_dir, class_name))) for class_name in class_subdirs]\n",
    "\n",
    "num_samples = sum(num_samples_per_class)\n",
    "fold_histories = []\n",
    "fold_f1_scores = []\n",
    "\n",
    "def create_image_dataframe(root_data_dir):\n",
    "    data = []\n",
    "    for class_name in os.listdir(root_data_dir):\n",
    "        class_dir = os.path.join(root_data_dir, class_name)\n",
    "        for image_name in os.listdir(class_dir):\n",
    "            data.append({'file_path': os.path.join(class_dir, image_name), 'label': class_name})\n",
    "    return pd.DataFrame(data)\n",
    "    \n",
    "image_dataframe = create_image_dataframe(root_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the Input layer\n",
    "input_layer = Input(shape=(img_width, img_height, 3))\n",
    "\n",
    "\n",
    "# 2. Load the pre-trained MobileNetV2 model without the top classification layers\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=input_layer, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True  # Make backbone trainable\n",
    "\n",
    "# 4. Add custom layers on top of the base_model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # Removed activation from here\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)  # Added Activation layer separately\n",
    "x = Dropout(0.65)(x)\n",
    "output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "\n",
    "\n",
    "# 5. Create the Functional API model\n",
    "full_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# 6. Compile the model with the SGD optimizer\n",
    "opt = SGD(learning_rate=0.01)\n",
    "full_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)              (None, 64, 64, 32)           864       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalizati  (None, 64, 64, 32)           128       ['Conv1[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)           (None, 64, 64, 32)           0         ['bn_Conv1[0][0]']            \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (D  (None, 64, 64, 32)           288       ['Conv1_relu[0][0]']          \n",
      " epthwiseConv2D)                                                                                  \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN  (None, 64, 64, 32)           128       ['expanded_conv_depthwise[0][0\n",
      "  (BatchNormalization)                                              ]']                           \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_re  (None, 64, 64, 32)           0         ['expanded_conv_depthwise_BN[0\n",
      " lu (ReLU)                                                          ][0]']                        \n",
      "                                                                                                  \n",
      " expanded_conv_project (Con  (None, 64, 64, 16)           512       ['expanded_conv_depthwise_relu\n",
      " v2D)                                                               [0][0]']                      \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (  (None, 64, 64, 16)           64        ['expanded_conv_project[0][0]'\n",
      " BatchNormalization)                                                ]                             \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)     (None, 64, 64, 96)           1536      ['expanded_conv_project_BN[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNo  (None, 64, 64, 96)           384       ['block_1_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)  (None, 64, 64, 96)           0         ['block_1_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D  (None, 65, 65, 96)           0         ['block_1_expand_relu[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_1_depthwise (Depthwi  (None, 32, 32, 96)           864       ['block_1_pad[0][0]']         \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (Batc  (None, 32, 32, 96)           384       ['block_1_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (Re  (None, 32, 32, 96)           0         ['block_1_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)    (None, 32, 32, 24)           2304      ['block_1_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchN  (None, 32, 32, 24)           96        ['block_1_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)     (None, 32, 32, 144)          3456      ['block_1_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNo  (None, 32, 32, 144)          576       ['block_2_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)  (None, 32, 32, 144)          0         ['block_2_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_depthwise (Depthwi  (None, 32, 32, 144)          1296      ['block_2_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (Batc  (None, 32, 32, 144)          576       ['block_2_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (Re  (None, 32, 32, 144)          0         ['block_2_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)    (None, 32, 32, 24)           3456      ['block_2_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchN  (None, 32, 32, 24)           96        ['block_2_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_2_add (Add)           (None, 32, 32, 24)           0         ['block_1_project_BN[0][0]',  \n",
      "                                                                     'block_2_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)     (None, 32, 32, 144)          3456      ['block_2_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNo  (None, 32, 32, 144)          576       ['block_3_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)  (None, 32, 32, 144)          0         ['block_3_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D  (None, 33, 33, 144)          0         ['block_3_expand_relu[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_3_depthwise (Depthwi  (None, 16, 16, 144)          1296      ['block_3_pad[0][0]']         \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (Batc  (None, 16, 16, 144)          576       ['block_3_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (Re  (None, 16, 16, 144)          0         ['block_3_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)    (None, 16, 16, 32)           4608      ['block_3_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchN  (None, 16, 16, 32)           128       ['block_3_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)     (None, 16, 16, 192)          6144      ['block_3_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNo  (None, 16, 16, 192)          768       ['block_4_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)  (None, 16, 16, 192)          0         ['block_4_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_depthwise (Depthwi  (None, 16, 16, 192)          1728      ['block_4_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (Batc  (None, 16, 16, 192)          768       ['block_4_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (Re  (None, 16, 16, 192)          0         ['block_4_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)    (None, 16, 16, 32)           6144      ['block_4_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchN  (None, 16, 16, 32)           128       ['block_4_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_4_add (Add)           (None, 16, 16, 32)           0         ['block_3_project_BN[0][0]',  \n",
      "                                                                     'block_4_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)     (None, 16, 16, 192)          6144      ['block_4_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNo  (None, 16, 16, 192)          768       ['block_5_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)  (None, 16, 16, 192)          0         ['block_5_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_depthwise (Depthwi  (None, 16, 16, 192)          1728      ['block_5_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (Batc  (None, 16, 16, 192)          768       ['block_5_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (Re  (None, 16, 16, 192)          0         ['block_5_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)    (None, 16, 16, 32)           6144      ['block_5_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchN  (None, 16, 16, 32)           128       ['block_5_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_5_add (Add)           (None, 16, 16, 32)           0         ['block_4_add[0][0]',         \n",
      "                                                                     'block_5_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)     (None, 16, 16, 192)          6144      ['block_5_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNo  (None, 16, 16, 192)          768       ['block_6_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)  (None, 16, 16, 192)          0         ['block_6_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D  (None, 17, 17, 192)          0         ['block_6_expand_relu[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_6_depthwise (Depthwi  (None, 8, 8, 192)            1728      ['block_6_pad[0][0]']         \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (Batc  (None, 8, 8, 192)            768       ['block_6_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (Re  (None, 8, 8, 192)            0         ['block_6_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)    (None, 8, 8, 64)             12288     ['block_6_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchN  (None, 8, 8, 64)             256       ['block_6_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)     (None, 8, 8, 384)            24576     ['block_6_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNo  (None, 8, 8, 384)            1536      ['block_7_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)  (None, 8, 8, 384)            0         ['block_7_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_depthwise (Depthwi  (None, 8, 8, 384)            3456      ['block_7_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (Batc  (None, 8, 8, 384)            1536      ['block_7_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (Re  (None, 8, 8, 384)            0         ['block_7_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)    (None, 8, 8, 64)             24576     ['block_7_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchN  (None, 8, 8, 64)             256       ['block_7_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_7_add (Add)           (None, 8, 8, 64)             0         ['block_6_project_BN[0][0]',  \n",
      "                                                                     'block_7_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)     (None, 8, 8, 384)            24576     ['block_7_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNo  (None, 8, 8, 384)            1536      ['block_8_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)  (None, 8, 8, 384)            0         ['block_8_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_depthwise (Depthwi  (None, 8, 8, 384)            3456      ['block_8_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (Batc  (None, 8, 8, 384)            1536      ['block_8_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (Re  (None, 8, 8, 384)            0         ['block_8_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)    (None, 8, 8, 64)             24576     ['block_8_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchN  (None, 8, 8, 64)             256       ['block_8_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_8_add (Add)           (None, 8, 8, 64)             0         ['block_7_add[0][0]',         \n",
      "                                                                     'block_8_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)     (None, 8, 8, 384)            24576     ['block_8_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNo  (None, 8, 8, 384)            1536      ['block_9_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)  (None, 8, 8, 384)            0         ['block_9_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_depthwise (Depthwi  (None, 8, 8, 384)            3456      ['block_9_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (Batc  (None, 8, 8, 384)            1536      ['block_9_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (Re  (None, 8, 8, 384)            0         ['block_9_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)    (None, 8, 8, 64)             24576     ['block_9_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchN  (None, 8, 8, 64)             256       ['block_9_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_9_add (Add)           (None, 8, 8, 64)             0         ['block_8_add[0][0]',         \n",
      "                                                                     'block_9_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)    (None, 8, 8, 384)            24576     ['block_9_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchN  (None, 8, 8, 384)            1536      ['block_10_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU  (None, 8, 8, 384)            0         ['block_10_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_10_depthwise (Depthw  (None, 8, 8, 384)            3456      ['block_10_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (Bat  (None, 8, 8, 384)            1536      ['block_10_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (R  (None, 8, 8, 384)            0         ['block_10_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)   (None, 8, 8, 96)             36864     ['block_10_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_10_project_BN (Batch  (None, 8, 8, 96)             384       ['block_10_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)    (None, 8, 8, 576)            55296     ['block_10_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchN  (None, 8, 8, 576)            2304      ['block_11_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU  (None, 8, 8, 576)            0         ['block_11_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_11_depthwise (Depthw  (None, 8, 8, 576)            5184      ['block_11_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (Bat  (None, 8, 8, 576)            2304      ['block_11_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (R  (None, 8, 8, 576)            0         ['block_11_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)   (None, 8, 8, 96)             55296     ['block_11_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_11_project_BN (Batch  (None, 8, 8, 96)             384       ['block_11_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_11_add (Add)          (None, 8, 8, 96)             0         ['block_10_project_BN[0][0]', \n",
      "                                                                     'block_11_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)    (None, 8, 8, 576)            55296     ['block_11_add[0][0]']        \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchN  (None, 8, 8, 576)            2304      ['block_12_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU  (None, 8, 8, 576)            0         ['block_12_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_12_depthwise (Depthw  (None, 8, 8, 576)            5184      ['block_12_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (Bat  (None, 8, 8, 576)            2304      ['block_12_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (R  (None, 8, 8, 576)            0         ['block_12_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)   (None, 8, 8, 96)             55296     ['block_12_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_12_project_BN (Batch  (None, 8, 8, 96)             384       ['block_12_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_12_add (Add)          (None, 8, 8, 96)             0         ['block_11_add[0][0]',        \n",
      "                                                                     'block_12_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)    (None, 8, 8, 576)            55296     ['block_12_add[0][0]']        \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchN  (None, 8, 8, 576)            2304      ['block_13_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU  (None, 8, 8, 576)            0         ['block_13_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2  (None, 9, 9, 576)            0         ['block_13_expand_relu[0][0]']\n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block_13_depthwise (Depthw  (None, 4, 4, 576)            5184      ['block_13_pad[0][0]']        \n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (Bat  (None, 4, 4, 576)            2304      ['block_13_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (R  (None, 4, 4, 576)            0         ['block_13_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)   (None, 4, 4, 160)            92160     ['block_13_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_13_project_BN (Batch  (None, 4, 4, 160)            640       ['block_13_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)    (None, 4, 4, 960)            153600    ['block_13_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchN  (None, 4, 4, 960)            3840      ['block_14_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU  (None, 4, 4, 960)            0         ['block_14_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_14_depthwise (Depthw  (None, 4, 4, 960)            8640      ['block_14_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (Bat  (None, 4, 4, 960)            3840      ['block_14_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (R  (None, 4, 4, 960)            0         ['block_14_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)   (None, 4, 4, 160)            153600    ['block_14_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_14_project_BN (Batch  (None, 4, 4, 160)            640       ['block_14_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_14_add (Add)          (None, 4, 4, 160)            0         ['block_13_project_BN[0][0]', \n",
      "                                                                     'block_14_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)    (None, 4, 4, 960)            153600    ['block_14_add[0][0]']        \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchN  (None, 4, 4, 960)            3840      ['block_15_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU  (None, 4, 4, 960)            0         ['block_15_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_15_depthwise (Depthw  (None, 4, 4, 960)            8640      ['block_15_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (Bat  (None, 4, 4, 960)            3840      ['block_15_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (R  (None, 4, 4, 960)            0         ['block_15_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)   (None, 4, 4, 160)            153600    ['block_15_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_15_project_BN (Batch  (None, 4, 4, 160)            640       ['block_15_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_15_add (Add)          (None, 4, 4, 160)            0         ['block_14_add[0][0]',        \n",
      "                                                                     'block_15_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)    (None, 4, 4, 960)            153600    ['block_15_add[0][0]']        \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchN  (None, 4, 4, 960)            3840      ['block_16_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU  (None, 4, 4, 960)            0         ['block_16_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_16_depthwise (Depthw  (None, 4, 4, 960)            8640      ['block_16_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (Bat  (None, 4, 4, 960)            3840      ['block_16_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (R  (None, 4, 4, 960)            0         ['block_16_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)   (None, 4, 4, 320)            307200    ['block_16_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_16_project_BN (Batch  (None, 4, 4, 320)            1280      ['block_16_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)             (None, 4, 4, 1280)           409600    ['block_16_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalizat  (None, 4, 4, 1280)           5120      ['Conv_1[0][0]']              \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " out_relu (ReLU)             (None, 4, 4, 1280)           0         ['Conv_1_bn[0][0]']           \n",
      "                                                                                                  \n",
      " global_average_pooling2d (  (None, 1280)                 0         ['out_relu[0][0]']            \n",
      " GlobalAveragePooling2D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 128)                  163968    ['global_average_pooling2d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 128)                  512       ['dense[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 128)                  0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 128)                  0         ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 3)                    387       ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2422851 (9.24 MB)\n",
      "Trainable params: 2388483 (9.11 MB)\n",
      "Non-trainable params: 34368 (134.25 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Optional: Display the model architecture\n",
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal model, no QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to track the best fold\n",
    "best_fold_idx = -1\n",
    "best_accuracy = -1\n",
    "best_test_data = None\n",
    "\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=SEED)\n",
    "fold_accuracies = []\n",
    "fold_f1_scores = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1/5\n",
      "Found 1578 validated image filenames belonging to 3 classes.\n",
      "Found 395 validated image filenames belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "48/49 [============================>.] - ETA: 0s - loss: 3.0482 - accuracy: 0.7077\n",
      "Epoch 1: val_accuracy improved from -inf to 0.57722, saving model to ./models/best_model_fold_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 17s 198ms/step - loss: 3.0467 - accuracy: 0.7070 - val_loss: 3.5961 - val_accuracy: 0.5772\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.6240 - accuracy: 0.8810\n",
      "Epoch 2: val_accuracy did not improve from 0.57722\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 2.6240 - accuracy: 0.8810 - val_loss: 3.5961 - val_accuracy: 0.5165\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4883 - accuracy: 0.9166\n",
      "Epoch 3: val_accuracy improved from 0.57722 to 0.59494, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 2.4883 - accuracy: 0.9166 - val_loss: 3.3445 - val_accuracy: 0.5949\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3834 - accuracy: 0.9314\n",
      "Epoch 4: val_accuracy improved from 0.59494 to 0.62532, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 2.3834 - accuracy: 0.9314 - val_loss: 3.2841 - val_accuracy: 0.6253\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3000 - accuracy: 0.9386\n",
      "Epoch 5: val_accuracy improved from 0.62532 to 0.64304, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 9s 179ms/step - loss: 2.3000 - accuracy: 0.9386 - val_loss: 3.1461 - val_accuracy: 0.6430\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2496 - accuracy: 0.9495\n",
      "Epoch 6: val_accuracy did not improve from 0.64304\n",
      "49/49 [==============================] - 8s 174ms/step - loss: 2.2496 - accuracy: 0.9495 - val_loss: 3.1093 - val_accuracy: 0.6430\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1928 - accuracy: 0.9567\n",
      "Epoch 7: val_accuracy did not improve from 0.64304\n",
      "49/49 [==============================] - 9s 176ms/step - loss: 2.1928 - accuracy: 0.9567 - val_loss: 3.2961 - val_accuracy: 0.6152\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1379 - accuracy: 0.9585\n",
      "Epoch 8: val_accuracy improved from 0.64304 to 0.69620, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 9s 179ms/step - loss: 2.1379 - accuracy: 0.9585 - val_loss: 2.9502 - val_accuracy: 0.6962\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0767 - accuracy: 0.9709\n",
      "Epoch 9: val_accuracy did not improve from 0.69620\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 2.0767 - accuracy: 0.9709 - val_loss: 2.8827 - val_accuracy: 0.6937\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0212 - accuracy: 0.9761\n",
      "Epoch 10: val_accuracy improved from 0.69620 to 0.72658, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 2.0212 - accuracy: 0.9761 - val_loss: 2.7675 - val_accuracy: 0.7266\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9889 - accuracy: 0.9741\n",
      "Epoch 11: val_accuracy improved from 0.72658 to 0.77975, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 1.9889 - accuracy: 0.9741 - val_loss: 2.5502 - val_accuracy: 0.7797\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9554 - accuracy: 0.9651\n",
      "Epoch 12: val_accuracy improved from 0.77975 to 0.79241, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 1.9554 - accuracy: 0.9651 - val_loss: 2.4054 - val_accuracy: 0.7924\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9028 - accuracy: 0.9728\n",
      "Epoch 13: val_accuracy improved from 0.79241 to 0.83291, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 1.9028 - accuracy: 0.9728 - val_loss: 2.2606 - val_accuracy: 0.8329\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8614 - accuracy: 0.9754\n",
      "Epoch 14: val_accuracy improved from 0.83291 to 0.84557, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 1.8614 - accuracy: 0.9754 - val_loss: 2.2429 - val_accuracy: 0.8456\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8211 - accuracy: 0.9799\n",
      "Epoch 15: val_accuracy did not improve from 0.84557\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 1.8211 - accuracy: 0.9799 - val_loss: 2.2902 - val_accuracy: 0.8329\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7881 - accuracy: 0.9799\n",
      "Epoch 16: val_accuracy did not improve from 0.84557\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 1.7881 - accuracy: 0.9799 - val_loss: 2.3463 - val_accuracy: 0.8152\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7564 - accuracy: 0.9793\n",
      "Epoch 17: val_accuracy improved from 0.84557 to 0.85823, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 9s 184ms/step - loss: 1.7564 - accuracy: 0.9793 - val_loss: 2.1191 - val_accuracy: 0.8582\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7157 - accuracy: 0.9799\n",
      "Epoch 18: val_accuracy did not improve from 0.85823\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 1.7157 - accuracy: 0.9799 - val_loss: 2.2011 - val_accuracy: 0.8304\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.6877 - accuracy: 0.9812\n",
      "Epoch 19: val_accuracy improved from 0.85823 to 0.87342, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 9s 176ms/step - loss: 1.6877 - accuracy: 0.9812 - val_loss: 2.0396 - val_accuracy: 0.8734\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.6561 - accuracy: 0.9780\n",
      "Epoch 20: val_accuracy did not improve from 0.87342\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 1.6561 - accuracy: 0.9780 - val_loss: 2.1134 - val_accuracy: 0.8304\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.6230 - accuracy: 0.9748\n",
      "Epoch 21: val_accuracy did not improve from 0.87342\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 1.6230 - accuracy: 0.9748 - val_loss: 2.0951 - val_accuracy: 0.8152\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.5974 - accuracy: 0.9774\n",
      "Epoch 22: val_accuracy did not improve from 0.87342\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 1.5974 - accuracy: 0.9774 - val_loss: 2.1090 - val_accuracy: 0.8304\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.5370 - accuracy: 0.9897\n",
      "Epoch 23: val_accuracy did not improve from 0.87342\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 1.5370 - accuracy: 0.9897 - val_loss: 2.0490 - val_accuracy: 0.8329\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.5124 - accuracy: 0.9864\n",
      "Epoch 24: val_accuracy did not improve from 0.87342\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 1.5124 - accuracy: 0.9864 - val_loss: 2.0230 - val_accuracy: 0.8380\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.4994 - accuracy: 0.9799\n",
      "Epoch 25: val_accuracy improved from 0.87342 to 0.89114, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 9s 178ms/step - loss: 1.4994 - accuracy: 0.9799 - val_loss: 1.7374 - val_accuracy: 0.8911\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.4721 - accuracy: 0.9825\n",
      "Epoch 26: val_accuracy did not improve from 0.89114\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 1.4721 - accuracy: 0.9825 - val_loss: 1.7466 - val_accuracy: 0.8810\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.4378 - accuracy: 0.9845\n",
      "Epoch 27: val_accuracy improved from 0.89114 to 0.89367, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 9s 179ms/step - loss: 1.4378 - accuracy: 0.9845 - val_loss: 1.6875 - val_accuracy: 0.8937\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3928 - accuracy: 0.9897\n",
      "Epoch 28: val_accuracy did not improve from 0.89367\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 1.3928 - accuracy: 0.9897 - val_loss: 1.7185 - val_accuracy: 0.8886\n",
      "Epoch 29/100\n",
      "48/49 [============================>.] - ETA: 0s - loss: 1.3698 - accuracy: 0.9915\n",
      "Epoch 29: val_accuracy did not improve from 0.89367\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 1.3718 - accuracy: 0.9909 - val_loss: 2.6940 - val_accuracy: 0.7241\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3926 - accuracy: 0.9715\n",
      "Epoch 30: val_accuracy improved from 0.89367 to 0.91139, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 1.3926 - accuracy: 0.9715 - val_loss: 1.5978 - val_accuracy: 0.9114\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3319 - accuracy: 0.9819\n",
      "Epoch 31: val_accuracy improved from 0.91139 to 0.92152, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 1.3319 - accuracy: 0.9819 - val_loss: 1.5136 - val_accuracy: 0.9215\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3000 - accuracy: 0.9871\n",
      "Epoch 32: val_accuracy did not improve from 0.92152\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 1.3000 - accuracy: 0.9871 - val_loss: 1.5532 - val_accuracy: 0.9013\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2807 - accuracy: 0.9838\n",
      "Epoch 33: val_accuracy did not improve from 0.92152\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 1.2807 - accuracy: 0.9838 - val_loss: 1.5162 - val_accuracy: 0.9215\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2585 - accuracy: 0.9812\n",
      "Epoch 34: val_accuracy did not improve from 0.92152\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 1.2585 - accuracy: 0.9812 - val_loss: 1.4646 - val_accuracy: 0.9190\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2374 - accuracy: 0.9847\n",
      "Epoch 35: val_accuracy did not improve from 0.92152\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 1.2374 - accuracy: 0.9847 - val_loss: 1.4609 - val_accuracy: 0.9139\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1990 - accuracy: 0.9909\n",
      "Epoch 36: val_accuracy improved from 0.92152 to 0.94684, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 1.1990 - accuracy: 0.9909 - val_loss: 1.3557 - val_accuracy: 0.9468\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1785 - accuracy: 0.9884\n",
      "Epoch 37: val_accuracy did not improve from 0.94684\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 1.1785 - accuracy: 0.9884 - val_loss: 1.3830 - val_accuracy: 0.9367\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1593 - accuracy: 0.9845\n",
      "Epoch 38: val_accuracy did not improve from 0.94684\n",
      "49/49 [==============================] - 9s 178ms/step - loss: 1.1593 - accuracy: 0.9845 - val_loss: 1.3227 - val_accuracy: 0.9392\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1212 - accuracy: 0.9917\n",
      "Epoch 39: val_accuracy did not improve from 0.94684\n",
      "49/49 [==============================] - 9s 177ms/step - loss: 1.1212 - accuracy: 0.9917 - val_loss: 1.3207 - val_accuracy: 0.9468\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1082 - accuracy: 0.9903\n",
      "Epoch 40: val_accuracy did not improve from 0.94684\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 1.1082 - accuracy: 0.9903 - val_loss: 1.2630 - val_accuracy: 0.9418\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0805 - accuracy: 0.9909\n",
      "Epoch 41: val_accuracy did not improve from 0.94684\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 1.0805 - accuracy: 0.9909 - val_loss: 1.3280 - val_accuracy: 0.9241\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0670 - accuracy: 0.9929\n",
      "Epoch 42: val_accuracy did not improve from 0.94684\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 1.0670 - accuracy: 0.9929 - val_loss: 1.2573 - val_accuracy: 0.9443\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0362 - accuracy: 0.9948\n",
      "Epoch 43: val_accuracy improved from 0.94684 to 0.95443, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 1.0362 - accuracy: 0.9948 - val_loss: 1.2202 - val_accuracy: 0.9544\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0103 - accuracy: 0.9968\n",
      "Epoch 44: val_accuracy did not improve from 0.95443\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 1.0103 - accuracy: 0.9968 - val_loss: 1.2007 - val_accuracy: 0.9392\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0048 - accuracy: 0.9897\n",
      "Epoch 45: val_accuracy improved from 0.95443 to 0.95696, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 9s 179ms/step - loss: 1.0048 - accuracy: 0.9897 - val_loss: 1.1426 - val_accuracy: 0.9570\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9933 - accuracy: 0.9884\n",
      "Epoch 46: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 0.9933 - accuracy: 0.9884 - val_loss: 1.2288 - val_accuracy: 0.9241\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9573 - accuracy: 0.9948\n",
      "Epoch 47: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 0.9573 - accuracy: 0.9948 - val_loss: 1.1344 - val_accuracy: 0.9316\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9323 - accuracy: 0.9981\n",
      "Epoch 48: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 160ms/step - loss: 0.9323 - accuracy: 0.9981 - val_loss: 1.1193 - val_accuracy: 0.9443\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9208 - accuracy: 0.9948\n",
      "Epoch 49: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 0.9208 - accuracy: 0.9948 - val_loss: 1.1117 - val_accuracy: 0.9468\n",
      "Epoch 50/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9045 - accuracy: 0.9942\n",
      "Epoch 50: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 0.9045 - accuracy: 0.9942 - val_loss: 1.0845 - val_accuracy: 0.9468\n",
      "Epoch 51/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8813 - accuracy: 0.9948\n",
      "Epoch 51: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.8813 - accuracy: 0.9948 - val_loss: 1.0379 - val_accuracy: 0.9494\n",
      "Epoch 52/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8796 - accuracy: 0.9916\n",
      "Epoch 52: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.8796 - accuracy: 0.9916 - val_loss: 1.0584 - val_accuracy: 0.9443\n",
      "Epoch 53/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8579 - accuracy: 0.9922\n",
      "Epoch 53: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 0.8579 - accuracy: 0.9922 - val_loss: 1.0742 - val_accuracy: 0.9392\n",
      "Epoch 54/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8371 - accuracy: 0.9935\n",
      "Epoch 54: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 0.8371 - accuracy: 0.9935 - val_loss: 1.0638 - val_accuracy: 0.9494\n",
      "Epoch 55/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8105 - accuracy: 0.9987\n",
      "Epoch 55: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 0.8105 - accuracy: 0.9987 - val_loss: 1.0671 - val_accuracy: 0.9392\n",
      "Epoch 56/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8044 - accuracy: 0.9948\n",
      "Epoch 56: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 0.8044 - accuracy: 0.9948 - val_loss: 1.0303 - val_accuracy: 0.9468\n",
      "Epoch 57/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7900 - accuracy: 0.9955\n",
      "Epoch 57: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 0.7900 - accuracy: 0.9955 - val_loss: 0.9734 - val_accuracy: 0.9418\n",
      "Epoch 58/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7732 - accuracy: 0.9961\n",
      "Epoch 58: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 0.7732 - accuracy: 0.9961 - val_loss: 0.9537 - val_accuracy: 0.9494\n",
      "Epoch 59/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7564 - accuracy: 0.9955\n",
      "Epoch 59: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 0.7564 - accuracy: 0.9955 - val_loss: 0.9283 - val_accuracy: 0.9519\n",
      "Epoch 60/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7445 - accuracy: 0.9935\n",
      "Epoch 60: val_accuracy improved from 0.95696 to 0.95949, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 0.7445 - accuracy: 0.9935 - val_loss: 0.9232 - val_accuracy: 0.9595\n",
      "Epoch 61/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7248 - accuracy: 0.9968\n",
      "Epoch 61: val_accuracy did not improve from 0.95949\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.7248 - accuracy: 0.9968 - val_loss: 0.9154 - val_accuracy: 0.9519\n",
      "Epoch 62/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7138 - accuracy: 0.9955\n",
      "Epoch 62: val_accuracy improved from 0.95949 to 0.96456, saving model to ./models/best_model_fold_1.h5\n",
      "49/49 [==============================] - 9s 178ms/step - loss: 0.7138 - accuracy: 0.9955 - val_loss: 0.8775 - val_accuracy: 0.9646\n",
      "Epoch 63/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7115 - accuracy: 0.9929\n",
      "Epoch 63: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 0.7115 - accuracy: 0.9929 - val_loss: 0.9904 - val_accuracy: 0.9291\n",
      "Epoch 64/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6892 - accuracy: 0.9961\n",
      "Epoch 64: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.6892 - accuracy: 0.9961 - val_loss: 0.9776 - val_accuracy: 0.9418\n",
      "Epoch 65/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6741 - accuracy: 0.9942\n",
      "Epoch 65: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 0.6741 - accuracy: 0.9942 - val_loss: 0.9510 - val_accuracy: 0.9392\n",
      "Epoch 66/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6612 - accuracy: 0.9961\n",
      "Epoch 66: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.6612 - accuracy: 0.9961 - val_loss: 0.9137 - val_accuracy: 0.9367\n",
      "Epoch 67/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6427 - accuracy: 0.9981Restoring model weights from the end of the best epoch: 62.\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.6427 - accuracy: 0.9981 - val_loss: 0.9023 - val_accuracy: 0.9418\n",
      "Epoch 67: early stopping\n",
      "13/13 [==============================] - 2s 113ms/step\n",
      "Fold 1 Validation Accuracy: 0.9418\n",
      "Fold 1 Validation F1 Score: 0.9545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved for fold 1 with accuracy 0.9418\n",
      "Training fold 2/5\n",
      "Found 1578 validated image filenames belonging to 3 classes.\n",
      "Found 395 validated image filenames belonging to 3 classes.\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 3.0659 - accuracy: 0.6895\n",
      "Epoch 1: val_accuracy improved from -inf to 0.67342, saving model to ./models/best_model_fold_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 16s 189ms/step - loss: 3.0659 - accuracy: 0.6895 - val_loss: 3.0470 - val_accuracy: 0.6734\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.6356 - accuracy: 0.8558\n",
      "Epoch 2: val_accuracy improved from 0.67342 to 0.75949, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 2.6356 - accuracy: 0.8558 - val_loss: 2.8495 - val_accuracy: 0.7595\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4637 - accuracy: 0.9243\n",
      "Epoch 3: val_accuracy improved from 0.75949 to 0.78734, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 9s 183ms/step - loss: 2.4637 - accuracy: 0.9243 - val_loss: 2.7431 - val_accuracy: 0.7873\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4020 - accuracy: 0.9172\n",
      "Epoch 4: val_accuracy improved from 0.78734 to 0.84810, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 9s 183ms/step - loss: 2.4020 - accuracy: 0.9172 - val_loss: 2.5547 - val_accuracy: 0.8481\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2907 - accuracy: 0.9450\n",
      "Epoch 5: val_accuracy improved from 0.84810 to 0.88101, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 2.2907 - accuracy: 0.9450 - val_loss: 2.4782 - val_accuracy: 0.8810\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2550 - accuracy: 0.9437\n",
      "Epoch 6: val_accuracy improved from 0.88101 to 0.88608, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 2.2550 - accuracy: 0.9437 - val_loss: 2.4268 - val_accuracy: 0.8861\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1741 - accuracy: 0.9573\n",
      "Epoch 7: val_accuracy did not improve from 0.88608\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 2.1741 - accuracy: 0.9573 - val_loss: 2.4994 - val_accuracy: 0.8658\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1209 - accuracy: 0.9612\n",
      "Epoch 8: val_accuracy did not improve from 0.88608\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 2.1209 - accuracy: 0.9612 - val_loss: 2.3506 - val_accuracy: 0.8835\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0727 - accuracy: 0.9657\n",
      "Epoch 9: val_accuracy did not improve from 0.88608\n",
      "49/49 [==============================] - 9s 176ms/step - loss: 2.0727 - accuracy: 0.9657 - val_loss: 2.3413 - val_accuracy: 0.8835\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0114 - accuracy: 0.9709\n",
      "Epoch 10: val_accuracy improved from 0.88608 to 0.89367, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 9s 176ms/step - loss: 2.0114 - accuracy: 0.9709 - val_loss: 2.2846 - val_accuracy: 0.8937\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9771 - accuracy: 0.9657\n",
      "Epoch 11: val_accuracy improved from 0.89367 to 0.90633, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 1.9771 - accuracy: 0.9657 - val_loss: 2.1664 - val_accuracy: 0.9063\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9350 - accuracy: 0.9735\n",
      "Epoch 12: val_accuracy improved from 0.90633 to 0.92658, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 9s 179ms/step - loss: 1.9350 - accuracy: 0.9735 - val_loss: 2.1329 - val_accuracy: 0.9266\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8905 - accuracy: 0.9735\n",
      "Epoch 13: val_accuracy did not improve from 0.92658\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 1.8905 - accuracy: 0.9735 - val_loss: 2.1764 - val_accuracy: 0.8810\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8735 - accuracy: 0.9728\n",
      "Epoch 14: val_accuracy did not improve from 0.92658\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 1.8735 - accuracy: 0.9728 - val_loss: 2.0916 - val_accuracy: 0.9089\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8284 - accuracy: 0.9741\n",
      "Epoch 15: val_accuracy improved from 0.92658 to 0.93165, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 1.8284 - accuracy: 0.9741 - val_loss: 2.0210 - val_accuracy: 0.9316\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7633 - accuracy: 0.9851\n",
      "Epoch 16: val_accuracy did not improve from 0.93165\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 1.7633 - accuracy: 0.9851 - val_loss: 1.9864 - val_accuracy: 0.9190\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7429 - accuracy: 0.9799\n",
      "Epoch 17: val_accuracy did not improve from 0.93165\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 1.7429 - accuracy: 0.9799 - val_loss: 1.9502 - val_accuracy: 0.9215\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7157 - accuracy: 0.9774\n",
      "Epoch 18: val_accuracy did not improve from 0.93165\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 1.7157 - accuracy: 0.9774 - val_loss: 1.9235 - val_accuracy: 0.9139\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.6812 - accuracy: 0.9774\n",
      "Epoch 19: val_accuracy improved from 0.93165 to 0.93418, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 9s 176ms/step - loss: 1.6812 - accuracy: 0.9774 - val_loss: 1.8250 - val_accuracy: 0.9342\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.6422 - accuracy: 0.9793\n",
      "Epoch 20: val_accuracy improved from 0.93418 to 0.93671, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 9s 181ms/step - loss: 1.6422 - accuracy: 0.9793 - val_loss: 1.7979 - val_accuracy: 0.9367\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.6167 - accuracy: 0.9793\n",
      "Epoch 21: val_accuracy did not improve from 0.93671\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 1.6167 - accuracy: 0.9793 - val_loss: 1.8327 - val_accuracy: 0.9266\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.5782 - accuracy: 0.9819\n",
      "Epoch 22: val_accuracy did not improve from 0.93671\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 1.5782 - accuracy: 0.9819 - val_loss: 1.7584 - val_accuracy: 0.9342\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.5570 - accuracy: 0.9793\n",
      "Epoch 23: val_accuracy did not improve from 0.93671\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 1.5570 - accuracy: 0.9793 - val_loss: 1.7432 - val_accuracy: 0.9367\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.5233 - accuracy: 0.9819\n",
      "Epoch 24: val_accuracy improved from 0.93671 to 0.94684, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 1.5233 - accuracy: 0.9819 - val_loss: 1.7181 - val_accuracy: 0.9468\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.4780 - accuracy: 0.9877\n",
      "Epoch 25: val_accuracy did not improve from 0.94684\n",
      "49/49 [==============================] - 9s 177ms/step - loss: 1.4780 - accuracy: 0.9877 - val_loss: 1.6916 - val_accuracy: 0.9367\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.4447 - accuracy: 0.9877\n",
      "Epoch 26: val_accuracy did not improve from 0.94684\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 1.4447 - accuracy: 0.9877 - val_loss: 1.6470 - val_accuracy: 0.9316\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.4152 - accuracy: 0.9903\n",
      "Epoch 27: val_accuracy did not improve from 0.94684\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 1.4152 - accuracy: 0.9903 - val_loss: 1.5921 - val_accuracy: 0.9443\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3967 - accuracy: 0.9851\n",
      "Epoch 28: val_accuracy improved from 0.94684 to 0.95190, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 1.3967 - accuracy: 0.9851 - val_loss: 1.5370 - val_accuracy: 0.9519\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3728 - accuracy: 0.9871\n",
      "Epoch 29: val_accuracy did not improve from 0.95190\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 1.3728 - accuracy: 0.9871 - val_loss: 1.5385 - val_accuracy: 0.9494\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3382 - accuracy: 0.9877\n",
      "Epoch 30: val_accuracy improved from 0.95190 to 0.95443, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 9s 180ms/step - loss: 1.3382 - accuracy: 0.9877 - val_loss: 1.4956 - val_accuracy: 0.9544\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2998 - accuracy: 0.9935\n",
      "Epoch 31: val_accuracy did not improve from 0.95443\n",
      "49/49 [==============================] - 9s 178ms/step - loss: 1.2998 - accuracy: 0.9935 - val_loss: 1.4867 - val_accuracy: 0.9519\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2766 - accuracy: 0.9922\n",
      "Epoch 32: val_accuracy improved from 0.95443 to 0.95696, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 9s 179ms/step - loss: 1.2766 - accuracy: 0.9922 - val_loss: 1.4647 - val_accuracy: 0.9570\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2471 - accuracy: 0.9948\n",
      "Epoch 33: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 1.2471 - accuracy: 0.9948 - val_loss: 1.4205 - val_accuracy: 0.9570\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2223 - accuracy: 0.9935\n",
      "Epoch 34: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 1.2223 - accuracy: 0.9935 - val_loss: 1.4958 - val_accuracy: 0.9241\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1999 - accuracy: 0.9961\n",
      "Epoch 35: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 1.1999 - accuracy: 0.9961 - val_loss: 1.4458 - val_accuracy: 0.9494\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1857 - accuracy: 0.9929\n",
      "Epoch 36: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 1.1857 - accuracy: 0.9929 - val_loss: 1.3348 - val_accuracy: 0.9468\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1548 - accuracy: 0.9948\n",
      "Epoch 37: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 1.1548 - accuracy: 0.9948 - val_loss: 1.3462 - val_accuracy: 0.9570\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1383 - accuracy: 0.9897\n",
      "Epoch 38: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 1.1383 - accuracy: 0.9897 - val_loss: 1.3756 - val_accuracy: 0.9544\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1273 - accuracy: 0.9890\n",
      "Epoch 39: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 1.1273 - accuracy: 0.9890 - val_loss: 1.3398 - val_accuracy: 0.9544\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0965 - accuracy: 0.9922\n",
      "Epoch 40: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 1.0965 - accuracy: 0.9922 - val_loss: 1.3172 - val_accuracy: 0.9468\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0645 - accuracy: 0.9968\n",
      "Epoch 41: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 1.0645 - accuracy: 0.9968 - val_loss: 1.2809 - val_accuracy: 0.9468\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0575 - accuracy: 0.9942\n",
      "Epoch 42: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 1.0575 - accuracy: 0.9942 - val_loss: 1.2778 - val_accuracy: 0.9367\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0485 - accuracy: 0.9845\n",
      "Epoch 43: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 1.0485 - accuracy: 0.9845 - val_loss: 1.2635 - val_accuracy: 0.9494\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0093 - accuracy: 0.9935\n",
      "Epoch 44: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 1.0093 - accuracy: 0.9935 - val_loss: 1.2244 - val_accuracy: 0.9468\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0030 - accuracy: 0.9877\n",
      "Epoch 45: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 1.0030 - accuracy: 0.9877 - val_loss: 1.1905 - val_accuracy: 0.9519\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9773 - accuracy: 0.9903\n",
      "Epoch 46: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 161ms/step - loss: 0.9773 - accuracy: 0.9903 - val_loss: 1.1855 - val_accuracy: 0.9494\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9522 - accuracy: 0.9929\n",
      "Epoch 47: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 0.9522 - accuracy: 0.9929 - val_loss: 1.2170 - val_accuracy: 0.9443\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9611 - accuracy: 0.9864\n",
      "Epoch 48: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 0.9611 - accuracy: 0.9864 - val_loss: 1.1611 - val_accuracy: 0.9544\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9331 - accuracy: 0.9903\n",
      "Epoch 49: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 0.9331 - accuracy: 0.9903 - val_loss: 1.1232 - val_accuracy: 0.9468\n",
      "Epoch 50/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9088 - accuracy: 0.9909\n",
      "Epoch 50: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.9088 - accuracy: 0.9909 - val_loss: 1.1618 - val_accuracy: 0.9291\n",
      "Epoch 51/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8879 - accuracy: 0.9916\n",
      "Epoch 51: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.8879 - accuracy: 0.9916 - val_loss: 1.1248 - val_accuracy: 0.9392\n",
      "Epoch 52/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8650 - accuracy: 0.9981\n",
      "Epoch 52: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 0.8650 - accuracy: 0.9981 - val_loss: 1.1028 - val_accuracy: 0.9494\n",
      "Epoch 53/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8485 - accuracy: 0.9955\n",
      "Epoch 53: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 0.8485 - accuracy: 0.9955 - val_loss: 1.1206 - val_accuracy: 0.9494\n",
      "Epoch 54/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8451 - accuracy: 0.9929\n",
      "Epoch 54: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 0.8451 - accuracy: 0.9929 - val_loss: 1.0370 - val_accuracy: 0.9544\n",
      "Epoch 55/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8189 - accuracy: 0.9916\n",
      "Epoch 55: val_accuracy did not improve from 0.95696\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.8189 - accuracy: 0.9916 - val_loss: 1.0212 - val_accuracy: 0.9494\n",
      "Epoch 56/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8246 - accuracy: 0.9890\n",
      "Epoch 56: val_accuracy improved from 0.95696 to 0.96456, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 0.8246 - accuracy: 0.9890 - val_loss: 0.9437 - val_accuracy: 0.9646\n",
      "Epoch 57/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7890 - accuracy: 0.9916\n",
      "Epoch 57: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.7890 - accuracy: 0.9916 - val_loss: 0.9581 - val_accuracy: 0.9646\n",
      "Epoch 58/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7725 - accuracy: 0.9916\n",
      "Epoch 58: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 0.7725 - accuracy: 0.9916 - val_loss: 1.0069 - val_accuracy: 0.9392\n",
      "Epoch 59/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7760 - accuracy: 0.9909\n",
      "Epoch 59: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 0.7760 - accuracy: 0.9909 - val_loss: 0.9638 - val_accuracy: 0.9544\n",
      "Epoch 60/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7397 - accuracy: 0.9955\n",
      "Epoch 60: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.7397 - accuracy: 0.9955 - val_loss: 1.0188 - val_accuracy: 0.9367\n",
      "Epoch 61/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7220 - accuracy: 0.9981\n",
      "Epoch 61: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.7220 - accuracy: 0.9981 - val_loss: 0.9081 - val_accuracy: 0.9620\n",
      "Epoch 62/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7159 - accuracy: 0.9961\n",
      "Epoch 62: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 0.7159 - accuracy: 0.9961 - val_loss: 0.9404 - val_accuracy: 0.9494\n",
      "Epoch 63/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.9968\n",
      "Epoch 63: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 0.6930 - accuracy: 0.9968 - val_loss: 0.9087 - val_accuracy: 0.9443\n",
      "Epoch 64/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6838 - accuracy: 0.9968\n",
      "Epoch 64: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 0.6838 - accuracy: 0.9968 - val_loss: 0.9197 - val_accuracy: 0.9519\n",
      "Epoch 65/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6801 - accuracy: 0.9922\n",
      "Epoch 65: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 0.6801 - accuracy: 0.9922 - val_loss: 0.8547 - val_accuracy: 0.9570\n",
      "Epoch 66/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6559 - accuracy: 0.9987\n",
      "Epoch 66: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 0.6559 - accuracy: 0.9987 - val_loss: 0.8533 - val_accuracy: 0.9544\n",
      "Epoch 67/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6471 - accuracy: 0.9974\n",
      "Epoch 67: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 160ms/step - loss: 0.6471 - accuracy: 0.9974 - val_loss: 0.8560 - val_accuracy: 0.9544\n",
      "Epoch 68/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6302 - accuracy: 0.9974\n",
      "Epoch 68: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 0.6302 - accuracy: 0.9974 - val_loss: 0.8445 - val_accuracy: 0.9595\n",
      "Epoch 69/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6158 - accuracy: 0.9987\n",
      "Epoch 69: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 174ms/step - loss: 0.6158 - accuracy: 0.9987 - val_loss: 0.8192 - val_accuracy: 0.9646\n",
      "Epoch 70/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6118 - accuracy: 0.9961\n",
      "Epoch 70: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 0.6118 - accuracy: 0.9961 - val_loss: 0.8048 - val_accuracy: 0.9570\n",
      "Epoch 71/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5946 - accuracy: 0.9974\n",
      "Epoch 71: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.5946 - accuracy: 0.9974 - val_loss: 0.7872 - val_accuracy: 0.9595\n",
      "Epoch 72/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5931 - accuracy: 0.9955\n",
      "Epoch 72: val_accuracy did not improve from 0.96456\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.5931 - accuracy: 0.9955 - val_loss: 0.7652 - val_accuracy: 0.9646\n",
      "Epoch 73/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5758 - accuracy: 0.9955\n",
      "Epoch 73: val_accuracy improved from 0.96456 to 0.96962, saving model to ./models/best_model_fold_2.h5\n",
      "49/49 [==============================] - 9s 177ms/step - loss: 0.5758 - accuracy: 0.9955 - val_loss: 0.7546 - val_accuracy: 0.9696\n",
      "Epoch 74/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5602 - accuracy: 0.9981\n",
      "Epoch 74: val_accuracy did not improve from 0.96962\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 0.5602 - accuracy: 0.9981 - val_loss: 0.7646 - val_accuracy: 0.9544\n",
      "Epoch 75/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5544 - accuracy: 0.9974\n",
      "Epoch 75: val_accuracy did not improve from 0.96962\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.5544 - accuracy: 0.9974 - val_loss: 0.7449 - val_accuracy: 0.9671\n",
      "Epoch 76/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5522 - accuracy: 0.9935\n",
      "Epoch 76: val_accuracy did not improve from 0.96962\n",
      "49/49 [==============================] - 8s 161ms/step - loss: 0.5522 - accuracy: 0.9935 - val_loss: 0.7480 - val_accuracy: 0.9468\n",
      "Epoch 77/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5467 - accuracy: 0.9903\n",
      "Epoch 77: val_accuracy did not improve from 0.96962\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 0.5467 - accuracy: 0.9903 - val_loss: 0.6993 - val_accuracy: 0.9671\n",
      "Epoch 78/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5524 - accuracy: 0.9871\n",
      "Epoch 78: val_accuracy did not improve from 0.96962\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 0.5524 - accuracy: 0.9871 - val_loss: 0.7518 - val_accuracy: 0.9392\n",
      "Epoch 79/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5223 - accuracy: 0.9942\n",
      "Epoch 79: val_accuracy did not improve from 0.96962\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 0.5223 - accuracy: 0.9942 - val_loss: 0.7229 - val_accuracy: 0.9544\n",
      "Epoch 80/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5241 - accuracy: 0.9909\n",
      "Epoch 80: val_accuracy did not improve from 0.96962\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 0.5241 - accuracy: 0.9909 - val_loss: 0.6977 - val_accuracy: 0.9494\n",
      "Epoch 81/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5035 - accuracy: 0.9942\n",
      "Epoch 81: val_accuracy did not improve from 0.96962\n",
      "49/49 [==============================] - 9s 177ms/step - loss: 0.5035 - accuracy: 0.9942 - val_loss: 0.6778 - val_accuracy: 0.9595\n",
      "Epoch 82/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4819 - accuracy: 0.9981\n",
      "Epoch 82: val_accuracy did not improve from 0.96962\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 0.4819 - accuracy: 0.9981 - val_loss: 0.6601 - val_accuracy: 0.9620\n",
      "Epoch 83/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4792 - accuracy: 0.9948\n",
      "Epoch 83: val_accuracy did not improve from 0.96962\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 0.4792 - accuracy: 0.9948 - val_loss: 0.7112 - val_accuracy: 0.9494\n",
      "Epoch 84/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4789 - accuracy: 0.9903\n",
      "Epoch 84: val_accuracy did not improve from 0.96962\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 0.4789 - accuracy: 0.9903 - val_loss: 0.6168 - val_accuracy: 0.9646\n",
      "Epoch 85/100\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.4598 - accuracy: 0.9967\n",
      "Epoch 85: val_accuracy did not improve from 0.96962\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 0.4610 - accuracy: 0.9961 - val_loss: 0.8962 - val_accuracy: 0.8911\n",
      "Epoch 86/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4592 - accuracy: 0.9909\n",
      "Epoch 86: val_accuracy did not improve from 0.96962\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 0.4592 - accuracy: 0.9909 - val_loss: 0.7560 - val_accuracy: 0.9316\n",
      "Epoch 87/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4501 - accuracy: 0.9909\n",
      "Epoch 87: val_accuracy did not improve from 0.96962\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 0.4501 - accuracy: 0.9909 - val_loss: 0.6876 - val_accuracy: 0.9418\n",
      "Epoch 88/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4330 - accuracy: 0.9955\n",
      "Epoch 88: val_accuracy did not improve from 0.96962\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.4330 - accuracy: 0.9955 - val_loss: 0.6386 - val_accuracy: 0.9570\n",
      "Epoch 89/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4270 - accuracy: 0.9955Restoring model weights from the end of the best epoch: 84.\n",
      "\n",
      "Epoch 89: val_accuracy did not improve from 0.96962\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.4270 - accuracy: 0.9955 - val_loss: 0.6307 - val_accuracy: 0.9570\n",
      "Epoch 89: early stopping\n",
      "13/13 [==============================] - 2s 120ms/step\n",
      "Fold 2 Validation Accuracy: 0.9570\n",
      "Fold 2 Validation F1 Score: 0.9670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved for fold 2 with accuracy 0.9570\n",
      "Training fold 3/5\n",
      "Found 1578 validated image filenames belonging to 3 classes.\n",
      "Found 395 validated image filenames belonging to 3 classes.\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 3.0975 - accuracy: 0.6863\n",
      "Epoch 1: val_accuracy improved from -inf to 0.51899, saving model to ./models/best_model_fold_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 16s 183ms/step - loss: 3.0975 - accuracy: 0.6863 - val_loss: 3.4209 - val_accuracy: 0.5190\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.6573 - accuracy: 0.8532\n",
      "Epoch 2: val_accuracy improved from 0.51899 to 0.52911, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 9s 178ms/step - loss: 2.6573 - accuracy: 0.8532 - val_loss: 3.4072 - val_accuracy: 0.5291\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5075 - accuracy: 0.8907\n",
      "Epoch 3: val_accuracy improved from 0.52911 to 0.55696, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 9s 176ms/step - loss: 2.5075 - accuracy: 0.8907 - val_loss: 3.2979 - val_accuracy: 0.5570\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4055 - accuracy: 0.9217\n",
      "Epoch 4: val_accuracy improved from 0.55696 to 0.60506, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 9s 179ms/step - loss: 2.4055 - accuracy: 0.9217 - val_loss: 3.0165 - val_accuracy: 0.6051\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3266 - accuracy: 0.9347\n",
      "Epoch 5: val_accuracy improved from 0.60506 to 0.74430, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 2.3266 - accuracy: 0.9347 - val_loss: 2.7648 - val_accuracy: 0.7443\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2576 - accuracy: 0.9373\n",
      "Epoch 6: val_accuracy did not improve from 0.74430\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 2.2576 - accuracy: 0.9373 - val_loss: 2.6603 - val_accuracy: 0.7392\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2076 - accuracy: 0.9508\n",
      "Epoch 7: val_accuracy improved from 0.74430 to 0.75190, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 2.2076 - accuracy: 0.9508 - val_loss: 2.7118 - val_accuracy: 0.7519\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1481 - accuracy: 0.9508\n",
      "Epoch 8: val_accuracy improved from 0.75190 to 0.83544, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 2.1481 - accuracy: 0.9508 - val_loss: 2.4732 - val_accuracy: 0.8354\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0957 - accuracy: 0.9586\n",
      "Epoch 9: val_accuracy did not improve from 0.83544\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 2.0957 - accuracy: 0.9586 - val_loss: 2.6453 - val_accuracy: 0.7316\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0522 - accuracy: 0.9625\n",
      "Epoch 10: val_accuracy did not improve from 0.83544\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 2.0522 - accuracy: 0.9625 - val_loss: 2.5302 - val_accuracy: 0.7722\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0151 - accuracy: 0.9612\n",
      "Epoch 11: val_accuracy improved from 0.83544 to 0.86329, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 2.0151 - accuracy: 0.9612 - val_loss: 2.3122 - val_accuracy: 0.8633\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9668 - accuracy: 0.9670\n",
      "Epoch 12: val_accuracy did not improve from 0.86329\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 1.9668 - accuracy: 0.9670 - val_loss: 2.2330 - val_accuracy: 0.8557\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9100 - accuracy: 0.9709\n",
      "Epoch 13: val_accuracy did not improve from 0.86329\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 1.9100 - accuracy: 0.9709 - val_loss: 2.2083 - val_accuracy: 0.8582\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8628 - accuracy: 0.9728\n",
      "Epoch 14: val_accuracy did not improve from 0.86329\n",
      "49/49 [==============================] - 9s 178ms/step - loss: 1.8628 - accuracy: 0.9728 - val_loss: 2.1765 - val_accuracy: 0.8557\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8107 - accuracy: 0.9787\n",
      "Epoch 15: val_accuracy improved from 0.86329 to 0.89620, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 8s 174ms/step - loss: 1.8107 - accuracy: 0.9787 - val_loss: 2.0429 - val_accuracy: 0.8962\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7808 - accuracy: 0.9780\n",
      "Epoch 16: val_accuracy did not improve from 0.89620\n",
      "49/49 [==============================] - 8s 161ms/step - loss: 1.7808 - accuracy: 0.9780 - val_loss: 2.0733 - val_accuracy: 0.8810\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7527 - accuracy: 0.9793\n",
      "Epoch 17: val_accuracy improved from 0.89620 to 0.89873, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 1.7527 - accuracy: 0.9793 - val_loss: 1.9873 - val_accuracy: 0.8987\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7300 - accuracy: 0.9774\n",
      "Epoch 18: val_accuracy did not improve from 0.89873\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 1.7300 - accuracy: 0.9774 - val_loss: 1.8905 - val_accuracy: 0.8962\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7060 - accuracy: 0.9702\n",
      "Epoch 19: val_accuracy improved from 0.89873 to 0.91139, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 1.7060 - accuracy: 0.9702 - val_loss: 1.8413 - val_accuracy: 0.9114\n",
      "Epoch 20/100\n",
      "48/49 [============================>.] - ETA: 0s - loss: 1.6500 - accuracy: 0.9792\n",
      "Epoch 20: val_accuracy improved from 0.91139 to 0.92152, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 9s 181ms/step - loss: 1.6496 - accuracy: 0.9793 - val_loss: 1.8343 - val_accuracy: 0.9215\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.6079 - accuracy: 0.9838\n",
      "Epoch 21: val_accuracy did not improve from 0.92152\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 1.6079 - accuracy: 0.9838 - val_loss: 1.7730 - val_accuracy: 0.9215\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.5817 - accuracy: 0.9825\n",
      "Epoch 22: val_accuracy did not improve from 0.92152\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 1.5817 - accuracy: 0.9825 - val_loss: 1.7831 - val_accuracy: 0.9063\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.5455 - accuracy: 0.9838\n",
      "Epoch 23: val_accuracy did not improve from 0.92152\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 1.5455 - accuracy: 0.9838 - val_loss: 1.7796 - val_accuracy: 0.9165\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.5311 - accuracy: 0.9799\n",
      "Epoch 24: val_accuracy did not improve from 0.92152\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 1.5311 - accuracy: 0.9799 - val_loss: 1.6837 - val_accuracy: 0.9114\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.4939 - accuracy: 0.9851\n",
      "Epoch 25: val_accuracy improved from 0.92152 to 0.93418, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 1.4939 - accuracy: 0.9851 - val_loss: 1.6105 - val_accuracy: 0.9342\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.4651 - accuracy: 0.9845\n",
      "Epoch 26: val_accuracy improved from 0.93418 to 0.93924, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 9s 177ms/step - loss: 1.4651 - accuracy: 0.9845 - val_loss: 1.6029 - val_accuracy: 0.9392\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.4413 - accuracy: 0.9812\n",
      "Epoch 27: val_accuracy did not improve from 0.93924\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 1.4413 - accuracy: 0.9812 - val_loss: 1.5902 - val_accuracy: 0.9392\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.4008 - accuracy: 0.9858\n",
      "Epoch 28: val_accuracy improved from 0.93924 to 0.95190, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 9s 183ms/step - loss: 1.4008 - accuracy: 0.9858 - val_loss: 1.5680 - val_accuracy: 0.9519\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3784 - accuracy: 0.9871\n",
      "Epoch 29: val_accuracy did not improve from 0.95190\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 1.3784 - accuracy: 0.9871 - val_loss: 1.5025 - val_accuracy: 0.9418\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3532 - accuracy: 0.9864\n",
      "Epoch 30: val_accuracy did not improve from 0.95190\n",
      "49/49 [==============================] - 9s 176ms/step - loss: 1.3532 - accuracy: 0.9864 - val_loss: 1.4660 - val_accuracy: 0.9468\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3261 - accuracy: 0.9851\n",
      "Epoch 31: val_accuracy did not improve from 0.95190\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 1.3261 - accuracy: 0.9851 - val_loss: 1.4555 - val_accuracy: 0.9519\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3019 - accuracy: 0.9884\n",
      "Epoch 32: val_accuracy did not improve from 0.95190\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 1.3019 - accuracy: 0.9884 - val_loss: 1.4231 - val_accuracy: 0.9418\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2787 - accuracy: 0.9851\n",
      "Epoch 33: val_accuracy improved from 0.95190 to 0.95443, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 1.2787 - accuracy: 0.9851 - val_loss: 1.3765 - val_accuracy: 0.9544\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2540 - accuracy: 0.9877\n",
      "Epoch 34: val_accuracy did not improve from 0.95443\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 1.2540 - accuracy: 0.9877 - val_loss: 1.4249 - val_accuracy: 0.9468\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2284 - accuracy: 0.9903\n",
      "Epoch 35: val_accuracy did not improve from 0.95443\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 1.2284 - accuracy: 0.9903 - val_loss: 1.3638 - val_accuracy: 0.9494\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1861 - accuracy: 0.9942\n",
      "Epoch 36: val_accuracy did not improve from 0.95443\n",
      "49/49 [==============================] - 9s 176ms/step - loss: 1.1861 - accuracy: 0.9942 - val_loss: 1.3375 - val_accuracy: 0.9544\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1883 - accuracy: 0.9877\n",
      "Epoch 37: val_accuracy did not improve from 0.95443\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 1.1883 - accuracy: 0.9877 - val_loss: 1.3943 - val_accuracy: 0.9190\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1632 - accuracy: 0.9864\n",
      "Epoch 38: val_accuracy did not improve from 0.95443\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 1.1632 - accuracy: 0.9864 - val_loss: 1.2668 - val_accuracy: 0.9544\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1262 - accuracy: 0.9916\n",
      "Epoch 39: val_accuracy improved from 0.95443 to 0.95696, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 9s 181ms/step - loss: 1.1262 - accuracy: 0.9916 - val_loss: 1.2740 - val_accuracy: 0.9570\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1048 - accuracy: 0.9890\n",
      "Epoch 40: val_accuracy improved from 0.95696 to 0.96203, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 1.1048 - accuracy: 0.9890 - val_loss: 1.2103 - val_accuracy: 0.9620\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0763 - accuracy: 0.9961\n",
      "Epoch 41: val_accuracy improved from 0.96203 to 0.97468, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 1.0763 - accuracy: 0.9961 - val_loss: 1.1713 - val_accuracy: 0.9747\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0577 - accuracy: 0.9935\n",
      "Epoch 42: val_accuracy did not improve from 0.97468\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 1.0577 - accuracy: 0.9935 - val_loss: 1.1742 - val_accuracy: 0.9620\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0350 - accuracy: 0.9942\n",
      "Epoch 43: val_accuracy did not improve from 0.97468\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 1.0350 - accuracy: 0.9942 - val_loss: 1.1508 - val_accuracy: 0.9646\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0201 - accuracy: 0.9922\n",
      "Epoch 44: val_accuracy did not improve from 0.97468\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 1.0201 - accuracy: 0.9922 - val_loss: 1.1701 - val_accuracy: 0.9494\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9980 - accuracy: 0.9942\n",
      "Epoch 45: val_accuracy did not improve from 0.97468\n",
      "49/49 [==============================] - 9s 176ms/step - loss: 0.9980 - accuracy: 0.9942 - val_loss: 1.1423 - val_accuracy: 0.9570\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9867 - accuracy: 0.9909\n",
      "Epoch 46: val_accuracy did not improve from 0.97468\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 0.9867 - accuracy: 0.9909 - val_loss: 1.1204 - val_accuracy: 0.9494\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9831 - accuracy: 0.9851\n",
      "Epoch 47: val_accuracy did not improve from 0.97468\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.9831 - accuracy: 0.9851 - val_loss: 1.0845 - val_accuracy: 0.9595\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9414 - accuracy: 0.9929\n",
      "Epoch 48: val_accuracy did not improve from 0.97468\n",
      "49/49 [==============================] - 9s 177ms/step - loss: 0.9414 - accuracy: 0.9929 - val_loss: 1.0651 - val_accuracy: 0.9620\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9368 - accuracy: 0.9877\n",
      "Epoch 49: val_accuracy did not improve from 0.97468\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 0.9368 - accuracy: 0.9877 - val_loss: 1.0416 - val_accuracy: 0.9646\n",
      "Epoch 50/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9109 - accuracy: 0.9884\n",
      "Epoch 50: val_accuracy did not improve from 0.97468\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 0.9109 - accuracy: 0.9884 - val_loss: 1.0627 - val_accuracy: 0.9595\n",
      "Epoch 51/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8971 - accuracy: 0.9890\n",
      "Epoch 51: val_accuracy did not improve from 0.97468\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 0.8971 - accuracy: 0.9890 - val_loss: 1.0421 - val_accuracy: 0.9595\n",
      "Epoch 52/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8694 - accuracy: 0.9948\n",
      "Epoch 52: val_accuracy improved from 0.97468 to 0.97722, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 0.8694 - accuracy: 0.9948 - val_loss: 0.9435 - val_accuracy: 0.9772\n",
      "Epoch 53/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8502 - accuracy: 0.9955\n",
      "Epoch 53: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 0.8502 - accuracy: 0.9955 - val_loss: 0.9490 - val_accuracy: 0.9696\n",
      "Epoch 54/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8320 - accuracy: 0.9955\n",
      "Epoch 54: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 0.8320 - accuracy: 0.9955 - val_loss: 0.9797 - val_accuracy: 0.9696\n",
      "Epoch 55/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8179 - accuracy: 0.9942\n",
      "Epoch 55: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.8179 - accuracy: 0.9942 - val_loss: 0.9349 - val_accuracy: 0.9595\n",
      "Epoch 56/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8209 - accuracy: 0.9890\n",
      "Epoch 56: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 0.8209 - accuracy: 0.9890 - val_loss: 0.8892 - val_accuracy: 0.9620\n",
      "Epoch 57/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7873 - accuracy: 0.9968\n",
      "Epoch 57: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 0.7873 - accuracy: 0.9968 - val_loss: 0.9418 - val_accuracy: 0.9570\n",
      "Epoch 58/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7766 - accuracy: 0.9942\n",
      "Epoch 58: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 0.7766 - accuracy: 0.9942 - val_loss: 0.8987 - val_accuracy: 0.9570\n",
      "Epoch 59/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7535 - accuracy: 0.9955\n",
      "Epoch 59: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 0.7535 - accuracy: 0.9955 - val_loss: 0.8971 - val_accuracy: 0.9620\n",
      "Epoch 60/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7380 - accuracy: 0.9987\n",
      "Epoch 60: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 159ms/step - loss: 0.7380 - accuracy: 0.9987 - val_loss: 0.8847 - val_accuracy: 0.9620\n",
      "Epoch 61/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7307 - accuracy: 0.9948\n",
      "Epoch 61: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 174ms/step - loss: 0.7307 - accuracy: 0.9948 - val_loss: 0.8681 - val_accuracy: 0.9620\n",
      "Epoch 62/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7223 - accuracy: 0.9935\n",
      "Epoch 62: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 0.7223 - accuracy: 0.9935 - val_loss: 0.8445 - val_accuracy: 0.9671\n",
      "Epoch 63/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7185 - accuracy: 0.9909\n",
      "Epoch 63: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.7185 - accuracy: 0.9909 - val_loss: 0.8302 - val_accuracy: 0.9570\n",
      "Epoch 64/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6848 - accuracy: 0.9987\n",
      "Epoch 64: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 0.6848 - accuracy: 0.9987 - val_loss: 0.8203 - val_accuracy: 0.9646\n",
      "Epoch 65/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6721 - accuracy: 0.9955\n",
      "Epoch 65: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 0.6721 - accuracy: 0.9955 - val_loss: 0.7817 - val_accuracy: 0.9722\n",
      "Epoch 66/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6594 - accuracy: 0.9974\n",
      "Epoch 66: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 0.6594 - accuracy: 0.9974 - val_loss: 0.8008 - val_accuracy: 0.9747\n",
      "Epoch 67/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6475 - accuracy: 0.9961\n",
      "Epoch 67: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 0.6475 - accuracy: 0.9961 - val_loss: 0.7983 - val_accuracy: 0.9620\n",
      "Epoch 68/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6359 - accuracy: 0.9961\n",
      "Epoch 68: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 0.6359 - accuracy: 0.9961 - val_loss: 0.7584 - val_accuracy: 0.9595\n",
      "Epoch 69/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6301 - accuracy: 0.9955\n",
      "Epoch 69: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 161ms/step - loss: 0.6301 - accuracy: 0.9955 - val_loss: 0.7641 - val_accuracy: 0.9620\n",
      "Epoch 70/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6291 - accuracy: 0.9916\n",
      "Epoch 70: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.6291 - accuracy: 0.9916 - val_loss: 0.8968 - val_accuracy: 0.9190\n",
      "Epoch 71/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6024 - accuracy: 0.9948\n",
      "Epoch 71: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 9s 180ms/step - loss: 0.6024 - accuracy: 0.9948 - val_loss: 0.6896 - val_accuracy: 0.9772\n",
      "Epoch 72/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5874 - accuracy: 0.9968\n",
      "Epoch 72: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 0.5874 - accuracy: 0.9968 - val_loss: 0.6930 - val_accuracy: 0.9696\n",
      "Epoch 73/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5879 - accuracy: 0.9903\n",
      "Epoch 73: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 0.5879 - accuracy: 0.9903 - val_loss: 0.6905 - val_accuracy: 0.9646\n",
      "Epoch 74/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5716 - accuracy: 0.9942\n",
      "Epoch 74: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 0.5716 - accuracy: 0.9942 - val_loss: 0.6930 - val_accuracy: 0.9595\n",
      "Epoch 75/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5589 - accuracy: 0.9968\n",
      "Epoch 75: val_accuracy did not improve from 0.97722\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 0.5589 - accuracy: 0.9968 - val_loss: 0.7025 - val_accuracy: 0.9620\n",
      "Epoch 76/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5470 - accuracy: 0.9961\n",
      "Epoch 76: val_accuracy improved from 0.97722 to 0.97975, saving model to ./models/best_model_fold_3.h5\n",
      "49/49 [==============================] - 8s 174ms/step - loss: 0.5470 - accuracy: 0.9961 - val_loss: 0.6469 - val_accuracy: 0.9797\n",
      "Epoch 77/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5326 - accuracy: 0.9974\n",
      "Epoch 77: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 0.5326 - accuracy: 0.9974 - val_loss: 0.6546 - val_accuracy: 0.9620\n",
      "Epoch 78/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5196 - accuracy: 0.9981\n",
      "Epoch 78: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 0.5196 - accuracy: 0.9981 - val_loss: 0.6637 - val_accuracy: 0.9646\n",
      "Epoch 79/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5163 - accuracy: 0.9935\n",
      "Epoch 79: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 9s 176ms/step - loss: 0.5163 - accuracy: 0.9935 - val_loss: 0.6527 - val_accuracy: 0.9646\n",
      "Epoch 80/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5123 - accuracy: 0.9948\n",
      "Epoch 80: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 0.5123 - accuracy: 0.9948 - val_loss: 0.6777 - val_accuracy: 0.9443\n",
      "Epoch 81/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4940 - accuracy: 0.9961\n",
      "Epoch 81: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 0.4940 - accuracy: 0.9961 - val_loss: 0.6157 - val_accuracy: 0.9646\n",
      "Epoch 82/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4833 - accuracy: 0.9981\n",
      "Epoch 82: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 0.4833 - accuracy: 0.9981 - val_loss: 0.5990 - val_accuracy: 0.9747\n",
      "Epoch 83/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4753 - accuracy: 0.9961\n",
      "Epoch 83: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 9s 177ms/step - loss: 0.4753 - accuracy: 0.9961 - val_loss: 0.6109 - val_accuracy: 0.9595\n",
      "Epoch 84/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4679 - accuracy: 0.9955\n",
      "Epoch 84: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 9s 176ms/step - loss: 0.4679 - accuracy: 0.9955 - val_loss: 0.5793 - val_accuracy: 0.9671\n",
      "Epoch 85/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4550 - accuracy: 0.9987\n",
      "Epoch 85: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.4550 - accuracy: 0.9987 - val_loss: 0.5953 - val_accuracy: 0.9620\n",
      "Epoch 86/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4508 - accuracy: 0.9929\n",
      "Epoch 86: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 0.4508 - accuracy: 0.9929 - val_loss: 0.6031 - val_accuracy: 0.9544\n",
      "Epoch 87/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4394 - accuracy: 0.9968\n",
      "Epoch 87: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 0.4394 - accuracy: 0.9968 - val_loss: 0.5786 - val_accuracy: 0.9671\n",
      "Epoch 88/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4337 - accuracy: 0.9968\n",
      "Epoch 88: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 9s 177ms/step - loss: 0.4337 - accuracy: 0.9968 - val_loss: 0.5395 - val_accuracy: 0.9722\n",
      "Epoch 89/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4236 - accuracy: 0.9974\n",
      "Epoch 89: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.4236 - accuracy: 0.9974 - val_loss: 0.5565 - val_accuracy: 0.9595\n",
      "Epoch 90/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4258 - accuracy: 0.9942\n",
      "Epoch 90: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 0.4258 - accuracy: 0.9942 - val_loss: 0.5194 - val_accuracy: 0.9772\n",
      "Epoch 91/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4241 - accuracy: 0.9935\n",
      "Epoch 91: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 0.4241 - accuracy: 0.9935 - val_loss: 0.5568 - val_accuracy: 0.9544\n",
      "Epoch 92/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4066 - accuracy: 0.9929\n",
      "Epoch 92: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.4066 - accuracy: 0.9929 - val_loss: 0.5191 - val_accuracy: 0.9722\n",
      "Epoch 93/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4168 - accuracy: 0.9877\n",
      "Epoch 93: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 0.4168 - accuracy: 0.9877 - val_loss: 0.5541 - val_accuracy: 0.9646\n",
      "Epoch 94/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3955 - accuracy: 0.9942\n",
      "Epoch 94: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 0.3955 - accuracy: 0.9942 - val_loss: 0.5454 - val_accuracy: 0.9544\n",
      "Epoch 95/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3774 - accuracy: 0.9981\n",
      "Epoch 95: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 0.3774 - accuracy: 0.9981 - val_loss: 0.5707 - val_accuracy: 0.9544\n",
      "Epoch 96/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3760 - accuracy: 0.9961\n",
      "Epoch 96: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 0.3760 - accuracy: 0.9961 - val_loss: 0.5344 - val_accuracy: 0.9519\n",
      "Epoch 97/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3707 - accuracy: 0.9942Restoring model weights from the end of the best epoch: 92.\n",
      "\n",
      "Epoch 97: val_accuracy did not improve from 0.97975\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 0.3707 - accuracy: 0.9942 - val_loss: 0.5529 - val_accuracy: 0.9544\n",
      "Epoch 97: early stopping\n",
      "13/13 [==============================] - 2s 113ms/step\n",
      "Fold 3 Validation Accuracy: 0.9544\n",
      "Fold 3 Validation F1 Score: 0.9646\n",
      "Training fold 4/5\n",
      "Found 1579 validated image filenames belonging to 3 classes.\n",
      "Found 394 validated image filenames belonging to 3 classes.\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 3.0397 - accuracy: 0.6923\n",
      "Epoch 1: val_accuracy improved from -inf to 0.66751, saving model to ./models/best_model_fold_4.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 17s 210ms/step - loss: 3.0397 - accuracy: 0.6923 - val_loss: 3.0202 - val_accuracy: 0.6675\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.6231 - accuracy: 0.8714\n",
      "Epoch 2: val_accuracy improved from 0.66751 to 0.75381, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 2.6231 - accuracy: 0.8714 - val_loss: 2.8864 - val_accuracy: 0.7538\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.5180 - accuracy: 0.8908\n",
      "Epoch 3: val_accuracy improved from 0.75381 to 0.77919, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 2.5180 - accuracy: 0.8908 - val_loss: 2.7618 - val_accuracy: 0.7792\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3868 - accuracy: 0.9276\n",
      "Epoch 4: val_accuracy did not improve from 0.77919\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 2.3868 - accuracy: 0.9276 - val_loss: 2.8286 - val_accuracy: 0.7310\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3178 - accuracy: 0.9367\n",
      "Epoch 5: val_accuracy did not improve from 0.77919\n",
      "49/49 [==============================] - 8s 160ms/step - loss: 2.3178 - accuracy: 0.9367 - val_loss: 2.7418 - val_accuracy: 0.7741\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2725 - accuracy: 0.9373\n",
      "Epoch 6: val_accuracy improved from 0.77919 to 0.78426, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 2.2725 - accuracy: 0.9373 - val_loss: 2.7245 - val_accuracy: 0.7843\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1861 - accuracy: 0.9509\n",
      "Epoch 7: val_accuracy improved from 0.78426 to 0.79442, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 2.1861 - accuracy: 0.9509 - val_loss: 2.6366 - val_accuracy: 0.7944\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1236 - accuracy: 0.9632\n",
      "Epoch 8: val_accuracy improved from 0.79442 to 0.82487, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 2.1236 - accuracy: 0.9632 - val_loss: 2.5177 - val_accuracy: 0.8249\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0834 - accuracy: 0.9625\n",
      "Epoch 9: val_accuracy did not improve from 0.82487\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 2.0834 - accuracy: 0.9625 - val_loss: 2.5071 - val_accuracy: 0.8223\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0274 - accuracy: 0.9638\n",
      "Epoch 10: val_accuracy improved from 0.82487 to 0.85533, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 2.0274 - accuracy: 0.9638 - val_loss: 2.2964 - val_accuracy: 0.8553\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9859 - accuracy: 0.9722\n",
      "Epoch 11: val_accuracy did not improve from 0.85533\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 1.9859 - accuracy: 0.9722 - val_loss: 2.3461 - val_accuracy: 0.8401\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9404 - accuracy: 0.9735\n",
      "Epoch 12: val_accuracy improved from 0.85533 to 0.85787, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 1.9404 - accuracy: 0.9735 - val_loss: 2.2744 - val_accuracy: 0.8579\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8954 - accuracy: 0.9741\n",
      "Epoch 13: val_accuracy did not improve from 0.85787\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 1.8954 - accuracy: 0.9741 - val_loss: 2.3230 - val_accuracy: 0.8579\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8541 - accuracy: 0.9800\n",
      "Epoch 14: val_accuracy improved from 0.85787 to 0.87056, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 8s 174ms/step - loss: 1.8541 - accuracy: 0.9800 - val_loss: 2.2018 - val_accuracy: 0.8706\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8276 - accuracy: 0.9741\n",
      "Epoch 15: val_accuracy did not improve from 0.87056\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 1.8276 - accuracy: 0.9741 - val_loss: 2.1316 - val_accuracy: 0.8706\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7777 - accuracy: 0.9761\n",
      "Epoch 16: val_accuracy did not improve from 0.87056\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 1.7777 - accuracy: 0.9761 - val_loss: 2.0638 - val_accuracy: 0.8706\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7412 - accuracy: 0.9806\n",
      "Epoch 17: val_accuracy improved from 0.87056 to 0.88071, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 1.7412 - accuracy: 0.9806 - val_loss: 2.0204 - val_accuracy: 0.8807\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7022 - accuracy: 0.9845\n",
      "Epoch 18: val_accuracy improved from 0.88071 to 0.88832, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 1.7022 - accuracy: 0.9845 - val_loss: 2.0150 - val_accuracy: 0.8883\n",
      "Epoch 19/100\n",
      "48/49 [============================>.] - ETA: 0s - loss: 1.6709 - accuracy: 0.9837\n",
      "Epoch 19: val_accuracy improved from 0.88832 to 0.89086, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 1.6718 - accuracy: 0.9832 - val_loss: 1.9403 - val_accuracy: 0.8909\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.6526 - accuracy: 0.9806\n",
      "Epoch 20: val_accuracy improved from 0.89086 to 0.89848, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 1.6526 - accuracy: 0.9806 - val_loss: 1.8801 - val_accuracy: 0.8985\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.6128 - accuracy: 0.9825\n",
      "Epoch 21: val_accuracy did not improve from 0.89848\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 1.6128 - accuracy: 0.9825 - val_loss: 1.8601 - val_accuracy: 0.8959\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.5988 - accuracy: 0.9813\n",
      "Epoch 22: val_accuracy improved from 0.89848 to 0.90102, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 9s 179ms/step - loss: 1.5988 - accuracy: 0.9813 - val_loss: 1.8229 - val_accuracy: 0.9010\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.5485 - accuracy: 0.9851\n",
      "Epoch 23: val_accuracy improved from 0.90102 to 0.91371, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 1.5485 - accuracy: 0.9851 - val_loss: 1.7574 - val_accuracy: 0.9137\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.5036 - accuracy: 0.9903\n",
      "Epoch 24: val_accuracy did not improve from 0.91371\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 1.5036 - accuracy: 0.9903 - val_loss: 1.8097 - val_accuracy: 0.9061\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.4803 - accuracy: 0.9890\n",
      "Epoch 25: val_accuracy improved from 0.91371 to 0.92640, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 1.4803 - accuracy: 0.9890 - val_loss: 1.7044 - val_accuracy: 0.9264\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.4519 - accuracy: 0.9864\n",
      "Epoch 26: val_accuracy did not improve from 0.92640\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 1.4519 - accuracy: 0.9864 - val_loss: 1.7269 - val_accuracy: 0.9137\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.4343 - accuracy: 0.9864\n",
      "Epoch 27: val_accuracy did not improve from 0.92640\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 1.4343 - accuracy: 0.9864 - val_loss: 1.6443 - val_accuracy: 0.9162\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3933 - accuracy: 0.9890\n",
      "Epoch 28: val_accuracy did not improve from 0.92640\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 1.3933 - accuracy: 0.9890 - val_loss: 1.6713 - val_accuracy: 0.8959\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3648 - accuracy: 0.9910\n",
      "Epoch 29: val_accuracy did not improve from 0.92640\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 1.3648 - accuracy: 0.9910 - val_loss: 1.6461 - val_accuracy: 0.9162\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3381 - accuracy: 0.9916\n",
      "Epoch 30: val_accuracy did not improve from 0.92640\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 1.3381 - accuracy: 0.9916 - val_loss: 1.5409 - val_accuracy: 0.9264\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3121 - accuracy: 0.9890\n",
      "Epoch 31: val_accuracy did not improve from 0.92640\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 1.3121 - accuracy: 0.9890 - val_loss: 1.5518 - val_accuracy: 0.9137\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2883 - accuracy: 0.9858\n",
      "Epoch 32: val_accuracy did not improve from 0.92640\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 1.2883 - accuracy: 0.9858 - val_loss: 1.5526 - val_accuracy: 0.9112\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2609 - accuracy: 0.9916\n",
      "Epoch 33: val_accuracy did not improve from 0.92640\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 1.2609 - accuracy: 0.9916 - val_loss: 1.5369 - val_accuracy: 0.9137\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2366 - accuracy: 0.9916\n",
      "Epoch 34: val_accuracy did not improve from 0.92640\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 1.2366 - accuracy: 0.9916 - val_loss: 1.5735 - val_accuracy: 0.9036\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2073 - accuracy: 0.9935\n",
      "Epoch 35: val_accuracy did not improve from 0.92640\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 1.2073 - accuracy: 0.9935 - val_loss: 1.4882 - val_accuracy: 0.9213\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1840 - accuracy: 0.9935\n",
      "Epoch 36: val_accuracy did not improve from 0.92640\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 1.1840 - accuracy: 0.9935 - val_loss: 1.4834 - val_accuracy: 0.9061\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1598 - accuracy: 0.9916\n",
      "Epoch 37: val_accuracy did not improve from 0.92640\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 1.1598 - accuracy: 0.9916 - val_loss: 1.4361 - val_accuracy: 0.9213\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1404 - accuracy: 0.9935\n",
      "Epoch 38: val_accuracy did not improve from 0.92640\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 1.1404 - accuracy: 0.9935 - val_loss: 1.4501 - val_accuracy: 0.9162\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1296 - accuracy: 0.9922\n",
      "Epoch 39: val_accuracy did not improve from 0.92640\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 1.1296 - accuracy: 0.9922 - val_loss: 1.4653 - val_accuracy: 0.9188\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0965 - accuracy: 0.9942\n",
      "Epoch 40: val_accuracy improved from 0.92640 to 0.92893, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 1.0965 - accuracy: 0.9942 - val_loss: 1.3481 - val_accuracy: 0.9289\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0705 - accuracy: 0.9955\n",
      "Epoch 41: val_accuracy did not improve from 0.92893\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 1.0705 - accuracy: 0.9955 - val_loss: 1.3138 - val_accuracy: 0.9213\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0655 - accuracy: 0.9871\n",
      "Epoch 42: val_accuracy did not improve from 0.92893\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 1.0655 - accuracy: 0.9871 - val_loss: 1.3648 - val_accuracy: 0.9289\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0318 - accuracy: 0.9961\n",
      "Epoch 43: val_accuracy did not improve from 0.92893\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 1.0318 - accuracy: 0.9961 - val_loss: 1.3331 - val_accuracy: 0.9289\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0280 - accuracy: 0.9884\n",
      "Epoch 44: val_accuracy improved from 0.92893 to 0.93147, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 1.0280 - accuracy: 0.9884 - val_loss: 1.2736 - val_accuracy: 0.9315\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9970 - accuracy: 0.9916\n",
      "Epoch 45: val_accuracy improved from 0.93147 to 0.93401, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 9s 177ms/step - loss: 0.9970 - accuracy: 0.9916 - val_loss: 1.2493 - val_accuracy: 0.9340\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9716 - accuracy: 0.9955\n",
      "Epoch 46: val_accuracy did not improve from 0.93401\n",
      "49/49 [==============================] - 9s 176ms/step - loss: 0.9716 - accuracy: 0.9955 - val_loss: 1.2915 - val_accuracy: 0.9264\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9510 - accuracy: 0.9961\n",
      "Epoch 47: val_accuracy did not improve from 0.93401\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.9510 - accuracy: 0.9961 - val_loss: 1.3565 - val_accuracy: 0.9112\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9286 - accuracy: 0.9981\n",
      "Epoch 48: val_accuracy did not improve from 0.93401\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 0.9286 - accuracy: 0.9981 - val_loss: 1.2603 - val_accuracy: 0.9239\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9141 - accuracy: 0.9948\n",
      "Epoch 49: val_accuracy did not improve from 0.93401\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 0.9141 - accuracy: 0.9948 - val_loss: 1.1803 - val_accuracy: 0.9340\n",
      "Epoch 50/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9049 - accuracy: 0.9942\n",
      "Epoch 50: val_accuracy improved from 0.93401 to 0.93655, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 0.9049 - accuracy: 0.9942 - val_loss: 1.1788 - val_accuracy: 0.9365\n",
      "Epoch 51/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8809 - accuracy: 0.9981\n",
      "Epoch 51: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.8809 - accuracy: 0.9981 - val_loss: 1.2501 - val_accuracy: 0.9137\n",
      "Epoch 52/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8602 - accuracy: 0.9968\n",
      "Epoch 52: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.8602 - accuracy: 0.9968 - val_loss: 1.1564 - val_accuracy: 0.9112\n",
      "Epoch 53/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8466 - accuracy: 0.9948\n",
      "Epoch 53: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 9s 176ms/step - loss: 0.8466 - accuracy: 0.9948 - val_loss: 1.0934 - val_accuracy: 0.9289\n",
      "Epoch 54/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8319 - accuracy: 0.9935\n",
      "Epoch 54: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 0.8319 - accuracy: 0.9935 - val_loss: 1.1232 - val_accuracy: 0.9340\n",
      "Epoch 55/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8213 - accuracy: 0.9935\n",
      "Epoch 55: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 0.8213 - accuracy: 0.9935 - val_loss: 1.0776 - val_accuracy: 0.9239\n",
      "Epoch 56/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8051 - accuracy: 0.9942\n",
      "Epoch 56: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 0.8051 - accuracy: 0.9942 - val_loss: 1.0375 - val_accuracy: 0.9340\n",
      "Epoch 57/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7916 - accuracy: 0.9942\n",
      "Epoch 57: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 0.7916 - accuracy: 0.9942 - val_loss: 1.0813 - val_accuracy: 0.9213\n",
      "Epoch 58/100\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.7800 - accuracy: 0.9902\n",
      "Epoch 58: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 0.7818 - accuracy: 0.9897 - val_loss: 1.1071 - val_accuracy: 0.9315\n",
      "Epoch 59/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7865 - accuracy: 0.9858\n",
      "Epoch 59: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.7865 - accuracy: 0.9858 - val_loss: 1.0311 - val_accuracy: 0.9340\n",
      "Epoch 60/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7530 - accuracy: 0.9903\n",
      "Epoch 60: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 0.7530 - accuracy: 0.9903 - val_loss: 1.0355 - val_accuracy: 0.9289\n",
      "Epoch 61/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7369 - accuracy: 0.9890\n",
      "Epoch 61: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 0.7369 - accuracy: 0.9890 - val_loss: 1.0787 - val_accuracy: 0.9061\n",
      "Epoch 62/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7321 - accuracy: 0.9916\n",
      "Epoch 62: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 0.7321 - accuracy: 0.9916 - val_loss: 0.9669 - val_accuracy: 0.9137\n",
      "Epoch 63/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7062 - accuracy: 0.9948\n",
      "Epoch 63: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 0.7062 - accuracy: 0.9948 - val_loss: 0.9971 - val_accuracy: 0.9137\n",
      "Epoch 64/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.9935\n",
      "Epoch 64: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.6943 - accuracy: 0.9935 - val_loss: 0.9629 - val_accuracy: 0.9289\n",
      "Epoch 65/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6744 - accuracy: 0.9968\n",
      "Epoch 65: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.6744 - accuracy: 0.9968 - val_loss: 0.9582 - val_accuracy: 0.9239\n",
      "Epoch 66/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6744 - accuracy: 0.9922\n",
      "Epoch 66: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 0.6744 - accuracy: 0.9922 - val_loss: 0.9005 - val_accuracy: 0.9213\n",
      "Epoch 67/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6451 - accuracy: 0.9974\n",
      "Epoch 67: val_accuracy did not improve from 0.93655\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 0.6451 - accuracy: 0.9974 - val_loss: 0.9277 - val_accuracy: 0.9264\n",
      "Epoch 68/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6324 - accuracy: 0.9981\n",
      "Epoch 68: val_accuracy improved from 0.93655 to 0.94162, saving model to ./models/best_model_fold_4.h5\n",
      "49/49 [==============================] - 9s 176ms/step - loss: 0.6324 - accuracy: 0.9981 - val_loss: 0.8734 - val_accuracy: 0.9416\n",
      "Epoch 69/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6201 - accuracy: 0.9968\n",
      "Epoch 69: val_accuracy did not improve from 0.94162\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 0.6201 - accuracy: 0.9968 - val_loss: 0.8993 - val_accuracy: 0.9213\n",
      "Epoch 70/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6157 - accuracy: 0.9942\n",
      "Epoch 70: val_accuracy did not improve from 0.94162\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 0.6157 - accuracy: 0.9942 - val_loss: 0.9884 - val_accuracy: 0.9036\n",
      "Epoch 71/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6188 - accuracy: 0.9916\n",
      "Epoch 71: val_accuracy did not improve from 0.94162\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 0.6188 - accuracy: 0.9916 - val_loss: 0.9919 - val_accuracy: 0.8985\n",
      "Epoch 72/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6133 - accuracy: 0.9897\n",
      "Epoch 72: val_accuracy did not improve from 0.94162\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 0.6133 - accuracy: 0.9897 - val_loss: 0.9275 - val_accuracy: 0.9264\n",
      "Epoch 73/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5844 - accuracy: 0.9916Restoring model weights from the end of the best epoch: 68.\n",
      "\n",
      "Epoch 73: val_accuracy did not improve from 0.94162\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 0.5844 - accuracy: 0.9916 - val_loss: 0.9294 - val_accuracy: 0.9264\n",
      "Epoch 73: early stopping\n",
      "13/13 [==============================] - 2s 127ms/step\n",
      "Fold 4 Validation Accuracy: 0.9264\n",
      "Fold 4 Validation F1 Score: 0.9339\n",
      "Training fold 5/5\n",
      "Found 1579 validated image filenames belonging to 3 classes.\n",
      "Found 394 validated image filenames belonging to 3 classes.\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 3.0254 - accuracy: 0.6988\n",
      "Epoch 1: val_accuracy improved from -inf to 0.65990, saving model to ./models/best_model_fold_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 16s 179ms/step - loss: 3.0254 - accuracy: 0.6988 - val_loss: 3.0645 - val_accuracy: 0.6599\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.6088 - accuracy: 0.8759\n",
      "Epoch 2: val_accuracy did not improve from 0.65990\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 2.6088 - accuracy: 0.8759 - val_loss: 3.0721 - val_accuracy: 0.6396\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4789 - accuracy: 0.8979\n",
      "Epoch 3: val_accuracy did not improve from 0.65990\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 2.4789 - accuracy: 0.8979 - val_loss: 3.7262 - val_accuracy: 0.5533\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3994 - accuracy: 0.9224\n",
      "Epoch 4: val_accuracy did not improve from 0.65990\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 2.3994 - accuracy: 0.9224 - val_loss: 3.2842 - val_accuracy: 0.6320\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.3100 - accuracy: 0.9373\n",
      "Epoch 5: val_accuracy improved from 0.65990 to 0.70812, saving model to ./models/best_model_fold_5.h5\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 2.3100 - accuracy: 0.9373 - val_loss: 2.8914 - val_accuracy: 0.7081\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.2575 - accuracy: 0.9405\n",
      "Epoch 6: val_accuracy did not improve from 0.70812\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 2.2575 - accuracy: 0.9405 - val_loss: 3.1952 - val_accuracy: 0.6650\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1822 - accuracy: 0.9567\n",
      "Epoch 7: val_accuracy did not improve from 0.70812\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 2.1822 - accuracy: 0.9567 - val_loss: 3.1117 - val_accuracy: 0.6777\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1147 - accuracy: 0.9625\n",
      "Epoch 8: val_accuracy did not improve from 0.70812\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 2.1147 - accuracy: 0.9625 - val_loss: 3.1108 - val_accuracy: 0.6904\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0916 - accuracy: 0.9554\n",
      "Epoch 9: val_accuracy improved from 0.70812 to 0.75381, saving model to ./models/best_model_fold_5.h5\n",
      "49/49 [==============================] - 9s 181ms/step - loss: 2.0916 - accuracy: 0.9554 - val_loss: 2.7744 - val_accuracy: 0.7538\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0389 - accuracy: 0.9644\n",
      "Epoch 10: val_accuracy improved from 0.75381 to 0.79442, saving model to ./models/best_model_fold_5.h5\n",
      "49/49 [==============================] - 9s 178ms/step - loss: 2.0389 - accuracy: 0.9644 - val_loss: 2.5704 - val_accuracy: 0.7944\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0053 - accuracy: 0.9638\n",
      "Epoch 11: val_accuracy improved from 0.79442 to 0.81980, saving model to ./models/best_model_fold_5.h5\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 2.0053 - accuracy: 0.9638 - val_loss: 2.4276 - val_accuracy: 0.8198\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.9407 - accuracy: 0.9716\n",
      "Epoch 12: val_accuracy improved from 0.81980 to 0.85787, saving model to ./models/best_model_fold_5.h5\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 1.9407 - accuracy: 0.9716 - val_loss: 2.2413 - val_accuracy: 0.8579\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8947 - accuracy: 0.9735\n",
      "Epoch 13: val_accuracy improved from 0.85787 to 0.89340, saving model to ./models/best_model_fold_5.h5\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 1.8947 - accuracy: 0.9735 - val_loss: 2.0777 - val_accuracy: 0.8934\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8675 - accuracy: 0.9729\n",
      "Epoch 14: val_accuracy did not improve from 0.89340\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 1.8675 - accuracy: 0.9729 - val_loss: 2.1024 - val_accuracy: 0.8629\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.8048 - accuracy: 0.9838\n",
      "Epoch 15: val_accuracy did not improve from 0.89340\n",
      "49/49 [==============================] - 8s 160ms/step - loss: 1.8048 - accuracy: 0.9838 - val_loss: 2.2031 - val_accuracy: 0.8452\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7784 - accuracy: 0.9787\n",
      "Epoch 16: val_accuracy did not improve from 0.89340\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 1.7784 - accuracy: 0.9787 - val_loss: 2.0230 - val_accuracy: 0.8807\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7586 - accuracy: 0.9761\n",
      "Epoch 17: val_accuracy did not improve from 0.89340\n",
      "49/49 [==============================] - 8s 158ms/step - loss: 1.7586 - accuracy: 0.9761 - val_loss: 2.0672 - val_accuracy: 0.8756\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.7136 - accuracy: 0.9774\n",
      "Epoch 18: val_accuracy did not improve from 0.89340\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 1.7136 - accuracy: 0.9774 - val_loss: 1.9612 - val_accuracy: 0.8934\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.6826 - accuracy: 0.9787\n",
      "Epoch 19: val_accuracy did not improve from 0.89340\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 1.6826 - accuracy: 0.9787 - val_loss: 1.9178 - val_accuracy: 0.8934\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.6233 - accuracy: 0.9884\n",
      "Epoch 20: val_accuracy improved from 0.89340 to 0.90355, saving model to ./models/best_model_fold_5.h5\n",
      "49/49 [==============================] - 9s 176ms/step - loss: 1.6233 - accuracy: 0.9884 - val_loss: 1.8276 - val_accuracy: 0.9036\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.6034 - accuracy: 0.9858\n",
      "Epoch 21: val_accuracy did not improve from 0.90355\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 1.6034 - accuracy: 0.9858 - val_loss: 1.8461 - val_accuracy: 0.8807\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.5730 - accuracy: 0.9864\n",
      "Epoch 22: val_accuracy improved from 0.90355 to 0.91117, saving model to ./models/best_model_fold_5.h5\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 1.5730 - accuracy: 0.9864 - val_loss: 1.7550 - val_accuracy: 0.9112\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.5340 - accuracy: 0.9877\n",
      "Epoch 23: val_accuracy improved from 0.91117 to 0.91371, saving model to ./models/best_model_fold_5.h5\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 1.5340 - accuracy: 0.9877 - val_loss: 1.7479 - val_accuracy: 0.9137\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.5211 - accuracy: 0.9845\n",
      "Epoch 24: val_accuracy improved from 0.91371 to 0.93909, saving model to ./models/best_model_fold_5.h5\n",
      "49/49 [==============================] - 9s 179ms/step - loss: 1.5211 - accuracy: 0.9845 - val_loss: 1.6353 - val_accuracy: 0.9391\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.4824 - accuracy: 0.9819\n",
      "Epoch 25: val_accuracy did not improve from 0.93909\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 1.4824 - accuracy: 0.9819 - val_loss: 1.6271 - val_accuracy: 0.9365\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.4597 - accuracy: 0.9851\n",
      "Epoch 26: val_accuracy did not improve from 0.93909\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 1.4597 - accuracy: 0.9851 - val_loss: 1.6428 - val_accuracy: 0.9112\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.4186 - accuracy: 0.9884\n",
      "Epoch 27: val_accuracy did not improve from 0.93909\n",
      "49/49 [==============================] - 9s 178ms/step - loss: 1.4186 - accuracy: 0.9884 - val_loss: 1.5541 - val_accuracy: 0.9391\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3985 - accuracy: 0.9858\n",
      "Epoch 28: val_accuracy did not improve from 0.93909\n",
      "49/49 [==============================] - 8s 159ms/step - loss: 1.3985 - accuracy: 0.9858 - val_loss: 1.5422 - val_accuracy: 0.9213\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3724 - accuracy: 0.9884\n",
      "Epoch 29: val_accuracy did not improve from 0.93909\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 1.3724 - accuracy: 0.9884 - val_loss: 1.4798 - val_accuracy: 0.9365\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3357 - accuracy: 0.9890\n",
      "Epoch 30: val_accuracy improved from 0.93909 to 0.95685, saving model to ./models/best_model_fold_5.h5\n",
      "49/49 [==============================] - 9s 181ms/step - loss: 1.3357 - accuracy: 0.9890 - val_loss: 1.4305 - val_accuracy: 0.9569\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3067 - accuracy: 0.9903\n",
      "Epoch 31: val_accuracy did not improve from 0.95685\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 1.3067 - accuracy: 0.9903 - val_loss: 1.4041 - val_accuracy: 0.9442\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2717 - accuracy: 0.9968\n",
      "Epoch 32: val_accuracy improved from 0.95685 to 0.96447, saving model to ./models/best_model_fold_5.h5\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 1.2717 - accuracy: 0.9968 - val_loss: 1.3754 - val_accuracy: 0.9645\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2613 - accuracy: 0.9916\n",
      "Epoch 33: val_accuracy did not improve from 0.96447\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 1.2613 - accuracy: 0.9916 - val_loss: 1.3613 - val_accuracy: 0.9645\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2327 - accuracy: 0.9916\n",
      "Epoch 34: val_accuracy did not improve from 0.96447\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 1.2327 - accuracy: 0.9916 - val_loss: 1.3522 - val_accuracy: 0.9594\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2037 - accuracy: 0.9929\n",
      "Epoch 35: val_accuracy did not improve from 0.96447\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 1.2037 - accuracy: 0.9929 - val_loss: 1.2908 - val_accuracy: 0.9619\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1969 - accuracy: 0.9877\n",
      "Epoch 36: val_accuracy did not improve from 0.96447\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 1.1969 - accuracy: 0.9877 - val_loss: 1.3148 - val_accuracy: 0.9467\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1757 - accuracy: 0.9884\n",
      "Epoch 37: val_accuracy did not improve from 0.96447\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 1.1757 - accuracy: 0.9884 - val_loss: 1.2591 - val_accuracy: 0.9492\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1575 - accuracy: 0.9871\n",
      "Epoch 38: val_accuracy did not improve from 0.96447\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 1.1575 - accuracy: 0.9871 - val_loss: 1.2602 - val_accuracy: 0.9467\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1125 - accuracy: 0.9929\n",
      "Epoch 39: val_accuracy did not improve from 0.96447\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 1.1125 - accuracy: 0.9929 - val_loss: 1.2209 - val_accuracy: 0.9518\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0941 - accuracy: 0.9942\n",
      "Epoch 40: val_accuracy did not improve from 0.96447\n",
      "49/49 [==============================] - 8s 161ms/step - loss: 1.0941 - accuracy: 0.9942 - val_loss: 1.1793 - val_accuracy: 0.9619\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0691 - accuracy: 0.9942\n",
      "Epoch 41: val_accuracy improved from 0.96447 to 0.97716, saving model to ./models/best_model_fold_5.h5\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 1.0691 - accuracy: 0.9942 - val_loss: 1.1432 - val_accuracy: 0.9772\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0515 - accuracy: 0.9935\n",
      "Epoch 42: val_accuracy did not improve from 0.97716\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 1.0515 - accuracy: 0.9935 - val_loss: 1.1340 - val_accuracy: 0.9670\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0361 - accuracy: 0.9942\n",
      "Epoch 43: val_accuracy did not improve from 0.97716\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 1.0361 - accuracy: 0.9942 - val_loss: 1.0933 - val_accuracy: 0.9721\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0095 - accuracy: 0.9929\n",
      "Epoch 44: val_accuracy did not improve from 0.97716\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 1.0095 - accuracy: 0.9929 - val_loss: 1.1078 - val_accuracy: 0.9619\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9996 - accuracy: 0.9922\n",
      "Epoch 45: val_accuracy did not improve from 0.97716\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.9996 - accuracy: 0.9922 - val_loss: 1.0969 - val_accuracy: 0.9594\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9739 - accuracy: 0.9961\n",
      "Epoch 46: val_accuracy did not improve from 0.97716\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 0.9739 - accuracy: 0.9961 - val_loss: 1.0822 - val_accuracy: 0.9492\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9487 - accuracy: 0.9942\n",
      "Epoch 47: val_accuracy did not improve from 0.97716\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 0.9487 - accuracy: 0.9942 - val_loss: 1.0447 - val_accuracy: 0.9772\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9327 - accuracy: 0.9955\n",
      "Epoch 48: val_accuracy did not improve from 0.97716\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 0.9327 - accuracy: 0.9955 - val_loss: 1.0296 - val_accuracy: 0.9670\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.9074 - accuracy: 0.9981\n",
      "Epoch 49: val_accuracy improved from 0.97716 to 0.97970, saving model to ./models/best_model_fold_5.h5\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 0.9074 - accuracy: 0.9981 - val_loss: 0.9865 - val_accuracy: 0.9797\n",
      "Epoch 50/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8981 - accuracy: 0.9929\n",
      "Epoch 50: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 160ms/step - loss: 0.8981 - accuracy: 0.9929 - val_loss: 1.0488 - val_accuracy: 0.9594\n",
      "Epoch 51/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8890 - accuracy: 0.9910\n",
      "Epoch 51: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 0.8890 - accuracy: 0.9910 - val_loss: 0.9878 - val_accuracy: 0.9645\n",
      "Epoch 52/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8672 - accuracy: 0.9948\n",
      "Epoch 52: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 0.8672 - accuracy: 0.9948 - val_loss: 0.9930 - val_accuracy: 0.9619\n",
      "Epoch 53/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8453 - accuracy: 0.9968\n",
      "Epoch 53: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 171ms/step - loss: 0.8453 - accuracy: 0.9968 - val_loss: 0.9588 - val_accuracy: 0.9695\n",
      "Epoch 54/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8331 - accuracy: 0.9948\n",
      "Epoch 54: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 0.8331 - accuracy: 0.9948 - val_loss: 0.9270 - val_accuracy: 0.9645\n",
      "Epoch 55/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.8112 - accuracy: 0.9974\n",
      "Epoch 55: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 162ms/step - loss: 0.8112 - accuracy: 0.9974 - val_loss: 0.9140 - val_accuracy: 0.9746\n",
      "Epoch 56/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7932 - accuracy: 0.9961\n",
      "Epoch 56: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 0.7932 - accuracy: 0.9961 - val_loss: 0.9162 - val_accuracy: 0.9670\n",
      "Epoch 57/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7789 - accuracy: 0.9961\n",
      "Epoch 57: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 0.7789 - accuracy: 0.9961 - val_loss: 0.8517 - val_accuracy: 0.9746\n",
      "Epoch 58/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7630 - accuracy: 0.9974\n",
      "Epoch 58: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 0.7630 - accuracy: 0.9974 - val_loss: 0.8956 - val_accuracy: 0.9569\n",
      "Epoch 59/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7591 - accuracy: 0.9910\n",
      "Epoch 59: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 160ms/step - loss: 0.7591 - accuracy: 0.9910 - val_loss: 0.8662 - val_accuracy: 0.9695\n",
      "Epoch 60/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7462 - accuracy: 0.9935\n",
      "Epoch 60: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.7462 - accuracy: 0.9935 - val_loss: 0.8434 - val_accuracy: 0.9594\n",
      "Epoch 61/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7452 - accuracy: 0.9922\n",
      "Epoch 61: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 0.7452 - accuracy: 0.9922 - val_loss: 0.8793 - val_accuracy: 0.9365\n",
      "Epoch 62/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7283 - accuracy: 0.9890\n",
      "Epoch 62: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 0.7283 - accuracy: 0.9890 - val_loss: 0.8411 - val_accuracy: 0.9416\n",
      "Epoch 63/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7014 - accuracy: 0.9935\n",
      "Epoch 63: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 0.7014 - accuracy: 0.9935 - val_loss: 0.7998 - val_accuracy: 0.9569\n",
      "Epoch 64/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6961 - accuracy: 0.9922\n",
      "Epoch 64: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 0.6961 - accuracy: 0.9922 - val_loss: 0.7747 - val_accuracy: 0.9645\n",
      "Epoch 65/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6737 - accuracy: 0.9935\n",
      "Epoch 65: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 0.6737 - accuracy: 0.9935 - val_loss: 0.7345 - val_accuracy: 0.9721\n",
      "Epoch 66/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6543 - accuracy: 0.9974\n",
      "Epoch 66: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 0.6543 - accuracy: 0.9974 - val_loss: 0.7708 - val_accuracy: 0.9645\n",
      "Epoch 67/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6416 - accuracy: 0.9987\n",
      "Epoch 67: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 0.6416 - accuracy: 0.9987 - val_loss: 0.7142 - val_accuracy: 0.9721\n",
      "Epoch 68/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6256 - accuracy: 1.0000\n",
      "Epoch 68: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 0.6256 - accuracy: 1.0000 - val_loss: 0.7149 - val_accuracy: 0.9746\n",
      "Epoch 69/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6164 - accuracy: 0.9968\n",
      "Epoch 69: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 0.6164 - accuracy: 0.9968 - val_loss: 0.7243 - val_accuracy: 0.9695\n",
      "Epoch 70/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6081 - accuracy: 0.9955\n",
      "Epoch 70: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 0.6081 - accuracy: 0.9955 - val_loss: 0.7144 - val_accuracy: 0.9619\n",
      "Epoch 71/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6281 - accuracy: 0.9884\n",
      "Epoch 71: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 158ms/step - loss: 0.6281 - accuracy: 0.9884 - val_loss: 0.7436 - val_accuracy: 0.9543\n",
      "Epoch 72/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5965 - accuracy: 0.9942\n",
      "Epoch 72: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 0.5965 - accuracy: 0.9942 - val_loss: 0.7067 - val_accuracy: 0.9594\n",
      "Epoch 73/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5730 - accuracy: 0.9974\n",
      "Epoch 73: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 0.5730 - accuracy: 0.9974 - val_loss: 0.6620 - val_accuracy: 0.9670\n",
      "Epoch 74/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5604 - accuracy: 0.9974\n",
      "Epoch 74: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 0.5604 - accuracy: 0.9974 - val_loss: 0.6625 - val_accuracy: 0.9670\n",
      "Epoch 75/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5529 - accuracy: 0.9961\n",
      "Epoch 75: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 0.5529 - accuracy: 0.9961 - val_loss: 0.6587 - val_accuracy: 0.9695\n",
      "Epoch 76/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5524 - accuracy: 0.9948\n",
      "Epoch 76: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 0.5524 - accuracy: 0.9948 - val_loss: 0.6829 - val_accuracy: 0.9619\n",
      "Epoch 77/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5316 - accuracy: 0.9968\n",
      "Epoch 77: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 0.5316 - accuracy: 0.9968 - val_loss: 0.6474 - val_accuracy: 0.9670\n",
      "Epoch 78/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5242 - accuracy: 0.9968\n",
      "Epoch 78: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 0.5242 - accuracy: 0.9968 - val_loss: 0.6097 - val_accuracy: 0.9721\n",
      "Epoch 79/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5100 - accuracy: 0.9961\n",
      "Epoch 79: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.5100 - accuracy: 0.9961 - val_loss: 0.5819 - val_accuracy: 0.9721\n",
      "Epoch 80/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5051 - accuracy: 0.9968\n",
      "Epoch 80: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.5051 - accuracy: 0.9968 - val_loss: 0.5960 - val_accuracy: 0.9695\n",
      "Epoch 81/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4900 - accuracy: 0.9974\n",
      "Epoch 81: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 0.4900 - accuracy: 0.9974 - val_loss: 0.5782 - val_accuracy: 0.9721\n",
      "Epoch 82/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4859 - accuracy: 0.9955\n",
      "Epoch 82: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 0.4859 - accuracy: 0.9955 - val_loss: 0.5797 - val_accuracy: 0.9645\n",
      "Epoch 83/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.9987\n",
      "Epoch 83: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.4749 - accuracy: 0.9987 - val_loss: 0.5810 - val_accuracy: 0.9746\n",
      "Epoch 84/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4589 - accuracy: 0.9987\n",
      "Epoch 84: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 0.4589 - accuracy: 0.9987 - val_loss: 0.5687 - val_accuracy: 0.9695\n",
      "Epoch 85/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4553 - accuracy: 0.9974\n",
      "Epoch 85: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 0.4553 - accuracy: 0.9974 - val_loss: 0.5599 - val_accuracy: 0.9746\n",
      "Epoch 86/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4488 - accuracy: 0.9968\n",
      "Epoch 86: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 168ms/step - loss: 0.4488 - accuracy: 0.9968 - val_loss: 0.5230 - val_accuracy: 0.9695\n",
      "Epoch 87/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4372 - accuracy: 0.9974\n",
      "Epoch 87: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 172ms/step - loss: 0.4372 - accuracy: 0.9974 - val_loss: 0.5708 - val_accuracy: 0.9569\n",
      "Epoch 88/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4248 - accuracy: 0.9994\n",
      "Epoch 88: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 167ms/step - loss: 0.4248 - accuracy: 0.9994 - val_loss: 0.5477 - val_accuracy: 0.9746\n",
      "Epoch 89/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4206 - accuracy: 0.9961\n",
      "Epoch 89: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.4206 - accuracy: 0.9961 - val_loss: 0.5461 - val_accuracy: 0.9645\n",
      "Epoch 90/100\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.4093 - accuracy: 0.9987\n",
      "Epoch 90: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 165ms/step - loss: 0.4137 - accuracy: 0.9981 - val_loss: 0.6267 - val_accuracy: 0.9543\n",
      "Epoch 91/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4203 - accuracy: 0.9922\n",
      "Epoch 91: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 169ms/step - loss: 0.4203 - accuracy: 0.9922 - val_loss: 0.4923 - val_accuracy: 0.9746\n",
      "Epoch 92/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3971 - accuracy: 0.9981\n",
      "Epoch 92: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 173ms/step - loss: 0.3971 - accuracy: 0.9981 - val_loss: 0.5231 - val_accuracy: 0.9594\n",
      "Epoch 93/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3854 - accuracy: 0.9994\n",
      "Epoch 93: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.3854 - accuracy: 0.9994 - val_loss: 0.4657 - val_accuracy: 0.9746\n",
      "Epoch 94/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3868 - accuracy: 0.9968\n",
      "Epoch 94: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 163ms/step - loss: 0.3868 - accuracy: 0.9968 - val_loss: 0.5062 - val_accuracy: 0.9670\n",
      "Epoch 95/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3697 - accuracy: 0.9994\n",
      "Epoch 95: val_accuracy did not improve from 0.97970\n",
      "49/49 [==============================] - 8s 164ms/step - loss: 0.3697 - accuracy: 0.9994 - val_loss: 0.4719 - val_accuracy: 0.9670\n",
      "Epoch 96/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3765 - accuracy: 0.9942\n",
      "Epoch 96: val_accuracy improved from 0.97970 to 0.98223, saving model to ./models/best_model_fold_5.h5\n",
      "49/49 [==============================] - 9s 182ms/step - loss: 0.3765 - accuracy: 0.9942 - val_loss: 0.4433 - val_accuracy: 0.9822\n",
      "Epoch 97/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3582 - accuracy: 0.9987\n",
      "Epoch 97: val_accuracy did not improve from 0.98223\n",
      "49/49 [==============================] - 8s 170ms/step - loss: 0.3582 - accuracy: 0.9987 - val_loss: 0.4609 - val_accuracy: 0.9772\n",
      "Epoch 98/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3562 - accuracy: 0.9981\n",
      "Epoch 98: val_accuracy did not improve from 0.98223\n",
      "49/49 [==============================] - 9s 175ms/step - loss: 0.3562 - accuracy: 0.9981 - val_loss: 0.4439 - val_accuracy: 0.9772\n",
      "Epoch 99/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3542 - accuracy: 0.9942\n",
      "Epoch 99: val_accuracy did not improve from 0.98223\n",
      "49/49 [==============================] - 8s 166ms/step - loss: 0.3542 - accuracy: 0.9942 - val_loss: 0.4689 - val_accuracy: 0.9670\n",
      "Epoch 100/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3579 - accuracy: 0.9929\n",
      "Epoch 100: val_accuracy did not improve from 0.98223\n",
      "49/49 [==============================] - 9s 174ms/step - loss: 0.3579 - accuracy: 0.9929 - val_loss: 0.5778 - val_accuracy: 0.9569\n",
      "13/13 [==============================] - 2s 122ms/step\n",
      "Fold 5 Validation Accuracy: 0.9569\n",
      "Fold 5 Validation F1 Score: 0.9483\n",
      "\n",
      "Average Validation Accuracy across all folds: 0.9473\n",
      "Average Validation F1 Score across all folds: 0.9536\n",
      "\n",
      "Best Fold: 2 with Validation Accuracy: 0.9570\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_index, test_index) in enumerate(kfold.split(image_dataframe)):\n",
    "    print(f\"Training fold {fold_idx + 1}/{num_folds}\")\n",
    "\n",
    "    train_data = image_dataframe.iloc[train_index]\n",
    "    test_data = image_dataframe.iloc[test_index]\n",
    "\n",
    "    # Data Generators\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_data,\n",
    "        x_col='file_path',\n",
    "        y_col='label',\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,  # Increased batch size\n",
    "        class_mode='categorical',\n",
    "        shuffle=True,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=test_data,\n",
    "        x_col='file_path',\n",
    "        y_col='label',\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "\n",
    "    # 1. Define the Input layer\n",
    "    input_layer = Input(shape=(img_width, img_height, 3))\n",
    "    \n",
    "    # 2. Load the pre-trained MobileNetV2 model without the top classification layers\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=input_layer, input_shape=(img_width, img_height, 3))\n",
    "    \n",
    "    # Set all layers in the base model to trainable\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # 4. Add custom layers on top of the base_model\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # Removed activation from here\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)  # Added Activation layer separately\n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # 5. Create the Functional API model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    # 6. Compile the model with the SGD optimizer\n",
    "    opt = SGD(learning_rate=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    # 7. Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "        # ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1),\n",
    "        ModelCheckpoint(\n",
    "            filepath=f'./models/best_model_fold_{fold_idx + 1}.h5',\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        TensorBoard(log_dir=f'./logs/fold_{fold_idx + 1}')\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "        epochs=100,\n",
    "        validation_data=test_generator,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "       # 9. Evaluation\n",
    "    y_true = test_generator.classes\n",
    "    y_pred_probs = model.predict(test_generator, verbose=1)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    fold_accuracies.append(accuracy)\n",
    "    print(f\"Fold {fold_idx + 1} Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    fold_f1_scores.append(f1)\n",
    "    print(f\"Fold {fold_idx + 1} Validation F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # 10. Plot and Save Loss Curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss Curve - Fold {fold_idx + 1}')\n",
    "    plt.legend()\n",
    "    loss_curve_path = f'./plots/loss/MNV2_loss_curve_fold_{fold_idx + 1}.png'\n",
    "    plt.savefig(loss_curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 11. Plot and Save Accuracy Curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy', color='green')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Accuracy Curve - Fold {fold_idx + 1}')\n",
    "    plt.legend()\n",
    "    accuracy_curve_path = f'./plots/accuracy/MNV2_accuracy_curve_fold_{fold_idx + 1}.png'\n",
    "    plt.savefig(accuracy_curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 12. Track the Best Fold\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_fold_idx = fold_idx\n",
    "        best_test_data = test_data.copy()\n",
    "        best_model_path = f'./models/best_overall_model_fold_{fold_idx + 1}.h5'\n",
    "        model.save(best_model_path)\n",
    "        print(f\"New best model saved for fold {fold_idx + 1} with accuracy {accuracy:.4f}\")\n",
    "\n",
    "# 13. Final Evaluation Across Folds\n",
    "avg_accuracy = np.mean(fold_accuracies)\n",
    "avg_f1 = np.mean(fold_f1_scores)\n",
    "print(f\"\\nAverage Validation Accuracy across all folds: {avg_accuracy:.4f}\")\n",
    "print(f\"Average Validation F1 Score across all folds: {avg_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Fold: {best_fold_idx + 1} with Validation Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 0.9417721629142761\n",
      "Fold 1 F1 Score: 0.9544740005472949\n",
      "Fold 2 Accuracy: 0.9569620490074158\n",
      "Fold 2 F1 Score: 0.9669836635699194\n",
      "Fold 3 Accuracy: 0.9544304013252258\n",
      "Fold 3 F1 Score: 0.9645949258048444\n",
      "Fold 4 Accuracy: 0.9263959527015686\n",
      "Fold 4 F1 Score: 0.9339205140601081\n",
      "Fold 5 Accuracy: 0.9568527936935425\n",
      "Fold 5 F1 Score: 0.948266342801424\n",
      "Average Test accuracy: 0.9472826719284058\n",
      "Standard Deviation of Test accuracy: 0.011857388889826518\n",
      "Average F1 Score: 0.9536478893567182\n"
     ]
    }
   ],
   "source": [
    "avg_test_acc = np.mean(fold_accuracies)\n",
    "std_test_acc = np.std(fold_accuracies)\n",
    "avg_f1_score = np.mean(fold_f1_scores)\n",
    "\n",
    "for fold_idx in range(num_folds):\n",
    "    print(f\"Fold {fold_idx + 1} Accuracy: {fold_accuracies[fold_idx]}\")\n",
    "    print(f\"Fold {fold_idx + 1} F1 Score: {fold_f1_scores[fold_idx]}\")\n",
    "    \n",
    "print(\"Average Test accuracy:\", avg_test_acc)\n",
    "print(\"Standard Deviation of Test accuracy:\", std_test_acc)\n",
    "print(\"Average F1 Score:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = r'/home/sec_team2/Engagement/model_MNV2_fold2.keras'\n",
    "# model.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Best Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best fold 2 with accuracy 0.9569620490074158 saved as validation data.\n",
      "Best model saved at ./models/best_overall_model_fold_2.h5\n"
     ]
    }
   ],
   "source": [
    "# After all folds, save the best test fold to a file\n",
    "if best_test_data is not None:\n",
    "    best_test_data.to_csv(f'best_fold_{best_fold_idx + 1}_validation_data.csv', index=False)\n",
    "    print(f\"Best fold {best_fold_idx + 1} with accuracy {best_accuracy} saved as validation data.\")\n",
    "    print(f\"Best model saved at {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing model on exported test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test data from best_fold_2_validation_data.csv\n",
      "                        file_path  label\n",
      "0  ./Dataset/paper/164317_071.jpg  paper\n",
      "1  ./Dataset/paper/164317_086.jpg  paper\n",
      "2  ./Dataset/paper/164325_061.jpg  paper\n",
      "3  ./Dataset/paper/164390_009.jpg  paper\n",
      "4  ./Dataset/paper/164397_022.jpg  paper\n"
     ]
    }
   ],
   "source": [
    "# Load the exported test set\n",
    "test_set_path = f'best_fold_{best_fold_idx + 1}_validation_data.csv'\n",
    "test_df = pd.read_csv(test_set_path)\n",
    "print(f\"Loaded test data from {test_set_path}\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 395 validated image filenames belonging to 3 classes.\n",
      "Loaded best model from: ./models/best_overall_model_fold_2.h5\n",
      "25/25 [==============================] - 2s 64ms/step - loss: 0.6304 - accuracy: 0.9494\n",
      "Test Loss: 0.6304085850715637\n",
      "Test Accuracy: 0.949367105960846\n",
      "25/25 [==============================] - 2s 62ms/step\n",
      "Test F1 Score: 0.9692321087603446\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       paper       0.95      1.00      0.97       132\n",
      "      screen       0.98      0.99      0.98       164\n",
      "      wander       0.99      0.90      0.94        99\n",
      "\n",
      "    accuracy                           0.97       395\n",
      "   macro avg       0.97      0.96      0.97       395\n",
      "weighted avg       0.97      0.97      0.97       395\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure the same preprocessing pipeline is applied to the test set\n",
    "test_generator = CustomImageDataGenerator().flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='file_path',\n",
    "    y_col='label',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Don't shuffle for evaluation\n",
    ")\n",
    "\n",
    "# Step 3: Load the best saved model\n",
    "best_model = load_model(f'./models/best_overall_model_fold_2.h5')\n",
    "# best_model_path = f'./models/best_overall_model_fold_{fold_idx + 1}.h5'\n",
    "print(f\"Loaded best model from: {best_model_path}\")\n",
    "\n",
    "# best_model_path = f'best_model_fold_{best_fold_idx + 1}.h5'  # Adjust if necessary\n",
    "# best_model = load_model(best_model_path)\n",
    "# print(f\"Loaded best model from: {best_model_path}\")\n",
    "\n",
    "# Step 4: Evaluate the model on the test set\n",
    "results = best_model.evaluate(test_generator, verbose=1)\n",
    "test_loss, test_accuracy = results[0], results[1]\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Step 5: Get predictions\n",
    "y_pred_probs = best_model.predict(test_generator, verbose=1)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Convert probabilities to class predictions\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Step 6: Calculate Metrics\n",
    "# F1 Score\n",
    "test_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"Test F1 Score: {test_f1}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=test_generator.class_indices.keys(),\n",
    "            yticklabels=test_generator.class_indices.keys())\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT training as finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Secondary K-Fold Loop for QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize K-Fold (Same as Primary Loop)\n",
    "kfold_qat = KFold(n_splits=num_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "# 2. Initialize Tracking Variables for QAT\n",
    "qat_fold_accuracies = []\n",
    "qat_fold_f1_scores = []\n",
    "best_qat_accuracy = 0\n",
    "best_qat_fold_idx = -1\n",
    "\n",
    "# 3. Define QAT Parameters\n",
    "QAT_EPOCHS = 20  # Fine-tuning epochs after QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying QAT on fold 1/5\n",
      "Found 1578 validated image filenames belonging to 3 classes.\n",
      "Found 395 validated image filenames belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model for fold 1.\n",
      "Applied QAT to the model for fold 1.\n",
      "Epoch 1/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.4084 - accuracy: 0.7983\n",
      "Epoch 1: val_accuracy improved from -inf to 0.62785, saving model to ./quantized_models/qat_model_fold_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 28s 178ms/step - loss: 1.4084 - accuracy: 0.7983 - val_loss: 1.5903 - val_accuracy: 0.6278 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9916 - accuracy: 0.8809\n",
      "Epoch 2: val_accuracy improved from 0.62785 to 0.71899, saving model to ./quantized_models/qat_model_fold_1.h5\n",
      "98/98 [==============================] - 16s 164ms/step - loss: 0.9916 - accuracy: 0.8809 - val_loss: 1.3332 - val_accuracy: 0.7190 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8445 - accuracy: 0.9309\n",
      "Epoch 3: val_accuracy did not improve from 0.71899\n",
      "98/98 [==============================] - 16s 159ms/step - loss: 0.8445 - accuracy: 0.9309 - val_loss: 1.3904 - val_accuracy: 0.6937 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7822 - accuracy: 0.9392\n",
      "Epoch 4: val_accuracy did not improve from 0.71899\n",
      "98/98 [==============================] - 16s 159ms/step - loss: 0.7822 - accuracy: 0.9392 - val_loss: 1.3533 - val_accuracy: 0.7063 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7227 - accuracy: 0.9571\n",
      "Epoch 5: val_accuracy did not improve from 0.71899\n",
      "98/98 [==============================] - 16s 161ms/step - loss: 0.7227 - accuracy: 0.9571 - val_loss: 1.3318 - val_accuracy: 0.7190 - lr: 1.0000e-03\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7294 - accuracy: 0.9507\n",
      "Epoch 6: val_accuracy improved from 0.71899 to 0.76709, saving model to ./quantized_models/qat_model_fold_1.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.7294 - accuracy: 0.9507 - val_loss: 1.1651 - val_accuracy: 0.7671 - lr: 1.0000e-03\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7079 - accuracy: 0.9603\n",
      "Epoch 7: val_accuracy improved from 0.76709 to 0.84304, saving model to ./quantized_models/qat_model_fold_1.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.7079 - accuracy: 0.9603 - val_loss: 1.0585 - val_accuracy: 0.8430 - lr: 1.0000e-03\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6991 - accuracy: 0.9635\n",
      "Epoch 8: val_accuracy improved from 0.84304 to 0.84810, saving model to ./quantized_models/qat_model_fold_1.h5\n",
      "98/98 [==============================] - 16s 166ms/step - loss: 0.6991 - accuracy: 0.9635 - val_loss: 1.0055 - val_accuracy: 0.8481 - lr: 1.0000e-03\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7024 - accuracy: 0.9686\n",
      "Epoch 9: val_accuracy improved from 0.84810 to 0.86076, saving model to ./quantized_models/qat_model_fold_1.h5\n",
      "98/98 [==============================] - 16s 166ms/step - loss: 0.7024 - accuracy: 0.9686 - val_loss: 0.9881 - val_accuracy: 0.8608 - lr: 1.0000e-03\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6798 - accuracy: 0.9693\n",
      "Epoch 10: val_accuracy improved from 0.86076 to 0.88608, saving model to ./quantized_models/qat_model_fold_1.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.6798 - accuracy: 0.9693 - val_loss: 0.9476 - val_accuracy: 0.8861 - lr: 1.0000e-03\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6962 - accuracy: 0.9686\n",
      "Epoch 11: val_accuracy did not improve from 0.88608\n",
      "98/98 [==============================] - 16s 160ms/step - loss: 0.6962 - accuracy: 0.9686 - val_loss: 0.9524 - val_accuracy: 0.8861 - lr: 1.0000e-03\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6676 - accuracy: 0.9750\n",
      "Epoch 12: val_accuracy improved from 0.88608 to 0.89620, saving model to ./quantized_models/qat_model_fold_1.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.6676 - accuracy: 0.9750 - val_loss: 0.8953 - val_accuracy: 0.8962 - lr: 1.0000e-03\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6625 - accuracy: 0.9776\n",
      "Epoch 13: val_accuracy improved from 0.89620 to 0.90886, saving model to ./quantized_models/qat_model_fold_1.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.6625 - accuracy: 0.9776 - val_loss: 0.8562 - val_accuracy: 0.9089 - lr: 1.0000e-03\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6583 - accuracy: 0.9814\n",
      "Epoch 14: val_accuracy improved from 0.90886 to 0.92152, saving model to ./quantized_models/qat_model_fold_1.h5\n",
      "98/98 [==============================] - 16s 166ms/step - loss: 0.6583 - accuracy: 0.9814 - val_loss: 0.8286 - val_accuracy: 0.9215 - lr: 1.0000e-03\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6644 - accuracy: 0.9712\n",
      "Epoch 15: val_accuracy did not improve from 0.92152\n",
      "98/98 [==============================] - 16s 160ms/step - loss: 0.6644 - accuracy: 0.9712 - val_loss: 0.8931 - val_accuracy: 0.9139 - lr: 1.0000e-03\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6518 - accuracy: 0.9731\n",
      "Epoch 16: val_accuracy improved from 0.92152 to 0.93671, saving model to ./quantized_models/qat_model_fold_1.h5\n",
      "98/98 [==============================] - 16s 166ms/step - loss: 0.6518 - accuracy: 0.9731 - val_loss: 0.8212 - val_accuracy: 0.9367 - lr: 1.0000e-03\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6421 - accuracy: 0.9763\n",
      "Epoch 17: val_accuracy did not improve from 0.93671\n",
      "98/98 [==============================] - 16s 161ms/step - loss: 0.6421 - accuracy: 0.9763 - val_loss: 0.8133 - val_accuracy: 0.9342 - lr: 1.0000e-03\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6397 - accuracy: 0.9802\n",
      "Epoch 18: val_accuracy did not improve from 0.93671\n",
      "98/98 [==============================] - 16s 160ms/step - loss: 0.6397 - accuracy: 0.9802 - val_loss: 0.7824 - val_accuracy: 0.9342 - lr: 1.0000e-03\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6306 - accuracy: 0.9776\n",
      "Epoch 19: val_accuracy improved from 0.93671 to 0.94430, saving model to ./quantized_models/qat_model_fold_1.h5\n",
      "98/98 [==============================] - 16s 166ms/step - loss: 0.6306 - accuracy: 0.9776 - val_loss: 0.7643 - val_accuracy: 0.9443 - lr: 1.0000e-03\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6370 - accuracy: 0.9757\n",
      "Epoch 20: val_accuracy did not improve from 0.94430\n",
      "98/98 [==============================] - 16s 159ms/step - loss: 0.6370 - accuracy: 0.9757 - val_loss: 0.7755 - val_accuracy: 0.9418 - lr: 1.0000e-03\n",
      "25/25 [==============================] - 3s 69ms/step\n",
      "Fold 1 QAT Validation Accuracy: 0.9418\n",
      "Fold 1 QAT Validation F1 Score: 0.9320\n",
      "New best QAT model saved for fold 1 with accuracy 0.9418\n",
      "\n",
      "Applying QAT on fold 2/5\n",
      "Found 1578 validated image filenames belonging to 3 classes.\n",
      "Found 395 validated image filenames belonging to 3 classes.\n",
      "Loaded pretrained model for fold 2.\n",
      "Applied QAT to the model for fold 2.\n",
      "Epoch 1/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.2996 - accuracy: 0.7945\n",
      "Epoch 1: val_accuracy improved from -inf to 0.45823, saving model to ./quantized_models/qat_model_fold_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 29s 178ms/step - loss: 1.2996 - accuracy: 0.7945 - val_loss: 1.7773 - val_accuracy: 0.4582 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8586 - accuracy: 0.8841\n",
      "Epoch 2: val_accuracy improved from 0.45823 to 0.69367, saving model to ./quantized_models/qat_model_fold_2.h5\n",
      "98/98 [==============================] - 16s 164ms/step - loss: 0.8586 - accuracy: 0.8841 - val_loss: 1.3206 - val_accuracy: 0.6937 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7180 - accuracy: 0.9251\n",
      "Epoch 3: val_accuracy improved from 0.69367 to 0.72911, saving model to ./quantized_models/qat_model_fold_2.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.7180 - accuracy: 0.9251 - val_loss: 1.3794 - val_accuracy: 0.7291 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6536 - accuracy: 0.9501\n",
      "Epoch 4: val_accuracy improved from 0.72911 to 0.82532, saving model to ./quantized_models/qat_model_fold_2.h5\n",
      "98/98 [==============================] - 16s 166ms/step - loss: 0.6536 - accuracy: 0.9501 - val_loss: 1.1706 - val_accuracy: 0.8253 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6079 - accuracy: 0.9539\n",
      "Epoch 5: val_accuracy did not improve from 0.82532\n",
      "98/98 [==============================] - 16s 159ms/step - loss: 0.6079 - accuracy: 0.9539 - val_loss: 1.4506 - val_accuracy: 0.7975 - lr: 0.0100\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5989 - accuracy: 0.9488\n",
      "Epoch 6: val_accuracy improved from 0.82532 to 0.85570, saving model to ./quantized_models/qat_model_fold_2.h5\n",
      "98/98 [==============================] - 16s 164ms/step - loss: 0.5989 - accuracy: 0.9488 - val_loss: 0.9620 - val_accuracy: 0.8557 - lr: 0.0100\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5385 - accuracy: 0.9686\n",
      "Epoch 7: val_accuracy improved from 0.85570 to 0.88354, saving model to ./quantized_models/qat_model_fold_2.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.5385 - accuracy: 0.9686 - val_loss: 0.7838 - val_accuracy: 0.8835 - lr: 0.0100\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5226 - accuracy: 0.9629\n",
      "Epoch 8: val_accuracy improved from 0.88354 to 0.88608, saving model to ./quantized_models/qat_model_fold_2.h5\n",
      "98/98 [==============================] - 16s 164ms/step - loss: 0.5226 - accuracy: 0.9629 - val_loss: 0.8743 - val_accuracy: 0.8861 - lr: 0.0100\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5251 - accuracy: 0.9590\n",
      "Epoch 9: val_accuracy improved from 0.88608 to 0.90380, saving model to ./quantized_models/qat_model_fold_2.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.5251 - accuracy: 0.9590 - val_loss: 0.7296 - val_accuracy: 0.9038 - lr: 0.0100\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4858 - accuracy: 0.9693\n",
      "Epoch 10: val_accuracy did not improve from 0.90380\n",
      "98/98 [==============================] - 16s 160ms/step - loss: 0.4858 - accuracy: 0.9693 - val_loss: 0.8222 - val_accuracy: 0.8633 - lr: 0.0100\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4507 - accuracy: 0.9782\n",
      "Epoch 11: val_accuracy improved from 0.90380 to 0.91392, saving model to ./quantized_models/qat_model_fold_2.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.4507 - accuracy: 0.9782 - val_loss: 0.6340 - val_accuracy: 0.9139 - lr: 0.0100\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4437 - accuracy: 0.9718\n",
      "Epoch 12: val_accuracy improved from 0.91392 to 0.93418, saving model to ./quantized_models/qat_model_fold_2.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.4437 - accuracy: 0.9718 - val_loss: 0.5786 - val_accuracy: 0.9342 - lr: 0.0100\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4023 - accuracy: 0.9834\n",
      "Epoch 13: val_accuracy improved from 0.93418 to 0.94937, saving model to ./quantized_models/qat_model_fold_2.h5\n",
      "98/98 [==============================] - 16s 164ms/step - loss: 0.4023 - accuracy: 0.9834 - val_loss: 0.5120 - val_accuracy: 0.9494 - lr: 0.0100\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3827 - accuracy: 0.9859\n",
      "Epoch 14: val_accuracy improved from 0.94937 to 0.95949, saving model to ./quantized_models/qat_model_fold_2.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.3827 - accuracy: 0.9859 - val_loss: 0.4954 - val_accuracy: 0.9595 - lr: 0.0100\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3997 - accuracy: 0.9776\n",
      "Epoch 15: val_accuracy improved from 0.95949 to 0.96456, saving model to ./quantized_models/qat_model_fold_2.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.3997 - accuracy: 0.9776 - val_loss: 0.4825 - val_accuracy: 0.9646 - lr: 0.0100\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3570 - accuracy: 0.9827\n",
      "Epoch 16: val_accuracy did not improve from 0.96456\n",
      "98/98 [==============================] - 16s 161ms/step - loss: 0.3570 - accuracy: 0.9827 - val_loss: 0.4402 - val_accuracy: 0.9595 - lr: 0.0100\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3516 - accuracy: 0.9866\n",
      "Epoch 17: val_accuracy did not improve from 0.96456\n",
      "98/98 [==============================] - 16s 161ms/step - loss: 0.3516 - accuracy: 0.9866 - val_loss: 0.4725 - val_accuracy: 0.9544 - lr: 0.0100\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3326 - accuracy: 0.9859\n",
      "Epoch 18: val_accuracy did not improve from 0.96456\n",
      "98/98 [==============================] - 16s 159ms/step - loss: 0.3326 - accuracy: 0.9859 - val_loss: 0.4879 - val_accuracy: 0.9595 - lr: 0.0100\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3211 - accuracy: 0.9878\n",
      "Epoch 19: val_accuracy did not improve from 0.96456\n",
      "98/98 [==============================] - 16s 160ms/step - loss: 0.3211 - accuracy: 0.9878 - val_loss: 0.4637 - val_accuracy: 0.9620 - lr: 1.0000e-03\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3087 - accuracy: 0.9923\n",
      "Epoch 20: val_accuracy did not improve from 0.96456\n",
      "98/98 [==============================] - 16s 158ms/step - loss: 0.3087 - accuracy: 0.9923 - val_loss: 0.4802 - val_accuracy: 0.9570 - lr: 1.0000e-03\n",
      "25/25 [==============================] - 3s 68ms/step\n",
      "Fold 2 QAT Validation Accuracy: 0.9570\n",
      "Fold 2 QAT Validation F1 Score: 0.9463\n",
      "New best QAT model saved for fold 2 with accuracy 0.9570\n",
      "\n",
      "Applying QAT on fold 3/5\n",
      "Found 1578 validated image filenames belonging to 3 classes.\n",
      "Found 395 validated image filenames belonging to 3 classes.\n",
      "Loaded pretrained model for fold 3.\n",
      "Applied QAT to the model for fold 3.\n",
      "Epoch 1/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.3683 - accuracy: 0.7561\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25316, saving model to ./quantized_models/qat_model_fold_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 28s 178ms/step - loss: 1.3683 - accuracy: 0.7561 - val_loss: 3.0457 - val_accuracy: 0.2532 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8932 - accuracy: 0.8643\n",
      "Epoch 2: val_accuracy improved from 0.25316 to 0.48101, saving model to ./quantized_models/qat_model_fold_3.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.8932 - accuracy: 0.8643 - val_loss: 1.6323 - val_accuracy: 0.4810 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7349 - accuracy: 0.9174\n",
      "Epoch 3: val_accuracy improved from 0.48101 to 0.69367, saving model to ./quantized_models/qat_model_fold_3.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.7349 - accuracy: 0.9174 - val_loss: 1.1610 - val_accuracy: 0.6937 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6757 - accuracy: 0.9277\n",
      "Epoch 4: val_accuracy improved from 0.69367 to 0.83291, saving model to ./quantized_models/qat_model_fold_3.h5\n",
      "98/98 [==============================] - 16s 166ms/step - loss: 0.6757 - accuracy: 0.9277 - val_loss: 0.9038 - val_accuracy: 0.8329 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6139 - accuracy: 0.9501\n",
      "Epoch 5: val_accuracy improved from 0.83291 to 0.83544, saving model to ./quantized_models/qat_model_fold_3.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.6139 - accuracy: 0.9501 - val_loss: 0.9699 - val_accuracy: 0.8354 - lr: 0.0100\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6077 - accuracy: 0.9475\n",
      "Epoch 6: val_accuracy improved from 0.83544 to 0.89620, saving model to ./quantized_models/qat_model_fold_3.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.6077 - accuracy: 0.9475 - val_loss: 0.8478 - val_accuracy: 0.8962 - lr: 0.0100\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5603 - accuracy: 0.9577\n",
      "Epoch 7: val_accuracy improved from 0.89620 to 0.92405, saving model to ./quantized_models/qat_model_fold_3.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.5603 - accuracy: 0.9577 - val_loss: 0.6702 - val_accuracy: 0.9241 - lr: 0.0100\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5244 - accuracy: 0.9565\n",
      "Epoch 8: val_accuracy did not improve from 0.92405\n",
      "98/98 [==============================] - 16s 161ms/step - loss: 0.5244 - accuracy: 0.9565 - val_loss: 0.7108 - val_accuracy: 0.8937 - lr: 0.0100\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5277 - accuracy: 0.9533\n",
      "Epoch 9: val_accuracy did not improve from 0.92405\n",
      "98/98 [==============================] - 16s 159ms/step - loss: 0.5277 - accuracy: 0.9533 - val_loss: 0.8760 - val_accuracy: 0.8203 - lr: 0.0100\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4970 - accuracy: 0.9603\n",
      "Epoch 10: val_accuracy did not improve from 0.92405\n",
      "98/98 [==============================] - 16s 160ms/step - loss: 0.4970 - accuracy: 0.9603 - val_loss: 0.7215 - val_accuracy: 0.8810 - lr: 1.0000e-03\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4839 - accuracy: 0.9693\n",
      "Epoch 11: val_accuracy did not improve from 0.92405\n",
      "98/98 [==============================] - 16s 159ms/step - loss: 0.4839 - accuracy: 0.9693 - val_loss: 0.7062 - val_accuracy: 0.9063 - lr: 1.0000e-03\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4697 - accuracy: 0.9712\n",
      "Epoch 12: val_accuracy improved from 0.92405 to 0.93418, saving model to ./quantized_models/qat_model_fold_3.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.4697 - accuracy: 0.9712 - val_loss: 0.6226 - val_accuracy: 0.9342 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4618 - accuracy: 0.9738\n",
      "Epoch 13: val_accuracy improved from 0.93418 to 0.94177, saving model to ./quantized_models/qat_model_fold_3.h5\n",
      "98/98 [==============================] - 16s 164ms/step - loss: 0.4618 - accuracy: 0.9738 - val_loss: 0.5851 - val_accuracy: 0.9418 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4856 - accuracy: 0.9648\n",
      "Epoch 14: val_accuracy did not improve from 0.94177\n",
      "98/98 [==============================] - 16s 159ms/step - loss: 0.4856 - accuracy: 0.9648 - val_loss: 0.5985 - val_accuracy: 0.9392 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4753 - accuracy: 0.9693\n",
      "Epoch 15: val_accuracy improved from 0.94177 to 0.95443, saving model to ./quantized_models/qat_model_fold_3.h5\n",
      "98/98 [==============================] - 16s 166ms/step - loss: 0.4753 - accuracy: 0.9693 - val_loss: 0.5498 - val_accuracy: 0.9544 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4778 - accuracy: 0.9712\n",
      "Epoch 16: val_accuracy did not improve from 0.95443\n",
      "98/98 [==============================] - 16s 159ms/step - loss: 0.4778 - accuracy: 0.9712 - val_loss: 0.5580 - val_accuracy: 0.9494 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4688 - accuracy: 0.9718\n",
      "Epoch 17: val_accuracy did not improve from 0.95443\n",
      "98/98 [==============================] - 16s 162ms/step - loss: 0.4688 - accuracy: 0.9718 - val_loss: 0.5440 - val_accuracy: 0.9418 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4693 - accuracy: 0.9654\n",
      "Epoch 18: val_accuracy improved from 0.95443 to 0.95949, saving model to ./quantized_models/qat_model_fold_3.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.4693 - accuracy: 0.9654 - val_loss: 0.5158 - val_accuracy: 0.9595 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4662 - accuracy: 0.9725\n",
      "Epoch 19: val_accuracy improved from 0.95949 to 0.96203, saving model to ./quantized_models/qat_model_fold_3.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.4662 - accuracy: 0.9725 - val_loss: 0.5274 - val_accuracy: 0.9620 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4744 - accuracy: 0.9667\n",
      "Epoch 20: val_accuracy did not improve from 0.96203\n",
      "98/98 [==============================] - 16s 161ms/step - loss: 0.4744 - accuracy: 0.9667 - val_loss: 0.5077 - val_accuracy: 0.9620 - lr: 1.0000e-04\n",
      "25/25 [==============================] - 3s 69ms/step\n",
      "Fold 3 QAT Validation Accuracy: 0.9620\n",
      "Fold 3 QAT Validation F1 Score: 0.9697\n",
      "New best QAT model saved for fold 3 with accuracy 0.9620\n",
      "\n",
      "Applying QAT on fold 4/5\n",
      "Found 1579 validated image filenames belonging to 3 classes.\n",
      "Found 394 validated image filenames belonging to 3 classes.\n",
      "Loaded pretrained model for fold 4.\n",
      "Applied QAT to the model for fold 4.\n",
      "Epoch 1/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.3419 - accuracy: 0.7953\n",
      "Epoch 1: val_accuracy improved from -inf to 0.27411, saving model to ./quantized_models/qat_model_fold_4.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 28s 179ms/step - loss: 1.3419 - accuracy: 0.7953 - val_loss: 1.9588 - val_accuracy: 0.2741 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9174 - accuracy: 0.8823\n",
      "Epoch 2: val_accuracy improved from 0.27411 to 0.73096, saving model to ./quantized_models/qat_model_fold_4.h5\n",
      "98/98 [==============================] - 16s 166ms/step - loss: 0.9174 - accuracy: 0.8823 - val_loss: 1.1830 - val_accuracy: 0.7310 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7961 - accuracy: 0.9175\n",
      "Epoch 3: val_accuracy improved from 0.73096 to 0.80457, saving model to ./quantized_models/qat_model_fold_4.h5\n",
      "98/98 [==============================] - 16s 166ms/step - loss: 0.7961 - accuracy: 0.9175 - val_loss: 1.1012 - val_accuracy: 0.8046 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.9443\n",
      "Epoch 4: val_accuracy did not improve from 0.80457\n",
      "98/98 [==============================] - 16s 159ms/step - loss: 0.6952 - accuracy: 0.9443 - val_loss: 1.1970 - val_accuracy: 0.7690 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6831 - accuracy: 0.9501\n",
      "Epoch 5: val_accuracy improved from 0.80457 to 0.87563, saving model to ./quantized_models/qat_model_fold_4.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.6831 - accuracy: 0.9501 - val_loss: 0.9847 - val_accuracy: 0.8756 - lr: 0.0100\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6378 - accuracy: 0.9559\n",
      "Epoch 6: val_accuracy did not improve from 0.87563\n",
      "98/98 [==============================] - 16s 162ms/step - loss: 0.6378 - accuracy: 0.9559 - val_loss: 0.9092 - val_accuracy: 0.8756 - lr: 0.0100\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6292 - accuracy: 0.9552\n",
      "Epoch 7: val_accuracy did not improve from 0.87563\n",
      "98/98 [==============================] - 16s 160ms/step - loss: 0.6292 - accuracy: 0.9552 - val_loss: 0.9503 - val_accuracy: 0.8731 - lr: 0.0100\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5387 - accuracy: 0.9802\n",
      "Epoch 8: val_accuracy improved from 0.87563 to 0.88579, saving model to ./quantized_models/qat_model_fold_4.h5\n",
      "98/98 [==============================] - 16s 165ms/step - loss: 0.5387 - accuracy: 0.9802 - val_loss: 0.9319 - val_accuracy: 0.8858 - lr: 0.0100\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5342 - accuracy: 0.9731\n",
      "Epoch 9: val_accuracy did not improve from 0.88579\n",
      "98/98 [==============================] - 16s 160ms/step - loss: 0.5342 - accuracy: 0.9731 - val_loss: 0.9507 - val_accuracy: 0.8832 - lr: 1.0000e-03\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5081 - accuracy: 0.9827\n",
      "Epoch 10: val_accuracy improved from 0.88579 to 0.89848, saving model to ./quantized_models/qat_model_fold_4.h5\n",
      "98/98 [==============================] - 16s 164ms/step - loss: 0.5081 - accuracy: 0.9827 - val_loss: 0.9596 - val_accuracy: 0.8985 - lr: 1.0000e-03\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5286 - accuracy: 0.9770Restoring model weights from the end of the best epoch: 6.\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.89848 to 0.90609, saving model to ./quantized_models/qat_model_fold_4.h5\n",
      "98/98 [==============================] - 16s 166ms/step - loss: 0.5286 - accuracy: 0.9770 - val_loss: 0.9472 - val_accuracy: 0.9061 - lr: 1.0000e-04\n",
      "Epoch 11: early stopping\n",
      "25/25 [==============================] - 3s 66ms/step\n",
      "Fold 4 QAT Validation Accuracy: 0.9061\n",
      "Fold 4 QAT Validation F1 Score: 0.8738\n",
      "\n",
      "Applying QAT on fold 5/5\n",
      "Found 1579 validated image filenames belonging to 3 classes.\n",
      "Found 394 validated image filenames belonging to 3 classes.\n",
      "Loaded pretrained model for fold 5.\n",
      "Applied QAT to the model for fold 5.\n",
      "Epoch 1/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.1384 - accuracy: 0.7748"
     ]
    }
   ],
   "source": [
    "# 4. Iterate through each fold for QAT\n",
    "for fold_idx, (train_index, test_index) in enumerate(kfold_qat.split(image_dataframe)):\n",
    "    print(f\"\\nApplying QAT on fold {fold_idx + 1}/{num_folds}\")\n",
    "\n",
    "    train_data = image_dataframe.iloc[train_index]\n",
    "    test_data = image_dataframe.iloc[test_index]\n",
    "\n",
    "    # Data Generators\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_data,\n",
    "        x_col='file_path',\n",
    "        y_col='label',\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,  # Increased batch size\n",
    "        class_mode='categorical',\n",
    "        shuffle=True,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=test_data,\n",
    "        x_col='file_path',\n",
    "        y_col='label',\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Paths\n",
    "    saved_model_path = f'./models/best_model_fold_{fold_idx + 1}.h5'\n",
    "    qat_model_path = f'./quantized_models/qat_model_fold_{fold_idx + 1}.h5'\n",
    "\n",
    "    # Check if the saved model exists\n",
    "    if not os.path.exists(saved_model_path):\n",
    "        print(f\"Saved model for fold {fold_idx + 1} not found at {saved_model_path}. Skipping QAT for this fold.\")\n",
    "        continue\n",
    "\n",
    "    # 5. Load the Pretrained Model\n",
    "    pretrained_model = load_model(saved_model_path)\n",
    "    print(f\"Loaded pretrained model for fold {fold_idx + 1}.\")\n",
    "\n",
    "    # 6. Apply Quantization-Aware Training (QAT)\n",
    "    qat_model = tfmot.quantization.keras.quantize_model(pretrained_model)\n",
    "    print(f\"Applied QAT to the model for fold {fold_idx + 1}.\")\n",
    "\n",
    "    # 7. Compile the QAT Model\n",
    "    qat_model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=SGD(learning_rate=0.01),  # Lower learning rate for fine-tuning\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # 8. Define QAT Callbacks\n",
    "    qat_callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2),\n",
    "        ModelCheckpoint(\n",
    "            filepath=qat_model_path,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        TensorBoard(log_dir=f'./logs/qat_fold_{fold_idx + 1}')\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    # 10. Fine-Tune the QAT Model\n",
    "    history_qat = qat_model.fit(\n",
    "        train_generator,  # Reuse the train_generator from the primary loop\n",
    "        steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "        epochs=QAT_EPOCHS,\n",
    "        validation_data=test_generator,\n",
    "        callbacks=qat_callbacks,\n",
    "        # class_weight=class_weight_dict  # Reuse class weights\n",
    "    )\n",
    "\n",
    "    # 11. Evaluation\n",
    "    y_true_qat = test_generator.classes\n",
    "    y_pred_probs_qat = qat_model.predict(test_generator, verbose=1)\n",
    "    y_pred_qat = np.argmax(y_pred_probs_qat, axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    qat_accuracy = history_qat.history['val_accuracy'][-1]\n",
    "    qat_fold_accuracies.append(qat_accuracy)\n",
    "    print(f\"Fold {fold_idx + 1} QAT Validation Accuracy: {qat_accuracy:.4f}\")\n",
    "\n",
    "    qat_f1 = f1_score(y_true_qat, y_pred_qat, average='weighted')\n",
    "    qat_fold_f1_scores.append(qat_f1)\n",
    "    print(f\"Fold {fold_idx + 1} QAT Validation F1 Score: {qat_f1:.4f}\")\n",
    "\n",
    "    # 12. Plot and Save QAT Loss Curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history_qat.history['loss'], label='QAT Training Loss', color='blue')\n",
    "    plt.plot(history_qat.history['val_loss'], label='QAT Validation Loss', color='orange')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'QAT Loss Curve - Fold {fold_idx + 1}')\n",
    "    plt.legend()\n",
    "    qat_loss_curve_path = f'./plots/loss/QAT_MNV2_loss_curve_fold_{fold_idx + 1}.png'\n",
    "    plt.savefig(qat_loss_curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 13. Plot and Save QAT Accuracy Curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history_qat.history['accuracy'], label='QAT Training Accuracy', color='green')\n",
    "    plt.plot(history_qat.history['val_accuracy'], label='QAT Validation Accuracy', color='red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'QAT Accuracy Curve - Fold {fold_idx + 1}')\n",
    "    plt.legend()\n",
    "    qat_accuracy_curve_path = f'./plots/accuracy/QAT_MNV2_accuracy_curve_fold_{fold_idx + 1}.png'\n",
    "    plt.savefig(qat_accuracy_curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 14. Track the Best QAT Fold\n",
    "    if qat_accuracy > best_qat_accuracy:\n",
    "        best_qat_accuracy = qat_accuracy\n",
    "        best_qat_fold_idx = fold_idx\n",
    "        print(f\"New best QAT model saved for fold {fold_idx + 1} with accuracy {qat_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 QAT Accuracy: 0.9063\n",
      "Fold 1 QAT F1 Score: 0.9045\n",
      "Fold 2 QAT Accuracy: 0.9494\n",
      "Fold 2 QAT F1 Score: 0.9509\n",
      "Fold 3 QAT Accuracy: 0.9671\n",
      "Fold 3 QAT F1 Score: 0.9698\n",
      "Fold 4 QAT Accuracy: 0.9112\n",
      "Fold 4 QAT F1 Score: 0.9079\n",
      "Fold 5 QAT Accuracy: 0.9619\n",
      "Fold 5 QAT F1 Score: 0.9536\n",
      "Average QAT Validation Accuracy: 0.9391762495040894\n",
      "Standard Deviation of QAT Validation Accuracy: 0.02555019846176908\n",
      "Average QAT F1 Score: 0.9373380168400158\n",
      "Standard Deviation of QAT F1 Score: 0.0262452003486703\n",
      "\n",
      "Best QAT Fold: 3 with Validation Accuracy: 0.9671\n"
     ]
    }
   ],
   "source": [
    "# 5. Final Evaluation Across QAT Folds\n",
    "avg_qat_accuracy = np.mean(qat_fold_accuracies)\n",
    "std_qat_accuracy = np.std(qat_fold_accuracies)\n",
    "avg_qat_f1 = np.mean(qat_fold_f1_scores)\n",
    "std_qat_f1 = np.std(qat_fold_f1_scores)  # Added standard deviation for F1 scores\n",
    "\n",
    "for fold_idx in range(num_folds):\n",
    "    print(f\"Fold {fold_idx + 1} QAT Accuracy: {qat_fold_accuracies[fold_idx]:.4f}\")\n",
    "    print(f\"Fold {fold_idx + 1} QAT F1 Score: {qat_fold_f1_scores[fold_idx]:.4f}\")\n",
    "\n",
    "print(\"Average QAT Validation Accuracy:\", avg_qat_accuracy)\n",
    "print(\"Standard Deviation of QAT Validation Accuracy:\", std_qat_accuracy)\n",
    "print(\"Average QAT F1 Score:\", avg_qat_f1)\n",
    "print(\"Standard Deviation of QAT F1 Score:\", std_qat_f1)  # Printed std for F1 scores\n",
    "\n",
    "print(f\"\\nBest QAT Fold: {best_qat_fold_idx + 1} with Validation Accuracy: {best_qat_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading saved validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded validation set from: best_fold_2_validation_data.csv\n",
      "Validation samples for QAT: 395\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the saved validation set\n",
    "val_set_path = f'best_fold_{best_fold_idx + 1}_validation_data.csv'  # Adjust fold index if needed\n",
    "val_data_qat = pd.read_csv(val_set_path)\n",
    "print(f\"Loaded validation set from: {val_set_path}\")\n",
    "print(f\"Validation samples for QAT: {len(val_data_qat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training set by excluding validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples for QAT: 1578\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create training set by excluding validation samples\n",
    "train_data_qat = image_dataframe[~image_dataframe['file_path'].isin(val_data_qat['file_path'])]\n",
    "print(f\"Training samples for QAT: {len(train_data_qat)}\")\n",
    "\n",
    "# Step 3: Define image dimensions and batch size\n",
    "img_width, img_height = 128, 128\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize data generators (reuse original augmentations)\n",
    "train_datagen_qat = CustomImageDataGenerator(  # Ensure all augmentations are identical\n",
    "    channel_shift_range=0.1,\n",
    "    # Include all original augmentation parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen_qat = CustomImageDataGenerator()  # No augmentations for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1578 validated image filenames belonging to 3 classes.\n",
      "Found 395 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Create training generator (with augmentations)\n",
    "train_generator_qat = train_datagen_qat.flow_from_dataframe(\n",
    "    dataframe=train_data_qat,\n",
    "    x_col='file_path',\n",
    "    y_col='label',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Step 6: Create validation generator (without augmentations)\n",
    "val_generator_qat = val_datagen_qat.flow_from_dataframe(\n",
    "    dataframe=val_data_qat,\n",
    "    x_col='file_path',\n",
    "    y_col='label',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # No shuffle for validation set\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model for QAT fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Load the pretrained model\n",
    "pretrained_model = tf.keras.models.load_model('./models/best_overall_model_fold_2.h5')\n",
    "print(\"Loaded pretrained model for QAT fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)              (None, 64, 64, 32)           864       ['input_5[0][0]']             \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalizati  (None, 64, 64, 32)           128       ['Conv1[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)           (None, 64, 64, 32)           0         ['bn_Conv1[0][0]']            \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (D  (None, 64, 64, 32)           288       ['Conv1_relu[0][0]']          \n",
      " epthwiseConv2D)                                                                                  \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN  (None, 64, 64, 32)           128       ['expanded_conv_depthwise[0][0\n",
      "  (BatchNormalization)                                              ]']                           \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_re  (None, 64, 64, 32)           0         ['expanded_conv_depthwise_BN[0\n",
      " lu (ReLU)                                                          ][0]']                        \n",
      "                                                                                                  \n",
      " expanded_conv_project (Con  (None, 64, 64, 16)           512       ['expanded_conv_depthwise_relu\n",
      " v2D)                                                               [0][0]']                      \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (  (None, 64, 64, 16)           64        ['expanded_conv_project[0][0]'\n",
      " BatchNormalization)                                                ]                             \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)     (None, 64, 64, 96)           1536      ['expanded_conv_project_BN[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNo  (None, 64, 64, 96)           384       ['block_1_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)  (None, 64, 64, 96)           0         ['block_1_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D  (None, 65, 65, 96)           0         ['block_1_expand_relu[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_1_depthwise (Depthwi  (None, 32, 32, 96)           864       ['block_1_pad[0][0]']         \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (Batc  (None, 32, 32, 96)           384       ['block_1_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (Re  (None, 32, 32, 96)           0         ['block_1_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)    (None, 32, 32, 24)           2304      ['block_1_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchN  (None, 32, 32, 24)           96        ['block_1_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)     (None, 32, 32, 144)          3456      ['block_1_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNo  (None, 32, 32, 144)          576       ['block_2_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)  (None, 32, 32, 144)          0         ['block_2_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_depthwise (Depthwi  (None, 32, 32, 144)          1296      ['block_2_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (Batc  (None, 32, 32, 144)          576       ['block_2_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (Re  (None, 32, 32, 144)          0         ['block_2_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)    (None, 32, 32, 24)           3456      ['block_2_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchN  (None, 32, 32, 24)           96        ['block_2_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_2_add (Add)           (None, 32, 32, 24)           0         ['block_1_project_BN[0][0]',  \n",
      "                                                                     'block_2_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)     (None, 32, 32, 144)          3456      ['block_2_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNo  (None, 32, 32, 144)          576       ['block_3_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)  (None, 32, 32, 144)          0         ['block_3_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D  (None, 33, 33, 144)          0         ['block_3_expand_relu[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_3_depthwise (Depthwi  (None, 16, 16, 144)          1296      ['block_3_pad[0][0]']         \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (Batc  (None, 16, 16, 144)          576       ['block_3_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (Re  (None, 16, 16, 144)          0         ['block_3_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)    (None, 16, 16, 32)           4608      ['block_3_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchN  (None, 16, 16, 32)           128       ['block_3_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)     (None, 16, 16, 192)          6144      ['block_3_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNo  (None, 16, 16, 192)          768       ['block_4_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)  (None, 16, 16, 192)          0         ['block_4_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_depthwise (Depthwi  (None, 16, 16, 192)          1728      ['block_4_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (Batc  (None, 16, 16, 192)          768       ['block_4_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (Re  (None, 16, 16, 192)          0         ['block_4_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)    (None, 16, 16, 32)           6144      ['block_4_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchN  (None, 16, 16, 32)           128       ['block_4_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_4_add (Add)           (None, 16, 16, 32)           0         ['block_3_project_BN[0][0]',  \n",
      "                                                                     'block_4_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)     (None, 16, 16, 192)          6144      ['block_4_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNo  (None, 16, 16, 192)          768       ['block_5_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)  (None, 16, 16, 192)          0         ['block_5_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_depthwise (Depthwi  (None, 16, 16, 192)          1728      ['block_5_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (Batc  (None, 16, 16, 192)          768       ['block_5_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (Re  (None, 16, 16, 192)          0         ['block_5_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)    (None, 16, 16, 32)           6144      ['block_5_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchN  (None, 16, 16, 32)           128       ['block_5_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_5_add (Add)           (None, 16, 16, 32)           0         ['block_4_add[0][0]',         \n",
      "                                                                     'block_5_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)     (None, 16, 16, 192)          6144      ['block_5_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNo  (None, 16, 16, 192)          768       ['block_6_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)  (None, 16, 16, 192)          0         ['block_6_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D  (None, 17, 17, 192)          0         ['block_6_expand_relu[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_6_depthwise (Depthwi  (None, 8, 8, 192)            1728      ['block_6_pad[0][0]']         \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (Batc  (None, 8, 8, 192)            768       ['block_6_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (Re  (None, 8, 8, 192)            0         ['block_6_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)    (None, 8, 8, 64)             12288     ['block_6_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchN  (None, 8, 8, 64)             256       ['block_6_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)     (None, 8, 8, 384)            24576     ['block_6_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNo  (None, 8, 8, 384)            1536      ['block_7_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)  (None, 8, 8, 384)            0         ['block_7_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_depthwise (Depthwi  (None, 8, 8, 384)            3456      ['block_7_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (Batc  (None, 8, 8, 384)            1536      ['block_7_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (Re  (None, 8, 8, 384)            0         ['block_7_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)    (None, 8, 8, 64)             24576     ['block_7_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchN  (None, 8, 8, 64)             256       ['block_7_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_7_add (Add)           (None, 8, 8, 64)             0         ['block_6_project_BN[0][0]',  \n",
      "                                                                     'block_7_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)     (None, 8, 8, 384)            24576     ['block_7_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNo  (None, 8, 8, 384)            1536      ['block_8_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)  (None, 8, 8, 384)            0         ['block_8_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_depthwise (Depthwi  (None, 8, 8, 384)            3456      ['block_8_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (Batc  (None, 8, 8, 384)            1536      ['block_8_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (Re  (None, 8, 8, 384)            0         ['block_8_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)    (None, 8, 8, 64)             24576     ['block_8_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchN  (None, 8, 8, 64)             256       ['block_8_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_8_add (Add)           (None, 8, 8, 64)             0         ['block_7_add[0][0]',         \n",
      "                                                                     'block_8_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)     (None, 8, 8, 384)            24576     ['block_8_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNo  (None, 8, 8, 384)            1536      ['block_9_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)  (None, 8, 8, 384)            0         ['block_9_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_depthwise (Depthwi  (None, 8, 8, 384)            3456      ['block_9_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (Batc  (None, 8, 8, 384)            1536      ['block_9_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (Re  (None, 8, 8, 384)            0         ['block_9_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)    (None, 8, 8, 64)             24576     ['block_9_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchN  (None, 8, 8, 64)             256       ['block_9_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_9_add (Add)           (None, 8, 8, 64)             0         ['block_8_add[0][0]',         \n",
      "                                                                     'block_9_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)    (None, 8, 8, 384)            24576     ['block_9_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchN  (None, 8, 8, 384)            1536      ['block_10_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU  (None, 8, 8, 384)            0         ['block_10_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_10_depthwise (Depthw  (None, 8, 8, 384)            3456      ['block_10_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (Bat  (None, 8, 8, 384)            1536      ['block_10_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (R  (None, 8, 8, 384)            0         ['block_10_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)   (None, 8, 8, 96)             36864     ['block_10_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_10_project_BN (Batch  (None, 8, 8, 96)             384       ['block_10_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)    (None, 8, 8, 576)            55296     ['block_10_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchN  (None, 8, 8, 576)            2304      ['block_11_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU  (None, 8, 8, 576)            0         ['block_11_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_11_depthwise (Depthw  (None, 8, 8, 576)            5184      ['block_11_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (Bat  (None, 8, 8, 576)            2304      ['block_11_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (R  (None, 8, 8, 576)            0         ['block_11_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)   (None, 8, 8, 96)             55296     ['block_11_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_11_project_BN (Batch  (None, 8, 8, 96)             384       ['block_11_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_11_add (Add)          (None, 8, 8, 96)             0         ['block_10_project_BN[0][0]', \n",
      "                                                                     'block_11_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)    (None, 8, 8, 576)            55296     ['block_11_add[0][0]']        \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchN  (None, 8, 8, 576)            2304      ['block_12_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU  (None, 8, 8, 576)            0         ['block_12_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_12_depthwise (Depthw  (None, 8, 8, 576)            5184      ['block_12_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (Bat  (None, 8, 8, 576)            2304      ['block_12_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (R  (None, 8, 8, 576)            0         ['block_12_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)   (None, 8, 8, 96)             55296     ['block_12_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_12_project_BN (Batch  (None, 8, 8, 96)             384       ['block_12_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_12_add (Add)          (None, 8, 8, 96)             0         ['block_11_add[0][0]',        \n",
      "                                                                     'block_12_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)    (None, 8, 8, 576)            55296     ['block_12_add[0][0]']        \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchN  (None, 8, 8, 576)            2304      ['block_13_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU  (None, 8, 8, 576)            0         ['block_13_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2  (None, 9, 9, 576)            0         ['block_13_expand_relu[0][0]']\n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block_13_depthwise (Depthw  (None, 4, 4, 576)            5184      ['block_13_pad[0][0]']        \n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (Bat  (None, 4, 4, 576)            2304      ['block_13_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (R  (None, 4, 4, 576)            0         ['block_13_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)   (None, 4, 4, 160)            92160     ['block_13_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_13_project_BN (Batch  (None, 4, 4, 160)            640       ['block_13_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)    (None, 4, 4, 960)            153600    ['block_13_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchN  (None, 4, 4, 960)            3840      ['block_14_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU  (None, 4, 4, 960)            0         ['block_14_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_14_depthwise (Depthw  (None, 4, 4, 960)            8640      ['block_14_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (Bat  (None, 4, 4, 960)            3840      ['block_14_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (R  (None, 4, 4, 960)            0         ['block_14_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)   (None, 4, 4, 160)            153600    ['block_14_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_14_project_BN (Batch  (None, 4, 4, 160)            640       ['block_14_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_14_add (Add)          (None, 4, 4, 160)            0         ['block_13_project_BN[0][0]', \n",
      "                                                                     'block_14_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)    (None, 4, 4, 960)            153600    ['block_14_add[0][0]']        \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchN  (None, 4, 4, 960)            3840      ['block_15_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU  (None, 4, 4, 960)            0         ['block_15_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_15_depthwise (Depthw  (None, 4, 4, 960)            8640      ['block_15_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (Bat  (None, 4, 4, 960)            3840      ['block_15_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (R  (None, 4, 4, 960)            0         ['block_15_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)   (None, 4, 4, 160)            153600    ['block_15_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_15_project_BN (Batch  (None, 4, 4, 160)            640       ['block_15_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_15_add (Add)          (None, 4, 4, 160)            0         ['block_14_add[0][0]',        \n",
      "                                                                     'block_15_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)    (None, 4, 4, 960)            153600    ['block_15_add[0][0]']        \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchN  (None, 4, 4, 960)            3840      ['block_16_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU  (None, 4, 4, 960)            0         ['block_16_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_16_depthwise (Depthw  (None, 4, 4, 960)            8640      ['block_16_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (Bat  (None, 4, 4, 960)            3840      ['block_16_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (R  (None, 4, 4, 960)            0         ['block_16_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)   (None, 4, 4, 320)            307200    ['block_16_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_16_project_BN (Batch  (None, 4, 4, 320)            1280      ['block_16_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)             (None, 4, 4, 1280)           409600    ['block_16_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalizat  (None, 4, 4, 1280)           5120      ['Conv_1[0][0]']              \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " out_relu (ReLU)             (None, 4, 4, 1280)           0         ['Conv_1_bn[0][0]']           \n",
      "                                                                                                  \n",
      " global_average_pooling2d_4  (None, 1280)                 0         ['out_relu[0][0]']            \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 128)                  163968    ['global_average_pooling2d_4[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 128)                  512       ['dense_8[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 128)                  0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 128)                  0         ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 3)                    387       ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2422851 (9.24 MB)\n",
      "Trainable params: 2388483 (9.11 MB)\n",
      "Non-trainable params: 34368 (134.25 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply quantization to Dense layers only\n",
    "def apply_quantization_to_dense(layer):\n",
    "    if isinstance(layer, keras.layers.Dense):\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_model = tf.keras.models.clone_model(\n",
    "        pretrained_model,\n",
    "        clone_function=apply_quantization_to_dense,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted model to QAT.\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Apply Quantization-Aware Training (QAT)\n",
    "qat_model = tfmot.quantization.keras.quantize_model(annotated_model)\n",
    "print(\"Converted model to QAT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Compile the QAT model\n",
    "opt = SGD(learning_rate=0.01)  # Define optimizer\n",
    "qat_model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 29s 175ms/step - loss: 0.2903 - accuracy: 0.9891 - val_loss: 0.3826 - val_accuracy: 0.9570\n",
      "Epoch 2/15\n",
      "98/98 [==============================] - 16s 161ms/step - loss: 0.2827 - accuracy: 0.9866 - val_loss: 0.3611 - val_accuracy: 0.9646\n",
      "Epoch 3/15\n",
      "98/98 [==============================] - 16s 161ms/step - loss: 0.2966 - accuracy: 0.9757 - val_loss: 0.3559 - val_accuracy: 0.9671\n",
      "Epoch 4/15\n",
      "98/98 [==============================] - 16s 162ms/step - loss: 0.2573 - accuracy: 0.9885 - val_loss: 0.3926 - val_accuracy: 0.9519\n",
      "Epoch 5/15\n",
      "98/98 [==============================] - 16s 160ms/step - loss: 0.2539 - accuracy: 0.9891 - val_loss: 0.3849 - val_accuracy: 0.9443\n",
      "Epoch 6/15\n",
      "98/98 [==============================] - 16s 161ms/step - loss: 0.2507 - accuracy: 0.9834 - val_loss: 0.3631 - val_accuracy: 0.9620\n",
      "Epoch 7/15\n",
      "98/98 [==============================] - 16s 162ms/step - loss: 0.2401 - accuracy: 0.9898 - val_loss: 0.3948 - val_accuracy: 0.9519\n",
      "Epoch 8/15\n",
      "98/98 [==============================] - 16s 162ms/step - loss: 0.2362 - accuracy: 0.9853 - val_loss: 0.3896 - val_accuracy: 0.9519\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Define callbacks\n",
    "early_stopping_qat = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Step 11: Fine-tune the QAT model\n",
    "qat_history = qat_model.fit(\n",
    "    train_generator_qat,\n",
    "    steps_per_epoch=train_generator_qat.samples // train_generator_qat.batch_size,\n",
    "    epochs=15,  # Adjust fine-tuning epochs as needed\n",
    "    validation_data=val_generator_qat,\n",
    "    callbacks=[early_stopping_qat]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(qat_history.history['loss'], label='Training Loss')\n",
    "plt.plot(qat_history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(qat_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(qat_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantizing model after QAT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide a representative dataset for better quantization accuracy\n",
    "def representative_data_gen():\n",
    "    # Number of samples you want to use for calibration\n",
    "    num_samples = 100\n",
    "    count = 0\n",
    "    \n",
    "    for input_value, _ in train_generator_qat:\n",
    "        yield [input_value]\n",
    "        count += 1\n",
    "        if count >= num_samples:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpv246u0wx/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpv246u0wx/assets\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:947: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2025-01-16 13:50:24.358074: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-01-16 13:50:24.358127: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-01-16 13:50:24.358723: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpv246u0wx\n",
      "2025-01-16 13:50:24.391904: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-01-16 13:50:24.391956: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpv246u0wx\n",
      "2025-01-16 13:50:24.461341: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n",
      "2025-01-16 13:50:24.506595: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-01-16 13:50:25.361805: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpv246u0wx\n",
      "2025-01-16 13:50:25.655269: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 1296547 microseconds.\n",
      "2025-01-16 13:50:25.952725: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\n",
    "\n",
    "# Enable full integer quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# Convert and save the model\n",
    "tflite_model = converter.convert()\n",
    "with open(\"fully_quantized_model_int8.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing QAT quantized model on saved test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded validation set from: best_fold_2_validation_data.csv\n",
      "Validation samples: 395\n",
      "Found 395 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the saved validation set\n",
    "val_set_path = f'best_fold_{best_fold_idx + 1}_validation_data.csv'  # Adjust fold index if needed\n",
    "val_data_qat = pd.read_csv(val_set_path)\n",
    "print(f\"Loaded validation set from: {val_set_path}\")\n",
    "print(f\"Validation samples: {len(val_data_qat)}\")\n",
    "\n",
    "# Step 2: Create a validation generator\n",
    "img_width, img_height = 128, 128\n",
    "batch_size = 1\n",
    "\n",
    "# Define the test data generator (no augmentation)\n",
    "val_datagen_qat = CustomImageDataGenerator()  # No augmentations for validation\n",
    "\n",
    "val_generator_qat = val_datagen_qat.flow_from_dataframe(\n",
    "    dataframe=val_data_qat,\n",
    "    x_col='file_path',\n",
    "    y_col='label',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Ensure order matches for true labels and predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorFlow Lite model\n",
    "tflite_model_path = \"fully_quantized_model_int8.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Allocate tensors\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Details with Quantization Parameters:\n",
      "  Name: serving_default_input_5:0\n",
      "  Shape: [  1 128 128   3]\n",
      "  Data Type: <class 'numpy.int8'>\n",
      "  Quantization Parameters: (0.007843137718737125, 0)\n",
      "  Quantization Scale: [0.00784314]\n",
      "  Quantization Zero Points: [0]\n",
      "\n",
      "\n",
      "Output Details with Quantization Parameters:\n",
      "  Name: StatefulPartitionedCall:0\n",
      "  Shape: [1 3]\n",
      "  Data Type: <class 'numpy.int8'>\n",
      "  Quantization Parameters: (0.00390625, -128)\n",
      "  Quantization Scale: [0.00390625]\n",
      "  Quantization Zero Points: [-128]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display input details including quantization parameters\n",
    "print(\"Input Details with Quantization Parameters:\")\n",
    "for input_detail in input_details:\n",
    "    print(f\"  Name: {input_detail['name']}\")\n",
    "    print(f\"  Shape: {input_detail['shape']}\")\n",
    "    print(f\"  Data Type: {input_detail['dtype']}\")\n",
    "    print(f\"  Quantization Parameters: {input_detail['quantization']}\")\n",
    "    print(f\"  Quantization Scale: {input_detail['quantization_parameters']['scales']}\")\n",
    "    print(f\"  Quantization Zero Points: {input_detail['quantization_parameters']['zero_points']}\")\n",
    "    print()\n",
    "\n",
    "# Display output details including quantization parameters\n",
    "print(\"\\nOutput Details with Quantization Parameters:\")\n",
    "for output_detail in output_details:\n",
    "    print(f\"  Name: {output_detail['name']}\")\n",
    "    print(f\"  Shape: {output_detail['shape']}\")\n",
    "    print(f\"  Data Type: {output_detail['dtype']}\")\n",
    "    print(f\"  Quantization Parameters: {output_detail['quantization']}\")\n",
    "    print(f\"  Quantization Scale: {output_detail['quantization_parameters']['scales']}\")\n",
    "    print(f\"  Quantization Zero Points: {output_detail['quantization_parameters']['zero_points']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9316\n",
      "Precision: 0.9276\n",
      "Recall: 0.9325\n",
      "F1-Score: 0.9291\n"
     ]
    }
   ],
   "source": [
    "# Function to run inference using TFLite model\n",
    "def run_inference_tflite(interpreter, input_data):\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output\n",
    "\n",
    "# Step 4: Collect predictions\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for i in range(len(val_generator_qat)):\n",
    "    X_batch, y_batch = val_generator_qat[i]\n",
    "    \n",
    "    # Convert to required input type (int8 if the model expects it)\n",
    "    input_data = X_batch.astype(np.float32)  # Change dtype if necessary\n",
    "\n",
    "    # If model expects int8 inputs\n",
    "    if input_details[0]['dtype'] == np.int8:\n",
    "        input_scale, input_zero_point = input_details[0]['quantization']\n",
    "        input_data = (input_data / input_scale + input_zero_point).astype(np.int8)\n",
    "    \n",
    "    # Run inference\n",
    "    predictions = run_inference_tflite(interpreter, input_data)\n",
    "    \n",
    "    # If output is quantized, dequantize it\n",
    "    if output_details[0]['dtype'] == np.int8:\n",
    "        output_scale, output_zero_point = output_details[0]['quantization']\n",
    "        predictions = (predictions.astype(np.float32) - output_zero_point) * output_scale\n",
    "    \n",
    "    y_pred.append(predictions)\n",
    "    y_true.append(y_batch)\n",
    "\n",
    "# Step 5: Evaluate Performance\n",
    "# Concatenate all batches\n",
    "y_pred = np.concatenate(y_pred, axis=0)\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "# Convert predictions and true labels to class indices\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_true, axis=1)\n",
    "\n",
    "# Compute Accuracy, Precision, Recall, and F1-Score\n",
    "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "precision = precision_score(y_true_classes, y_pred_classes, average='macro')\n",
    "recall = recall_score(y_true_classes, y_pred_classes, average='macro')\n",
    "f1 = f1_score(y_true_classes, y_pred_classes, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Model MBNV1 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the Input layer\n",
    "input_layer = Input(shape=(img_width, img_height, 3))\n",
    "\n",
    "# 2. Load the pre-trained MobileNetV2 model without the top classification layers\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_tensor=input_layer, input_shape=(img_width, img_height, 3), alpha=0.25)\n",
    "base_model.trainable = True\n",
    "\n",
    "# 4. Add custom layers on top of the base_model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x) \n",
    "x = Dropout(0.6)(x)\n",
    "output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# 5. Create the Functional API model\n",
    "small_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# 6. Compile the model with the SGD optimizer\n",
    "opt = SGD(learning_rate=0.01)\n",
    "small_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 64, 64, 8)         216       \n",
      "                                                                 \n",
      " conv1_bn (BatchNormalizati  (None, 64, 64, 8)         32        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " conv1_relu (ReLU)           (None, 64, 64, 8)         0         \n",
      "                                                                 \n",
      " conv_dw_1 (DepthwiseConv2D  (None, 64, 64, 8)         72        \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_1_bn (BatchNormali  (None, 64, 64, 8)         32        \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_1_relu (ReLU)       (None, 64, 64, 8)         0         \n",
      "                                                                 \n",
      " conv_pw_1 (Conv2D)          (None, 64, 64, 16)        128       \n",
      "                                                                 \n",
      " conv_pw_1_bn (BatchNormali  (None, 64, 64, 16)        64        \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_1_relu (ReLU)       (None, 64, 64, 16)        0         \n",
      "                                                                 \n",
      " conv_pad_2 (ZeroPadding2D)  (None, 65, 65, 16)        0         \n",
      "                                                                 \n",
      " conv_dw_2 (DepthwiseConv2D  (None, 32, 32, 16)        144       \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_2_bn (BatchNormali  (None, 32, 32, 16)        64        \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_2_relu (ReLU)       (None, 32, 32, 16)        0         \n",
      "                                                                 \n",
      " conv_pw_2 (Conv2D)          (None, 32, 32, 32)        512       \n",
      "                                                                 \n",
      " conv_pw_2_bn (BatchNormali  (None, 32, 32, 32)        128       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_2_relu (ReLU)       (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv_dw_3 (DepthwiseConv2D  (None, 32, 32, 32)        288       \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_3_bn (BatchNormali  (None, 32, 32, 32)        128       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_3_relu (ReLU)       (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv_pw_3 (Conv2D)          (None, 32, 32, 32)        1024      \n",
      "                                                                 \n",
      " conv_pw_3_bn (BatchNormali  (None, 32, 32, 32)        128       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_3_relu (ReLU)       (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv_pad_4 (ZeroPadding2D)  (None, 33, 33, 32)        0         \n",
      "                                                                 \n",
      " conv_dw_4 (DepthwiseConv2D  (None, 16, 16, 32)        288       \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_4_bn (BatchNormali  (None, 16, 16, 32)        128       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_4_relu (ReLU)       (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv_pw_4 (Conv2D)          (None, 16, 16, 64)        2048      \n",
      "                                                                 \n",
      " conv_pw_4_bn (BatchNormali  (None, 16, 16, 64)        256       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_4_relu (ReLU)       (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv_dw_5 (DepthwiseConv2D  (None, 16, 16, 64)        576       \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_5_bn (BatchNormali  (None, 16, 16, 64)        256       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_5_relu (ReLU)       (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv_pw_5 (Conv2D)          (None, 16, 16, 64)        4096      \n",
      "                                                                 \n",
      " conv_pw_5_bn (BatchNormali  (None, 16, 16, 64)        256       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_5_relu (ReLU)       (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv_pad_6 (ZeroPadding2D)  (None, 17, 17, 64)        0         \n",
      "                                                                 \n",
      " conv_dw_6 (DepthwiseConv2D  (None, 8, 8, 64)          576       \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_6_bn (BatchNormali  (None, 8, 8, 64)          256       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_6_relu (ReLU)       (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv_pw_6 (Conv2D)          (None, 8, 8, 128)         8192      \n",
      "                                                                 \n",
      " conv_pw_6_bn (BatchNormali  (None, 8, 8, 128)         512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_6_relu (ReLU)       (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_dw_7 (DepthwiseConv2D  (None, 8, 8, 128)         1152      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_7_bn (BatchNormali  (None, 8, 8, 128)         512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_7_relu (ReLU)       (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_pw_7 (Conv2D)          (None, 8, 8, 128)         16384     \n",
      "                                                                 \n",
      " conv_pw_7_bn (BatchNormali  (None, 8, 8, 128)         512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_7_relu (ReLU)       (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_dw_8 (DepthwiseConv2D  (None, 8, 8, 128)         1152      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_8_bn (BatchNormali  (None, 8, 8, 128)         512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_8_relu (ReLU)       (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_pw_8 (Conv2D)          (None, 8, 8, 128)         16384     \n",
      "                                                                 \n",
      " conv_pw_8_bn (BatchNormali  (None, 8, 8, 128)         512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_8_relu (ReLU)       (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_dw_9 (DepthwiseConv2D  (None, 8, 8, 128)         1152      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_9_bn (BatchNormali  (None, 8, 8, 128)         512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_9_relu (ReLU)       (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_pw_9 (Conv2D)          (None, 8, 8, 128)         16384     \n",
      "                                                                 \n",
      " conv_pw_9_bn (BatchNormali  (None, 8, 8, 128)         512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_9_relu (ReLU)       (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_dw_10 (DepthwiseConv2  (None, 8, 8, 128)         1152      \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv_dw_10_bn (BatchNormal  (None, 8, 8, 128)         512       \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_dw_10_relu (ReLU)      (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_pw_10 (Conv2D)         (None, 8, 8, 128)         16384     \n",
      "                                                                 \n",
      " conv_pw_10_bn (BatchNormal  (None, 8, 8, 128)         512       \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_pw_10_relu (ReLU)      (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_dw_11 (DepthwiseConv2  (None, 8, 8, 128)         1152      \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv_dw_11_bn (BatchNormal  (None, 8, 8, 128)         512       \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_dw_11_relu (ReLU)      (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_pw_11 (Conv2D)         (None, 8, 8, 128)         16384     \n",
      "                                                                 \n",
      " conv_pw_11_bn (BatchNormal  (None, 8, 8, 128)         512       \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_pw_11_relu (ReLU)      (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_pad_12 (ZeroPadding2D  (None, 9, 9, 128)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_12 (DepthwiseConv2  (None, 4, 4, 128)         1152      \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv_dw_12_bn (BatchNormal  (None, 4, 4, 128)         512       \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_dw_12_relu (ReLU)      (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv_pw_12 (Conv2D)         (None, 4, 4, 256)         32768     \n",
      "                                                                 \n",
      " conv_pw_12_bn (BatchNormal  (None, 4, 4, 256)         1024      \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_pw_12_relu (ReLU)      (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv_dw_13 (DepthwiseConv2  (None, 4, 4, 256)         2304      \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv_dw_13_bn (BatchNormal  (None, 4, 4, 256)         1024      \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_dw_13_relu (ReLU)      (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv_pw_13 (Conv2D)         (None, 4, 4, 256)         65536     \n",
      "                                                                 \n",
      " conv_pw_13_bn (BatchNormal  (None, 4, 4, 256)         1024      \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_pw_13_relu (ReLU)      (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 256)               0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252339 (985.70 KB)\n",
      "Trainable params: 246611 (963.32 KB)\n",
      "Non-trainable params: 5728 (22.38 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "small_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to track the best fold\n",
    "best_fold_idx = -1\n",
    "best_accuracy = -1\n",
    "best_test_data = None\n",
    "\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "fold_f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1/5\n",
      "Found 1578 validated image filenames belonging to 3 classes.\n",
      "Found 395 validated image filenames belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 12:32:11.587168: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2025-01-19 12:32:13.211206: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f05b1745aa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-01-19 12:32:13.211241: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA L4, Compute Capability 8.9\n",
      "2025-01-19 12:32:13.311242: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 2.7100 - accuracy: 0.5621\n",
      "Epoch 1: val_accuracy improved from -inf to 0.65316, saving model to ./models/best_small_model_fold_1.h5\n",
      "98/98 [==============================] - 16s 95ms/step - loss: 2.7100 - accuracy: 0.5621 - val_loss: 2.4111 - val_accuracy: 0.6532 - lr: 0.0100\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 2.2482 - accuracy: 0.7484\n",
      "Epoch 2: val_accuracy improved from 0.65316 to 0.81519, saving model to ./models/best_small_model_fold_1.h5\n",
      "98/98 [==============================] - 9s 91ms/step - loss: 2.2482 - accuracy: 0.7484 - val_loss: 2.0250 - val_accuracy: 0.8152 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 2.0298 - accuracy: 0.8195\n",
      "Epoch 3: val_accuracy did not improve from 0.81519\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 2.0298 - accuracy: 0.8195 - val_loss: 2.0325 - val_accuracy: 0.8051 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.9436 - accuracy: 0.8355\n",
      "Epoch 4: val_accuracy improved from 0.81519 to 0.84810, saving model to ./models/best_small_model_fold_1.h5\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 1.9436 - accuracy: 0.8355 - val_loss: 1.8491 - val_accuracy: 0.8481 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.8384 - accuracy: 0.8387\n",
      "Epoch 5: val_accuracy improved from 0.84810 to 0.87089, saving model to ./models/best_small_model_fold_1.h5\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 1.8384 - accuracy: 0.8387 - val_loss: 1.7544 - val_accuracy: 0.8709 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.7422 - accuracy: 0.8700\n",
      "Epoch 6: val_accuracy improved from 0.87089 to 0.87595, saving model to ./models/best_small_model_fold_1.h5\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 1.7422 - accuracy: 0.8700 - val_loss: 1.6740 - val_accuracy: 0.8759 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.6633 - accuracy: 0.8764\n",
      "Epoch 7: val_accuracy improved from 0.87595 to 0.90380, saving model to ./models/best_small_model_fold_1.h5\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 1.6633 - accuracy: 0.8764 - val_loss: 1.5978 - val_accuracy: 0.9038 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.6034 - accuracy: 0.8841\n",
      "Epoch 8: val_accuracy did not improve from 0.90380\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 1.6034 - accuracy: 0.8841 - val_loss: 1.5771 - val_accuracy: 0.8810 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.5390 - accuracy: 0.8860\n",
      "Epoch 9: val_accuracy did not improve from 0.90380\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1.5390 - accuracy: 0.8860 - val_loss: 1.5368 - val_accuracy: 0.8734 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.4708 - accuracy: 0.8988\n",
      "Epoch 10: val_accuracy did not improve from 0.90380\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 1.4708 - accuracy: 0.8988 - val_loss: 1.4991 - val_accuracy: 0.8658 - lr: 0.0100\n",
      "Epoch 11/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.4032 - accuracy: 0.9062\n",
      "Epoch 11: val_accuracy did not improve from 0.90380\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 1.4032 - accuracy: 0.9062 - val_loss: 1.4371 - val_accuracy: 0.8835 - lr: 0.0100\n",
      "Epoch 12/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.3137 - accuracy: 0.9283\n",
      "Epoch 12: val_accuracy did not improve from 0.90380\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 1.3137 - accuracy: 0.9283 - val_loss: 1.3766 - val_accuracy: 0.8987 - lr: 0.0100\n",
      "Epoch 13/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.2599 - accuracy: 0.9283\n",
      "Epoch 13: val_accuracy improved from 0.90380 to 0.90633, saving model to ./models/best_small_model_fold_1.h5\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 1.2599 - accuracy: 0.9283 - val_loss: 1.3320 - val_accuracy: 0.9063 - lr: 0.0100\n",
      "Epoch 14/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.2323 - accuracy: 0.9219\n",
      "Epoch 14: val_accuracy did not improve from 0.90633\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 1.2323 - accuracy: 0.9219 - val_loss: 1.2938 - val_accuracy: 0.8987 - lr: 0.0100\n",
      "Epoch 15/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.1831 - accuracy: 0.9219\n",
      "Epoch 15: val_accuracy did not improve from 0.90633\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 1.1831 - accuracy: 0.9219 - val_loss: 1.2867 - val_accuracy: 0.8785 - lr: 0.0100\n",
      "Epoch 16/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.1302 - accuracy: 0.9360\n",
      "Epoch 16: val_accuracy did not improve from 0.90633\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 1.1302 - accuracy: 0.9360 - val_loss: 1.1937 - val_accuracy: 0.8911 - lr: 0.0100\n",
      "Epoch 17/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.9245\n",
      "Epoch 17: val_accuracy improved from 0.90633 to 0.90886, saving model to ./models/best_small_model_fold_1.h5\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 1.0986 - accuracy: 0.9245 - val_loss: 1.1335 - val_accuracy: 0.9089 - lr: 0.0100\n",
      "Epoch 18/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.0465 - accuracy: 0.9353\n",
      "Epoch 18: val_accuracy improved from 0.90886 to 0.91392, saving model to ./models/best_small_model_fold_1.h5\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 1.0465 - accuracy: 0.9353 - val_loss: 1.1045 - val_accuracy: 0.9139 - lr: 0.0100\n",
      "Epoch 19/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.0211 - accuracy: 0.9315\n",
      "Epoch 19: val_accuracy did not improve from 0.91392\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 1.0211 - accuracy: 0.9315 - val_loss: 1.1808 - val_accuracy: 0.8810 - lr: 0.0100\n",
      "Epoch 20/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9988 - accuracy: 0.9283\n",
      "Epoch 20: val_accuracy improved from 0.91392 to 0.92658, saving model to ./models/best_small_model_fold_1.h5\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.9988 - accuracy: 0.9283 - val_loss: 1.0341 - val_accuracy: 0.9266 - lr: 0.0100\n",
      "Epoch 21/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9702 - accuracy: 0.9270\n",
      "Epoch 21: val_accuracy did not improve from 0.92658\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 0.9702 - accuracy: 0.9270 - val_loss: 1.0031 - val_accuracy: 0.9139 - lr: 0.0100\n",
      "Epoch 22/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8991 - accuracy: 0.9417\n",
      "Epoch 22: val_accuracy did not improve from 0.92658\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.8991 - accuracy: 0.9417 - val_loss: 0.9885 - val_accuracy: 0.9089 - lr: 0.0100\n",
      "Epoch 23/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8567 - accuracy: 0.9494\n",
      "Epoch 23: val_accuracy did not improve from 0.92658\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.8567 - accuracy: 0.9494 - val_loss: 0.9403 - val_accuracy: 0.9139 - lr: 0.0100\n",
      "Epoch 24/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8517 - accuracy: 0.9443\n",
      "Epoch 24: val_accuracy did not improve from 0.92658\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 0.8517 - accuracy: 0.9443 - val_loss: 1.0041 - val_accuracy: 0.8987 - lr: 0.0100\n",
      "Epoch 25/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8145 - accuracy: 0.9520\n",
      "Epoch 25: val_accuracy did not improve from 0.92658\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.8145 - accuracy: 0.9520 - val_loss: 0.8907 - val_accuracy: 0.9190 - lr: 0.0100\n",
      "Epoch 26/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7859 - accuracy: 0.9488\n",
      "Epoch 26: val_accuracy improved from 0.92658 to 0.93924, saving model to ./models/best_small_model_fold_1.h5\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.7859 - accuracy: 0.9488 - val_loss: 0.9099 - val_accuracy: 0.9392 - lr: 0.0100\n",
      "Epoch 27/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7544 - accuracy: 0.9501\n",
      "Epoch 27: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.7544 - accuracy: 0.9501 - val_loss: 0.9028 - val_accuracy: 0.9114 - lr: 0.0100\n",
      "Epoch 28/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7317 - accuracy: 0.9533\n",
      "Epoch 28: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.7317 - accuracy: 0.9533 - val_loss: 0.8726 - val_accuracy: 0.9013 - lr: 0.0100\n",
      "Epoch 29/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7161 - accuracy: 0.9424\n",
      "Epoch 29: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.7161 - accuracy: 0.9424 - val_loss: 0.8080 - val_accuracy: 0.9165 - lr: 0.0100\n",
      "Epoch 30/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6837 - accuracy: 0.9520\n",
      "Epoch 30: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.6837 - accuracy: 0.9520 - val_loss: 0.7873 - val_accuracy: 0.9089 - lr: 0.0100\n",
      "Epoch 31/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6699 - accuracy: 0.9481\n",
      "Epoch 31: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.6699 - accuracy: 0.9481 - val_loss: 0.7397 - val_accuracy: 0.9139 - lr: 0.0100\n",
      "Epoch 32/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6407 - accuracy: 0.9501\n",
      "Epoch 32: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 0.6407 - accuracy: 0.9501 - val_loss: 0.7291 - val_accuracy: 0.9266 - lr: 0.0100\n",
      "Epoch 33/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6245 - accuracy: 0.9481\n",
      "Epoch 33: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 0.6245 - accuracy: 0.9481 - val_loss: 0.7199 - val_accuracy: 0.9190 - lr: 0.0100\n",
      "Epoch 34/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6020 - accuracy: 0.9507\n",
      "Epoch 34: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.6020 - accuracy: 0.9507 - val_loss: 0.7991 - val_accuracy: 0.9013 - lr: 0.0100\n",
      "Epoch 35/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5767 - accuracy: 0.9539\n",
      "Epoch 35: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.5767 - accuracy: 0.9539 - val_loss: 0.7396 - val_accuracy: 0.9089 - lr: 0.0100\n",
      "Epoch 36/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5455 - accuracy: 0.9565\n",
      "Epoch 36: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.5455 - accuracy: 0.9565 - val_loss: 0.6728 - val_accuracy: 0.9215 - lr: 0.0100\n",
      "Epoch 37/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5313 - accuracy: 0.9635\n",
      "Epoch 37: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.5313 - accuracy: 0.9635 - val_loss: 0.6069 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 38/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5213 - accuracy: 0.9590\n",
      "Epoch 38: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 0.5213 - accuracy: 0.9590 - val_loss: 0.6651 - val_accuracy: 0.9139 - lr: 0.0100\n",
      "Epoch 39/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5086 - accuracy: 0.9552\n",
      "Epoch 39: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.5086 - accuracy: 0.9552 - val_loss: 0.7370 - val_accuracy: 0.9114 - lr: 0.0100\n",
      "Epoch 40/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4869 - accuracy: 0.9654\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.4869 - accuracy: 0.9654 - val_loss: 0.6280 - val_accuracy: 0.8962 - lr: 0.0100\n",
      "Epoch 41/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4637 - accuracy: 0.9641\n",
      "Epoch 41: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.4637 - accuracy: 0.9641 - val_loss: 0.6331 - val_accuracy: 0.9063 - lr: 1.0000e-03\n",
      "Epoch 42/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4735 - accuracy: 0.9635\n",
      "Epoch 42: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 0.4735 - accuracy: 0.9635 - val_loss: 0.6046 - val_accuracy: 0.9241 - lr: 1.0000e-03\n",
      "Epoch 43/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4518 - accuracy: 0.9738\n",
      "Epoch 43: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.4518 - accuracy: 0.9738 - val_loss: 0.6298 - val_accuracy: 0.9241 - lr: 1.0000e-03\n",
      "Epoch 44/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4457 - accuracy: 0.9661\n",
      "Epoch 44: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 0.4457 - accuracy: 0.9661 - val_loss: 0.6178 - val_accuracy: 0.9241 - lr: 1.0000e-03\n",
      "Epoch 45/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4635 - accuracy: 0.9641\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.4635 - accuracy: 0.9641 - val_loss: 0.6229 - val_accuracy: 0.9291 - lr: 1.0000e-03\n",
      "Epoch 46/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4430 - accuracy: 0.9731\n",
      "Epoch 46: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.4430 - accuracy: 0.9731 - val_loss: 0.6109 - val_accuracy: 0.9165 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4690 - accuracy: 0.9603Restoring model weights from the end of the best epoch: 42.\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.4690 - accuracy: 0.9603 - val_loss: 0.6082 - val_accuracy: 0.9139 - lr: 1.0000e-04\n",
      "Epoch 47: early stopping\n",
      "25/25 [==============================] - 2s 60ms/step\n",
      "Fold 1 Validation Accuracy: 0.9139\n",
      "Fold 1 Validation F1 Score: 0.9236\n",
      "New best model saved for fold 1 with accuracy 0.9139\n",
      "Training fold 2/5\n",
      "Found 1578 validated image filenames belonging to 3 classes.\n",
      "Found 395 validated image filenames belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 2.6965 - accuracy: 0.5832\n",
      "Epoch 1: val_accuracy improved from -inf to 0.69873, saving model to ./models/best_small_model_fold_2.h5\n",
      "98/98 [==============================] - 12s 86ms/step - loss: 2.6965 - accuracy: 0.5832 - val_loss: 2.4336 - val_accuracy: 0.6987 - lr: 0.0100\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 2.2220 - accuracy: 0.7490\n",
      "Epoch 2: val_accuracy improved from 0.69873 to 0.86076, saving model to ./models/best_small_model_fold_2.h5\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 2.2220 - accuracy: 0.7490 - val_loss: 2.0008 - val_accuracy: 0.8608 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 2.0859 - accuracy: 0.8003\n",
      "Epoch 3: val_accuracy did not improve from 0.86076\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 2.0859 - accuracy: 0.8003 - val_loss: 1.9062 - val_accuracy: 0.8557 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.9447 - accuracy: 0.8303\n",
      "Epoch 4: val_accuracy improved from 0.86076 to 0.87595, saving model to ./models/best_small_model_fold_2.h5\n",
      "98/98 [==============================] - 9s 91ms/step - loss: 1.9447 - accuracy: 0.8303 - val_loss: 1.8179 - val_accuracy: 0.8759 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.7951 - accuracy: 0.8675\n",
      "Epoch 5: val_accuracy improved from 0.87595 to 0.90380, saving model to ./models/best_small_model_fold_2.h5\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 1.7951 - accuracy: 0.8675 - val_loss: 1.7002 - val_accuracy: 0.9038 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.7386 - accuracy: 0.8720\n",
      "Epoch 6: val_accuracy improved from 0.90380 to 0.91392, saving model to ./models/best_small_model_fold_2.h5\n",
      "98/98 [==============================] - 9s 90ms/step - loss: 1.7386 - accuracy: 0.8720 - val_loss: 1.6168 - val_accuracy: 0.9139 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.6619 - accuracy: 0.8803\n",
      "Epoch 7: val_accuracy did not improve from 0.91392\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 1.6619 - accuracy: 0.8803 - val_loss: 1.5985 - val_accuracy: 0.8911 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.6014 - accuracy: 0.8803\n",
      "Epoch 8: val_accuracy did not improve from 0.91392\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1.6014 - accuracy: 0.8803 - val_loss: 1.5586 - val_accuracy: 0.8810 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.5313 - accuracy: 0.8956\n",
      "Epoch 9: val_accuracy improved from 0.91392 to 0.92911, saving model to ./models/best_small_model_fold_2.h5\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 1.5313 - accuracy: 0.8956 - val_loss: 1.4641 - val_accuracy: 0.9291 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.4412 - accuracy: 0.9027\n",
      "Epoch 10: val_accuracy did not improve from 0.92911\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 1.4412 - accuracy: 0.9027 - val_loss: 1.4144 - val_accuracy: 0.9089 - lr: 0.0100\n",
      "Epoch 11/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.3841 - accuracy: 0.9027\n",
      "Epoch 11: val_accuracy did not improve from 0.92911\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1.3841 - accuracy: 0.9027 - val_loss: 1.3732 - val_accuracy: 0.9215 - lr: 0.0100\n",
      "Epoch 12/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.3416 - accuracy: 0.9097\n",
      "Epoch 12: val_accuracy did not improve from 0.92911\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 1.3416 - accuracy: 0.9097 - val_loss: 1.4081 - val_accuracy: 0.8937 - lr: 0.0100\n",
      "Epoch 13/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.2974 - accuracy: 0.9117\n",
      "Epoch 13: val_accuracy did not improve from 0.92911\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 1.2974 - accuracy: 0.9117 - val_loss: 1.2902 - val_accuracy: 0.8937 - lr: 0.0100\n",
      "Epoch 14/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.2389 - accuracy: 0.9238\n",
      "Epoch 14: val_accuracy improved from 0.92911 to 0.93418, saving model to ./models/best_small_model_fold_2.h5\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 1.2389 - accuracy: 0.9238 - val_loss: 1.2202 - val_accuracy: 0.9342 - lr: 0.0100\n",
      "Epoch 15/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.2014 - accuracy: 0.9155\n",
      "Epoch 15: val_accuracy did not improve from 0.93418\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 1.2014 - accuracy: 0.9155 - val_loss: 1.1855 - val_accuracy: 0.9342 - lr: 0.0100\n",
      "Epoch 16/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.1354 - accuracy: 0.9330\n",
      "Epoch 16: val_accuracy did not improve from 0.93418\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 1.1354 - accuracy: 0.9330 - val_loss: 1.1806 - val_accuracy: 0.9266 - lr: 0.0100\n",
      "Epoch 17/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.1014 - accuracy: 0.9366\n",
      "Epoch 17: val_accuracy did not improve from 0.93418\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 1.1014 - accuracy: 0.9366 - val_loss: 1.1505 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 18/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.0593 - accuracy: 0.9289\n",
      "Epoch 18: val_accuracy did not improve from 0.93418\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1.0593 - accuracy: 0.9289 - val_loss: 1.0802 - val_accuracy: 0.9165 - lr: 0.0100\n",
      "Epoch 19/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.0036 - accuracy: 0.9353\n",
      "Epoch 19: val_accuracy did not improve from 0.93418\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 1.0036 - accuracy: 0.9353 - val_loss: 1.0491 - val_accuracy: 0.9139 - lr: 0.0100\n",
      "Epoch 20/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9655 - accuracy: 0.9366\n",
      "Epoch 20: val_accuracy did not improve from 0.93418\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.9655 - accuracy: 0.9366 - val_loss: 1.0227 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 21/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9929 - accuracy: 0.9123\n",
      "Epoch 21: val_accuracy improved from 0.93418 to 0.93924, saving model to ./models/best_small_model_fold_2.h5\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 0.9929 - accuracy: 0.9123 - val_loss: 0.9727 - val_accuracy: 0.9392 - lr: 0.0100\n",
      "Epoch 22/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9204 - accuracy: 0.9347\n",
      "Epoch 22: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.9204 - accuracy: 0.9347 - val_loss: 0.9176 - val_accuracy: 0.9367 - lr: 0.0100\n",
      "Epoch 23/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8809 - accuracy: 0.9411\n",
      "Epoch 23: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.8809 - accuracy: 0.9411 - val_loss: 0.9697 - val_accuracy: 0.9266 - lr: 0.0100\n",
      "Epoch 24/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8592 - accuracy: 0.9417\n",
      "Epoch 24: val_accuracy did not improve from 0.93924\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.8592 - accuracy: 0.9417 - val_loss: 0.9045 - val_accuracy: 0.9266 - lr: 0.0100\n",
      "Epoch 25/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8289 - accuracy: 0.9373\n",
      "Epoch 25: val_accuracy improved from 0.93924 to 0.94177, saving model to ./models/best_small_model_fold_2.h5\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.8289 - accuracy: 0.9373 - val_loss: 0.8484 - val_accuracy: 0.9418 - lr: 0.0100\n",
      "Epoch 26/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7778 - accuracy: 0.9469\n",
      "Epoch 26: val_accuracy did not improve from 0.94177\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 0.7778 - accuracy: 0.9469 - val_loss: 0.8151 - val_accuracy: 0.9418 - lr: 0.0100\n",
      "Epoch 27/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7626 - accuracy: 0.9449\n",
      "Epoch 27: val_accuracy did not improve from 0.94177\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.7626 - accuracy: 0.9449 - val_loss: 0.8158 - val_accuracy: 0.9367 - lr: 0.0100\n",
      "Epoch 28/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7390 - accuracy: 0.9507\n",
      "Epoch 28: val_accuracy did not improve from 0.94177\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.7390 - accuracy: 0.9507 - val_loss: 0.7714 - val_accuracy: 0.9367 - lr: 0.0100\n",
      "Epoch 29/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7310 - accuracy: 0.9430\n",
      "Epoch 29: val_accuracy did not improve from 0.94177\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.7310 - accuracy: 0.9430 - val_loss: 0.7733 - val_accuracy: 0.9367 - lr: 0.0100\n",
      "Epoch 30/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6756 - accuracy: 0.9539\n",
      "Epoch 30: val_accuracy did not improve from 0.94177\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 0.6756 - accuracy: 0.9539 - val_loss: 0.7295 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 31/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6618 - accuracy: 0.9533\n",
      "Epoch 31: val_accuracy did not improve from 0.94177\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.6618 - accuracy: 0.9533 - val_loss: 0.7507 - val_accuracy: 0.9241 - lr: 0.0100\n",
      "Epoch 32/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6513 - accuracy: 0.9424\n",
      "Epoch 32: val_accuracy did not improve from 0.94177\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.6513 - accuracy: 0.9424 - val_loss: 0.7115 - val_accuracy: 0.9367 - lr: 0.0100\n",
      "Epoch 33/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6191 - accuracy: 0.9456\n",
      "Epoch 33: val_accuracy did not improve from 0.94177\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 0.6191 - accuracy: 0.9456 - val_loss: 0.7449 - val_accuracy: 0.9190 - lr: 0.0100\n",
      "Epoch 34/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5742 - accuracy: 0.9603\n",
      "Epoch 34: val_accuracy improved from 0.94177 to 0.94684, saving model to ./models/best_small_model_fold_2.h5\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.5742 - accuracy: 0.9603 - val_loss: 0.6126 - val_accuracy: 0.9468 - lr: 0.0100\n",
      "Epoch 35/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5848 - accuracy: 0.9494\n",
      "Epoch 35: val_accuracy did not improve from 0.94684\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.5848 - accuracy: 0.9494 - val_loss: 0.6212 - val_accuracy: 0.9443 - lr: 0.0100\n",
      "Epoch 36/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5660 - accuracy: 0.9545\n",
      "Epoch 36: val_accuracy improved from 0.94684 to 0.95190, saving model to ./models/best_small_model_fold_2.h5\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 0.5660 - accuracy: 0.9545 - val_loss: 0.6250 - val_accuracy: 0.9519 - lr: 0.0100\n",
      "Epoch 37/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5268 - accuracy: 0.9526\n",
      "Epoch 37: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.5268 - accuracy: 0.9526 - val_loss: 0.6090 - val_accuracy: 0.9418 - lr: 0.0100\n",
      "Epoch 38/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5065 - accuracy: 0.9609\n",
      "Epoch 38: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.5065 - accuracy: 0.9609 - val_loss: 0.5994 - val_accuracy: 0.9342 - lr: 0.0100\n",
      "Epoch 39/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5007 - accuracy: 0.9571\n",
      "Epoch 39: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.5007 - accuracy: 0.9571 - val_loss: 0.6233 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 40/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4859 - accuracy: 0.9603\n",
      "Epoch 40: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.4859 - accuracy: 0.9603 - val_loss: 0.5719 - val_accuracy: 0.9494 - lr: 0.0100\n",
      "Epoch 41/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4632 - accuracy: 0.9584\n",
      "Epoch 41: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.4632 - accuracy: 0.9584 - val_loss: 0.5684 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 42/100\n",
      "97/98 [============================>.] - ETA: 0s - loss: 0.4804 - accuracy: 0.9543\n",
      "Epoch 42: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 0.4801 - accuracy: 0.9545 - val_loss: 0.5243 - val_accuracy: 0.9392 - lr: 0.0100\n",
      "Epoch 43/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4642 - accuracy: 0.9539\n",
      "Epoch 43: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.4642 - accuracy: 0.9539 - val_loss: 0.5565 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 44/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4306 - accuracy: 0.9603\n",
      "Epoch 44: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.4306 - accuracy: 0.9603 - val_loss: 0.5295 - val_accuracy: 0.9443 - lr: 0.0100\n",
      "Epoch 45/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4119 - accuracy: 0.9622\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.4119 - accuracy: 0.9622 - val_loss: 0.5345 - val_accuracy: 0.9418 - lr: 0.0100\n",
      "Epoch 46/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4019 - accuracy: 0.9648\n",
      "Epoch 46: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.4019 - accuracy: 0.9648 - val_loss: 0.5137 - val_accuracy: 0.9392 - lr: 1.0000e-03\n",
      "Epoch 47/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4071 - accuracy: 0.9622\n",
      "Epoch 47: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.4071 - accuracy: 0.9622 - val_loss: 0.4984 - val_accuracy: 0.9468 - lr: 1.0000e-03\n",
      "Epoch 48/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4022 - accuracy: 0.9590\n",
      "Epoch 48: val_accuracy improved from 0.95190 to 0.95949, saving model to ./models/best_small_model_fold_2.h5\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.4022 - accuracy: 0.9590 - val_loss: 0.4859 - val_accuracy: 0.9595 - lr: 1.0000e-03\n",
      "Epoch 49/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3781 - accuracy: 0.9712\n",
      "Epoch 49: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 0.3781 - accuracy: 0.9712 - val_loss: 0.4814 - val_accuracy: 0.9544 - lr: 1.0000e-03\n",
      "Epoch 50/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3678 - accuracy: 0.9744\n",
      "Epoch 50: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.3678 - accuracy: 0.9744 - val_loss: 0.4702 - val_accuracy: 0.9570 - lr: 1.0000e-03\n",
      "Epoch 51/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3977 - accuracy: 0.9661\n",
      "Epoch 51: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.3977 - accuracy: 0.9661 - val_loss: 0.4805 - val_accuracy: 0.9519 - lr: 1.0000e-03\n",
      "Epoch 52/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.9744\n",
      "Epoch 52: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 0.3788 - accuracy: 0.9744 - val_loss: 0.5048 - val_accuracy: 0.9392 - lr: 1.0000e-03\n",
      "Epoch 53/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3695 - accuracy: 0.9731\n",
      "Epoch 53: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.3695 - accuracy: 0.9731 - val_loss: 0.4596 - val_accuracy: 0.9519 - lr: 1.0000e-03\n",
      "Epoch 54/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3766 - accuracy: 0.9686\n",
      "Epoch 54: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.3766 - accuracy: 0.9686 - val_loss: 0.4610 - val_accuracy: 0.9468 - lr: 1.0000e-03\n",
      "Epoch 55/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3934 - accuracy: 0.9622\n",
      "Epoch 55: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 0.3934 - accuracy: 0.9622 - val_loss: 0.4750 - val_accuracy: 0.9544 - lr: 1.0000e-03\n",
      "Epoch 56/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3823 - accuracy: 0.9763\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.3823 - accuracy: 0.9763 - val_loss: 0.5031 - val_accuracy: 0.9418 - lr: 1.0000e-03\n",
      "Epoch 57/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3712 - accuracy: 0.9725\n",
      "Epoch 57: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.3712 - accuracy: 0.9725 - val_loss: 0.4804 - val_accuracy: 0.9570 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3604 - accuracy: 0.9782\n",
      "Epoch 58: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.3604 - accuracy: 0.9782 - val_loss: 0.4500 - val_accuracy: 0.9443 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.9699\n",
      "Epoch 59: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.3696 - accuracy: 0.9699 - val_loss: 0.4809 - val_accuracy: 0.9418 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3688 - accuracy: 0.9699\n",
      "Epoch 60: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.3688 - accuracy: 0.9699 - val_loss: 0.4650 - val_accuracy: 0.9494 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3798 - accuracy: 0.9673\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "\n",
      "Epoch 61: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.3798 - accuracy: 0.9673 - val_loss: 0.4858 - val_accuracy: 0.9494 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3593 - accuracy: 0.9789\n",
      "Epoch 62: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.3593 - accuracy: 0.9789 - val_loss: 0.4762 - val_accuracy: 0.9468 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3796 - accuracy: 0.9654Restoring model weights from the end of the best epoch: 58.\n",
      "\n",
      "Epoch 63: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.3796 - accuracy: 0.9654 - val_loss: 0.4691 - val_accuracy: 0.9519 - lr: 1.0000e-05\n",
      "Epoch 63: early stopping\n",
      "25/25 [==============================] - 2s 59ms/step\n",
      "Fold 2 Validation Accuracy: 0.9519\n",
      "Fold 2 Validation F1 Score: 0.9567\n",
      "New best model saved for fold 2 with accuracy 0.9519\n",
      "Training fold 3/5\n",
      "Found 1578 validated image filenames belonging to 3 classes.\n",
      "Found 395 validated image filenames belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 2.6858 - accuracy: 0.5864\n",
      "Epoch 1: val_accuracy improved from -inf to 0.64304, saving model to ./models/best_small_model_fold_3.h5\n",
      "98/98 [==============================] - 13s 93ms/step - loss: 2.6858 - accuracy: 0.5864 - val_loss: 2.5424 - val_accuracy: 0.6430 - lr: 0.0100\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 2.2426 - accuracy: 0.7407\n",
      "Epoch 2: val_accuracy improved from 0.64304 to 0.83544, saving model to ./models/best_small_model_fold_3.h5\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 2.2426 - accuracy: 0.7407 - val_loss: 2.0810 - val_accuracy: 0.8354 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 2.0767 - accuracy: 0.7964\n",
      "Epoch 3: val_accuracy improved from 0.83544 to 0.85316, saving model to ./models/best_small_model_fold_3.h5\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 2.0767 - accuracy: 0.7964 - val_loss: 1.9304 - val_accuracy: 0.8532 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.9412 - accuracy: 0.8399\n",
      "Epoch 4: val_accuracy improved from 0.85316 to 0.89873, saving model to ./models/best_small_model_fold_3.h5\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1.9412 - accuracy: 0.8399 - val_loss: 1.7901 - val_accuracy: 0.8987 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.8216 - accuracy: 0.8713\n",
      "Epoch 5: val_accuracy improved from 0.89873 to 0.90380, saving model to ./models/best_small_model_fold_3.h5\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 1.8216 - accuracy: 0.8713 - val_loss: 1.7273 - val_accuracy: 0.9038 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.7670 - accuracy: 0.8528\n",
      "Epoch 6: val_accuracy improved from 0.90380 to 0.91646, saving model to ./models/best_small_model_fold_3.h5\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 1.7670 - accuracy: 0.8528 - val_loss: 1.6327 - val_accuracy: 0.9165 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.6794 - accuracy: 0.8803\n",
      "Epoch 7: val_accuracy improved from 0.91646 to 0.92152, saving model to ./models/best_small_model_fold_3.h5\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 1.6794 - accuracy: 0.8803 - val_loss: 1.5815 - val_accuracy: 0.9215 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.5782 - accuracy: 0.8995\n",
      "Epoch 8: val_accuracy did not improve from 0.92152\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 1.5782 - accuracy: 0.8995 - val_loss: 1.5271 - val_accuracy: 0.9165 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.5499 - accuracy: 0.8790\n",
      "Epoch 9: val_accuracy did not improve from 0.92152\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 1.5499 - accuracy: 0.8790 - val_loss: 1.4533 - val_accuracy: 0.9114 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.4565 - accuracy: 0.9082\n",
      "Epoch 10: val_accuracy improved from 0.92152 to 0.92405, saving model to ./models/best_small_model_fold_3.h5\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 1.4565 - accuracy: 0.9082 - val_loss: 1.3954 - val_accuracy: 0.9241 - lr: 0.0100\n",
      "Epoch 11/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.4247 - accuracy: 0.9040\n",
      "Epoch 11: val_accuracy did not improve from 0.92405\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 1.4247 - accuracy: 0.9040 - val_loss: 1.3548 - val_accuracy: 0.9139 - lr: 0.0100\n",
      "Epoch 12/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.3696 - accuracy: 0.8931\n",
      "Epoch 12: val_accuracy did not improve from 0.92405\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1.3696 - accuracy: 0.8931 - val_loss: 1.3033 - val_accuracy: 0.9215 - lr: 0.0100\n",
      "Epoch 13/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.3313 - accuracy: 0.9065\n",
      "Epoch 13: val_accuracy did not improve from 0.92405\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 1.3313 - accuracy: 0.9065 - val_loss: 1.2402 - val_accuracy: 0.9190 - lr: 0.0100\n",
      "Epoch 14/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.2645 - accuracy: 0.9065\n",
      "Epoch 14: val_accuracy improved from 0.92405 to 0.92911, saving model to ./models/best_small_model_fold_3.h5\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1.2645 - accuracy: 0.9065 - val_loss: 1.1864 - val_accuracy: 0.9291 - lr: 0.0100\n",
      "Epoch 15/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.2228 - accuracy: 0.9110\n",
      "Epoch 15: val_accuracy did not improve from 0.92911\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 1.2228 - accuracy: 0.9110 - val_loss: 1.1594 - val_accuracy: 0.9266 - lr: 0.0100\n",
      "Epoch 16/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.1664 - accuracy: 0.9200\n",
      "Epoch 16: val_accuracy did not improve from 0.92911\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1.1664 - accuracy: 0.9200 - val_loss: 1.1370 - val_accuracy: 0.9266 - lr: 0.0100\n",
      "Epoch 17/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.1382 - accuracy: 0.9174\n",
      "Epoch 17: val_accuracy did not improve from 0.92911\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 1.1382 - accuracy: 0.9174 - val_loss: 1.1352 - val_accuracy: 0.9215 - lr: 0.0100\n",
      "Epoch 18/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.0955 - accuracy: 0.9206\n",
      "Epoch 18: val_accuracy did not improve from 0.92911\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 1.0955 - accuracy: 0.9206 - val_loss: 1.0540 - val_accuracy: 0.9291 - lr: 0.0100\n",
      "Epoch 19/100\n",
      "97/98 [============================>.] - ETA: 0s - loss: 1.0244 - accuracy: 0.9356\n",
      "Epoch 19: val_accuracy improved from 0.92911 to 0.94430, saving model to ./models/best_small_model_fold_3.h5\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 1.0255 - accuracy: 0.9347 - val_loss: 0.9861 - val_accuracy: 0.9443 - lr: 0.0100\n",
      "Epoch 20/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.0121 - accuracy: 0.9232\n",
      "Epoch 20: val_accuracy improved from 0.94430 to 0.94937, saving model to ./models/best_small_model_fold_3.h5\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 1.0121 - accuracy: 0.9232 - val_loss: 0.9610 - val_accuracy: 0.9494 - lr: 0.0100\n",
      "Epoch 21/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9630 - accuracy: 0.9289\n",
      "Epoch 21: val_accuracy did not improve from 0.94937\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 0.9630 - accuracy: 0.9289 - val_loss: 0.9241 - val_accuracy: 0.9494 - lr: 0.0100\n",
      "Epoch 22/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9193 - accuracy: 0.9449\n",
      "Epoch 22: val_accuracy did not improve from 0.94937\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.9193 - accuracy: 0.9449 - val_loss: 0.9187 - val_accuracy: 0.9367 - lr: 0.0100\n",
      "Epoch 23/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9071 - accuracy: 0.9341\n",
      "Epoch 23: val_accuracy did not improve from 0.94937\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.9071 - accuracy: 0.9341 - val_loss: 0.8902 - val_accuracy: 0.9443 - lr: 0.0100\n",
      "Epoch 24/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8741 - accuracy: 0.9379\n",
      "Epoch 24: val_accuracy did not improve from 0.94937\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.8741 - accuracy: 0.9379 - val_loss: 0.8447 - val_accuracy: 0.9392 - lr: 0.0100\n",
      "Epoch 25/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8482 - accuracy: 0.9366\n",
      "Epoch 25: val_accuracy did not improve from 0.94937\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 0.8482 - accuracy: 0.9366 - val_loss: 0.8085 - val_accuracy: 0.9468 - lr: 0.0100\n",
      "Epoch 26/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8122 - accuracy: 0.9379\n",
      "Epoch 26: val_accuracy did not improve from 0.94937\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.8122 - accuracy: 0.9379 - val_loss: 0.8044 - val_accuracy: 0.9367 - lr: 0.0100\n",
      "Epoch 27/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7846 - accuracy: 0.9437\n",
      "Epoch 27: val_accuracy did not improve from 0.94937\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.7846 - accuracy: 0.9437 - val_loss: 0.8017 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 28/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7496 - accuracy: 0.9443\n",
      "Epoch 28: val_accuracy did not improve from 0.94937\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.7496 - accuracy: 0.9443 - val_loss: 0.7510 - val_accuracy: 0.9418 - lr: 0.0100\n",
      "Epoch 29/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6963 - accuracy: 0.9507\n",
      "Epoch 29: val_accuracy did not improve from 0.94937\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 0.6963 - accuracy: 0.9507 - val_loss: 0.7152 - val_accuracy: 0.9468 - lr: 0.0100\n",
      "Epoch 30/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6910 - accuracy: 0.9462\n",
      "Epoch 30: val_accuracy did not improve from 0.94937\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 0.6910 - accuracy: 0.9462 - val_loss: 0.7144 - val_accuracy: 0.9342 - lr: 0.0100\n",
      "Epoch 31/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6786 - accuracy: 0.9437\n",
      "Epoch 31: val_accuracy did not improve from 0.94937\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.6786 - accuracy: 0.9437 - val_loss: 0.7033 - val_accuracy: 0.9392 - lr: 0.0100\n",
      "Epoch 32/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6415 - accuracy: 0.9494\n",
      "Epoch 32: val_accuracy did not improve from 0.94937\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.6415 - accuracy: 0.9494 - val_loss: 0.6782 - val_accuracy: 0.9342 - lr: 0.0100\n",
      "Epoch 33/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6487 - accuracy: 0.9385\n",
      "Epoch 33: val_accuracy improved from 0.94937 to 0.95190, saving model to ./models/best_small_model_fold_3.h5\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.6487 - accuracy: 0.9385 - val_loss: 0.6639 - val_accuracy: 0.9519 - lr: 0.0100\n",
      "Epoch 34/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6149 - accuracy: 0.9430\n",
      "Epoch 34: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 0.6149 - accuracy: 0.9430 - val_loss: 0.6372 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 35/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5774 - accuracy: 0.9584\n",
      "Epoch 35: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 0.5774 - accuracy: 0.9584 - val_loss: 0.6089 - val_accuracy: 0.9443 - lr: 0.0100\n",
      "Epoch 36/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5604 - accuracy: 0.9552\n",
      "Epoch 36: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.5604 - accuracy: 0.9552 - val_loss: 0.6046 - val_accuracy: 0.9494 - lr: 0.0100\n",
      "Epoch 37/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5473 - accuracy: 0.9545\n",
      "Epoch 37: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.5473 - accuracy: 0.9545 - val_loss: 0.6401 - val_accuracy: 0.9190 - lr: 0.0100\n",
      "Epoch 38/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5215 - accuracy: 0.9552\n",
      "Epoch 38: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.5215 - accuracy: 0.9552 - val_loss: 0.5973 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 39/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5200 - accuracy: 0.9520\n",
      "Epoch 39: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 0.5200 - accuracy: 0.9520 - val_loss: 0.5734 - val_accuracy: 0.9392 - lr: 0.0100\n",
      "Epoch 40/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4850 - accuracy: 0.9622\n",
      "Epoch 40: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.4850 - accuracy: 0.9622 - val_loss: 0.5475 - val_accuracy: 0.9494 - lr: 0.0100\n",
      "Epoch 41/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4747 - accuracy: 0.9635\n",
      "Epoch 41: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.4747 - accuracy: 0.9635 - val_loss: 0.5338 - val_accuracy: 0.9418 - lr: 0.0100\n",
      "Epoch 42/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4427 - accuracy: 0.9680\n",
      "Epoch 42: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.4427 - accuracy: 0.9680 - val_loss: 0.5215 - val_accuracy: 0.9443 - lr: 0.0100\n",
      "Epoch 43/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4282 - accuracy: 0.9661\n",
      "Epoch 43: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 0.4282 - accuracy: 0.9661 - val_loss: 0.5023 - val_accuracy: 0.9519 - lr: 0.0100\n",
      "Epoch 44/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4391 - accuracy: 0.9603\n",
      "Epoch 44: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.4391 - accuracy: 0.9603 - val_loss: 0.4829 - val_accuracy: 0.9468 - lr: 0.0100\n",
      "Epoch 45/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4320 - accuracy: 0.9577\n",
      "Epoch 45: val_accuracy improved from 0.95190 to 0.95696, saving model to ./models/best_small_model_fold_3.h5\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.4320 - accuracy: 0.9577 - val_loss: 0.4531 - val_accuracy: 0.9570 - lr: 0.0100\n",
      "Epoch 46/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4119 - accuracy: 0.9520\n",
      "Epoch 46: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 0.4119 - accuracy: 0.9520 - val_loss: 0.4841 - val_accuracy: 0.9392 - lr: 0.0100\n",
      "Epoch 47/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3920 - accuracy: 0.9616\n",
      "Epoch 47: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 0.3920 - accuracy: 0.9616 - val_loss: 0.4409 - val_accuracy: 0.9519 - lr: 0.0100\n",
      "Epoch 48/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3778 - accuracy: 0.9635\n",
      "Epoch 48: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.3778 - accuracy: 0.9635 - val_loss: 0.4749 - val_accuracy: 0.9418 - lr: 0.0100\n",
      "Epoch 49/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3697 - accuracy: 0.9699\n",
      "Epoch 49: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.3697 - accuracy: 0.9699 - val_loss: 0.4403 - val_accuracy: 0.9544 - lr: 0.0100\n",
      "Epoch 50/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3654 - accuracy: 0.9616\n",
      "Epoch 50: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.3654 - accuracy: 0.9616 - val_loss: 0.4154 - val_accuracy: 0.9519 - lr: 0.0100\n",
      "Epoch 51/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3577 - accuracy: 0.9590\n",
      "Epoch 51: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.3577 - accuracy: 0.9590 - val_loss: 0.4407 - val_accuracy: 0.9443 - lr: 0.0100\n",
      "Epoch 52/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3554 - accuracy: 0.9565\n",
      "Epoch 52: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.3554 - accuracy: 0.9565 - val_loss: 0.4404 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 53/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3383 - accuracy: 0.9590\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "\n",
      "Epoch 53: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.3383 - accuracy: 0.9590 - val_loss: 0.4300 - val_accuracy: 0.9494 - lr: 0.0100\n",
      "Epoch 54/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3272 - accuracy: 0.9629\n",
      "Epoch 54: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.3272 - accuracy: 0.9629 - val_loss: 0.4179 - val_accuracy: 0.9570 - lr: 1.0000e-03\n",
      "Epoch 55/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3087 - accuracy: 0.9686\n",
      "Epoch 55: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.3087 - accuracy: 0.9686 - val_loss: 0.4096 - val_accuracy: 0.9468 - lr: 1.0000e-03\n",
      "Epoch 56/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3027 - accuracy: 0.9782\n",
      "Epoch 56: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 0.3027 - accuracy: 0.9782 - val_loss: 0.4269 - val_accuracy: 0.9519 - lr: 1.0000e-03\n",
      "Epoch 57/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2986 - accuracy: 0.9744\n",
      "Epoch 57: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 0.2986 - accuracy: 0.9744 - val_loss: 0.3802 - val_accuracy: 0.9519 - lr: 1.0000e-03\n",
      "Epoch 58/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3116 - accuracy: 0.9699\n",
      "Epoch 58: val_accuracy improved from 0.95696 to 0.95949, saving model to ./models/best_small_model_fold_3.h5\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.3116 - accuracy: 0.9699 - val_loss: 0.3790 - val_accuracy: 0.9595 - lr: 1.0000e-03\n",
      "Epoch 59/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2947 - accuracy: 0.9744\n",
      "Epoch 59: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.2947 - accuracy: 0.9744 - val_loss: 0.3923 - val_accuracy: 0.9494 - lr: 1.0000e-03\n",
      "Epoch 60/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3134 - accuracy: 0.9643\n",
      "Epoch 60: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 0.3134 - accuracy: 0.9643 - val_loss: 0.3700 - val_accuracy: 0.9544 - lr: 1.0000e-03\n",
      "Epoch 61/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.9750\n",
      "Epoch 61: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.2849 - accuracy: 0.9750 - val_loss: 0.4112 - val_accuracy: 0.9443 - lr: 1.0000e-03\n",
      "Epoch 62/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2979 - accuracy: 0.9699\n",
      "Epoch 62: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.2979 - accuracy: 0.9699 - val_loss: 0.3663 - val_accuracy: 0.9544 - lr: 1.0000e-03\n",
      "Epoch 63/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2792 - accuracy: 0.9782\n",
      "Epoch 63: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 0.2792 - accuracy: 0.9782 - val_loss: 0.3932 - val_accuracy: 0.9443 - lr: 1.0000e-03\n",
      "Epoch 64/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.9827\n",
      "Epoch 64: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 0.2861 - accuracy: 0.9827 - val_loss: 0.3619 - val_accuracy: 0.9494 - lr: 1.0000e-03\n",
      "Epoch 65/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3043 - accuracy: 0.9680\n",
      "Epoch 65: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.3043 - accuracy: 0.9680 - val_loss: 0.4023 - val_accuracy: 0.9443 - lr: 1.0000e-03\n",
      "Epoch 66/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2960 - accuracy: 0.9706\n",
      "Epoch 66: val_accuracy did not improve from 0.95949\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.2960 - accuracy: 0.9706 - val_loss: 0.3874 - val_accuracy: 0.9544 - lr: 1.0000e-03\n",
      "Epoch 67/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2909 - accuracy: 0.9744\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "\n",
      "Epoch 67: val_accuracy improved from 0.95949 to 0.96709, saving model to ./models/best_small_model_fold_3.h5\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.2909 - accuracy: 0.9744 - val_loss: 0.3698 - val_accuracy: 0.9671 - lr: 1.0000e-03\n",
      "Epoch 68/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2838 - accuracy: 0.9763\n",
      "Epoch 68: val_accuracy did not improve from 0.96709\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.2838 - accuracy: 0.9763 - val_loss: 0.3822 - val_accuracy: 0.9494 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2800 - accuracy: 0.9763Restoring model weights from the end of the best epoch: 64.\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.96709\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.2800 - accuracy: 0.9763 - val_loss: 0.3725 - val_accuracy: 0.9646 - lr: 1.0000e-04\n",
      "Epoch 69: early stopping\n",
      "25/25 [==============================] - 2s 62ms/step\n",
      "Fold 3 Validation Accuracy: 0.9646\n",
      "Fold 3 Validation F1 Score: 0.9442\n",
      "New best model saved for fold 3 with accuracy 0.9646\n",
      "Training fold 4/5\n",
      "Found 1579 validated image filenames belonging to 3 classes.\n",
      "Found 394 validated image filenames belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 2.6983 - accuracy: 0.5611\n",
      "Epoch 1: val_accuracy improved from -inf to 0.61421, saving model to ./models/best_small_model_fold_4.h5\n",
      "98/98 [==============================] - 13s 93ms/step - loss: 2.6983 - accuracy: 0.5611 - val_loss: 2.6047 - val_accuracy: 0.6142 - lr: 0.0100\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 2.2391 - accuracy: 0.7486\n",
      "Epoch 2: val_accuracy improved from 0.61421 to 0.73604, saving model to ./models/best_small_model_fold_4.h5\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 2.2391 - accuracy: 0.7486 - val_loss: 2.2091 - val_accuracy: 0.7360 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 2.0124 - accuracy: 0.8106\n",
      "Epoch 3: val_accuracy improved from 0.73604 to 0.82234, saving model to ./models/best_small_model_fold_4.h5\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 2.0124 - accuracy: 0.8106 - val_loss: 1.9941 - val_accuracy: 0.8223 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.9213 - accuracy: 0.8381\n",
      "Epoch 4: val_accuracy did not improve from 0.82234\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 1.9213 - accuracy: 0.8381 - val_loss: 1.9007 - val_accuracy: 0.8198 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.7975 - accuracy: 0.8688\n",
      "Epoch 5: val_accuracy improved from 0.82234 to 0.85533, saving model to ./models/best_small_model_fold_4.h5\n",
      "98/98 [==============================] - 9s 90ms/step - loss: 1.7975 - accuracy: 0.8688 - val_loss: 1.7500 - val_accuracy: 0.8553 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.7416 - accuracy: 0.8669\n",
      "Epoch 6: val_accuracy did not improve from 0.85533\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 1.7416 - accuracy: 0.8669 - val_loss: 1.8448 - val_accuracy: 0.8096 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.6481 - accuracy: 0.8836\n",
      "Epoch 7: val_accuracy improved from 0.85533 to 0.86294, saving model to ./models/best_small_model_fold_4.h5\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 1.6481 - accuracy: 0.8836 - val_loss: 1.6455 - val_accuracy: 0.8629 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.5771 - accuracy: 0.8893\n",
      "Epoch 8: val_accuracy improved from 0.86294 to 0.87563, saving model to ./models/best_small_model_fold_4.h5\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 1.5771 - accuracy: 0.8893 - val_loss: 1.5546 - val_accuracy: 0.8756 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.5249 - accuracy: 0.8906\n",
      "Epoch 9: val_accuracy improved from 0.87563 to 0.88325, saving model to ./models/best_small_model_fold_4.h5\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 1.5249 - accuracy: 0.8906 - val_loss: 1.5141 - val_accuracy: 0.8832 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.4324 - accuracy: 0.9130\n",
      "Epoch 10: val_accuracy improved from 0.88325 to 0.88832, saving model to ./models/best_small_model_fold_4.h5\n",
      "98/98 [==============================] - 9s 90ms/step - loss: 1.4324 - accuracy: 0.9130 - val_loss: 1.4764 - val_accuracy: 0.8883 - lr: 0.0100\n",
      "Epoch 11/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.3990 - accuracy: 0.9040\n",
      "Epoch 11: val_accuracy improved from 0.88832 to 0.89086, saving model to ./models/best_small_model_fold_4.h5\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 1.3990 - accuracy: 0.9040 - val_loss: 1.4150 - val_accuracy: 0.8909 - lr: 0.0100\n",
      "Epoch 12/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.3148 - accuracy: 0.9155\n",
      "Epoch 12: val_accuracy did not improve from 0.89086\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 1.3148 - accuracy: 0.9155 - val_loss: 1.3781 - val_accuracy: 0.8883 - lr: 0.0100\n",
      "Epoch 13/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.2825 - accuracy: 0.9245\n",
      "Epoch 13: val_accuracy did not improve from 0.89086\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 1.2825 - accuracy: 0.9245 - val_loss: 1.3292 - val_accuracy: 0.8756 - lr: 0.0100\n",
      "Epoch 14/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.2213 - accuracy: 0.9187\n",
      "Epoch 14: val_accuracy improved from 0.89086 to 0.89848, saving model to ./models/best_small_model_fold_4.h5\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 1.2213 - accuracy: 0.9187 - val_loss: 1.2934 - val_accuracy: 0.8985 - lr: 0.0100\n",
      "Epoch 15/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.1998 - accuracy: 0.9213\n",
      "Epoch 15: val_accuracy improved from 0.89848 to 0.90355, saving model to ./models/best_small_model_fold_4.h5\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 1.1998 - accuracy: 0.9213 - val_loss: 1.2590 - val_accuracy: 0.9036 - lr: 0.0100\n",
      "Epoch 16/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.1311 - accuracy: 0.9264\n",
      "Epoch 16: val_accuracy did not improve from 0.90355\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 1.1311 - accuracy: 0.9264 - val_loss: 1.2046 - val_accuracy: 0.8858 - lr: 0.0100\n",
      "Epoch 17/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.0763 - accuracy: 0.9437\n",
      "Epoch 17: val_accuracy did not improve from 0.90355\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 1.0763 - accuracy: 0.9437 - val_loss: 1.1890 - val_accuracy: 0.8883 - lr: 0.0100\n",
      "Epoch 18/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.0602 - accuracy: 0.9277\n",
      "Epoch 18: val_accuracy did not improve from 0.90355\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 1.0602 - accuracy: 0.9277 - val_loss: 1.1490 - val_accuracy: 0.8985 - lr: 0.0100\n",
      "Epoch 19/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.0091 - accuracy: 0.9405\n",
      "Epoch 19: val_accuracy improved from 0.90355 to 0.90863, saving model to ./models/best_small_model_fold_4.h5\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 1.0091 - accuracy: 0.9405 - val_loss: 1.0617 - val_accuracy: 0.9086 - lr: 0.0100\n",
      "Epoch 20/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9827 - accuracy: 0.9328\n",
      "Epoch 20: val_accuracy did not improve from 0.90863\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.9827 - accuracy: 0.9328 - val_loss: 1.0613 - val_accuracy: 0.8985 - lr: 0.0100\n",
      "Epoch 21/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9739 - accuracy: 0.9226\n",
      "Epoch 21: val_accuracy did not improve from 0.90863\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.9739 - accuracy: 0.9226 - val_loss: 1.0640 - val_accuracy: 0.9010 - lr: 0.0100\n",
      "Epoch 22/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9151 - accuracy: 0.9392\n",
      "Epoch 22: val_accuracy improved from 0.90863 to 0.91371, saving model to ./models/best_small_model_fold_4.h5\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 0.9151 - accuracy: 0.9392 - val_loss: 1.0058 - val_accuracy: 0.9137 - lr: 0.0100\n",
      "Epoch 23/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8984 - accuracy: 0.9354\n",
      "Epoch 23: val_accuracy improved from 0.91371 to 0.91624, saving model to ./models/best_small_model_fold_4.h5\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 0.8984 - accuracy: 0.9354 - val_loss: 0.9253 - val_accuracy: 0.9162 - lr: 0.0100\n",
      "Epoch 24/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8522 - accuracy: 0.9386\n",
      "Epoch 24: val_accuracy did not improve from 0.91624\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.8522 - accuracy: 0.9386 - val_loss: 0.9278 - val_accuracy: 0.9112 - lr: 0.0100\n",
      "Epoch 25/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7986 - accuracy: 0.9552\n",
      "Epoch 25: val_accuracy did not improve from 0.91624\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 0.7986 - accuracy: 0.9552 - val_loss: 0.9343 - val_accuracy: 0.9036 - lr: 0.0100\n",
      "Epoch 26/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7877 - accuracy: 0.9475\n",
      "Epoch 26: val_accuracy did not improve from 0.91624\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.7877 - accuracy: 0.9475 - val_loss: 0.9077 - val_accuracy: 0.8959 - lr: 0.0100\n",
      "Epoch 27/100\n",
      "97/98 [============================>.] - ETA: 0s - loss: 0.7826 - accuracy: 0.9381\n",
      "Epoch 27: val_accuracy did not improve from 0.91624\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.7816 - accuracy: 0.9386 - val_loss: 0.8868 - val_accuracy: 0.9010 - lr: 0.0100\n",
      "Epoch 28/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7370 - accuracy: 0.9463\n",
      "Epoch 28: val_accuracy did not improve from 0.91624\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.7370 - accuracy: 0.9463 - val_loss: 0.8724 - val_accuracy: 0.9010 - lr: 0.0100\n",
      "Epoch 29/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7256 - accuracy: 0.9424\n",
      "Epoch 29: val_accuracy did not improve from 0.91624\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.7256 - accuracy: 0.9424 - val_loss: 0.7811 - val_accuracy: 0.9112 - lr: 0.0100\n",
      "Epoch 30/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6770 - accuracy: 0.9488\n",
      "Epoch 30: val_accuracy did not improve from 0.91624\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.6770 - accuracy: 0.9488 - val_loss: 0.8348 - val_accuracy: 0.8985 - lr: 0.0100\n",
      "Epoch 31/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6480 - accuracy: 0.9539\n",
      "Epoch 31: val_accuracy did not improve from 0.91624\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 0.6480 - accuracy: 0.9539 - val_loss: 0.7884 - val_accuracy: 0.8934 - lr: 0.0100\n",
      "Epoch 32/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6166 - accuracy: 0.9623\n",
      "Epoch 32: val_accuracy did not improve from 0.91624\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 0.6166 - accuracy: 0.9623 - val_loss: 0.7725 - val_accuracy: 0.9137 - lr: 0.0100\n",
      "Epoch 33/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6057 - accuracy: 0.9584\n",
      "Epoch 33: val_accuracy improved from 0.91624 to 0.92893, saving model to ./models/best_small_model_fold_4.h5\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.6057 - accuracy: 0.9584 - val_loss: 0.7080 - val_accuracy: 0.9289 - lr: 0.0100\n",
      "Epoch 34/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6019 - accuracy: 0.9559\n",
      "Epoch 34: val_accuracy did not improve from 0.92893\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.6019 - accuracy: 0.9559 - val_loss: 0.6918 - val_accuracy: 0.9289 - lr: 0.0100\n",
      "Epoch 35/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5676 - accuracy: 0.9597\n",
      "Epoch 35: val_accuracy improved from 0.92893 to 0.93147, saving model to ./models/best_small_model_fold_4.h5\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 0.5676 - accuracy: 0.9597 - val_loss: 0.6844 - val_accuracy: 0.9315 - lr: 0.0100\n",
      "Epoch 36/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5553 - accuracy: 0.9578\n",
      "Epoch 36: val_accuracy did not improve from 0.93147\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.5553 - accuracy: 0.9578 - val_loss: 0.6962 - val_accuracy: 0.9162 - lr: 0.0100\n",
      "Epoch 37/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5244 - accuracy: 0.9635\n",
      "Epoch 37: val_accuracy did not improve from 0.93147\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.5244 - accuracy: 0.9635 - val_loss: 0.6950 - val_accuracy: 0.9188 - lr: 0.0100\n",
      "Epoch 38/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5112 - accuracy: 0.9623\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.93147\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.5112 - accuracy: 0.9623 - val_loss: 0.7168 - val_accuracy: 0.9086 - lr: 0.0100\n",
      "Epoch 39/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4764 - accuracy: 0.9674\n",
      "Epoch 39: val_accuracy did not improve from 0.93147\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.4764 - accuracy: 0.9674 - val_loss: 0.6547 - val_accuracy: 0.9239 - lr: 1.0000e-03\n",
      "Epoch 40/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4947 - accuracy: 0.9642\n",
      "Epoch 40: val_accuracy did not improve from 0.93147\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.4947 - accuracy: 0.9642 - val_loss: 0.6619 - val_accuracy: 0.9188 - lr: 1.0000e-03\n",
      "Epoch 41/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4809 - accuracy: 0.9718\n",
      "Epoch 41: val_accuracy did not improve from 0.93147\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.4809 - accuracy: 0.9718 - val_loss: 0.6429 - val_accuracy: 0.9213 - lr: 1.0000e-03\n",
      "Epoch 42/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4828 - accuracy: 0.9693\n",
      "Epoch 42: val_accuracy did not improve from 0.93147\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.4828 - accuracy: 0.9693 - val_loss: 0.6479 - val_accuracy: 0.9112 - lr: 1.0000e-03\n",
      "Epoch 43/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5034 - accuracy: 0.9605\n",
      "Epoch 43: val_accuracy did not improve from 0.93147\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.5034 - accuracy: 0.9605 - val_loss: 0.6647 - val_accuracy: 0.9213 - lr: 1.0000e-03\n",
      "Epoch 44/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4817 - accuracy: 0.9629\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.93147\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 0.4817 - accuracy: 0.9629 - val_loss: 0.6559 - val_accuracy: 0.9315 - lr: 1.0000e-03\n",
      "Epoch 45/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4744 - accuracy: 0.9687\n",
      "Epoch 45: val_accuracy did not improve from 0.93147\n",
      "98/98 [==============================] - 9s 90ms/step - loss: 0.4744 - accuracy: 0.9687 - val_loss: 0.6533 - val_accuracy: 0.9289 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "97/98 [============================>.] - ETA: 0s - loss: 0.4845 - accuracy: 0.9652Restoring model weights from the end of the best epoch: 41.\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.93147\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.4838 - accuracy: 0.9655 - val_loss: 0.6886 - val_accuracy: 0.9137 - lr: 1.0000e-04\n",
      "Epoch 46: early stopping\n",
      "25/25 [==============================] - 2s 63ms/step\n",
      "Fold 4 Validation Accuracy: 0.9137\n",
      "Fold 4 Validation F1 Score: 0.9157\n",
      "Training fold 5/5\n",
      "Found 1579 validated image filenames belonging to 3 classes.\n",
      "Found 394 validated image filenames belonging to 3 classes.\n",
      "Epoch 1/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 2.7356 - accuracy: 0.5528\n",
      "Epoch 1: val_accuracy improved from -inf to 0.67005, saving model to ./models/best_small_model_fold_5.h5\n",
      "98/98 [==============================] - 13s 89ms/step - loss: 2.7356 - accuracy: 0.5528 - val_loss: 2.3778 - val_accuracy: 0.6701 - lr: 0.0100\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 2.2681 - accuracy: 0.7383\n",
      "Epoch 2: val_accuracy improved from 0.67005 to 0.83756, saving model to ./models/best_small_model_fold_5.h5\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 2.2681 - accuracy: 0.7383 - val_loss: 2.0221 - val_accuracy: 0.8376 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 2.0354 - accuracy: 0.8061\n",
      "Epoch 3: val_accuracy improved from 0.83756 to 0.86802, saving model to ./models/best_small_model_fold_5.h5\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 2.0354 - accuracy: 0.8061 - val_loss: 1.8514 - val_accuracy: 0.8680 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.9284 - accuracy: 0.8349\n",
      "Epoch 4: val_accuracy improved from 0.86802 to 0.88325, saving model to ./models/best_small_model_fold_5.h5\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 1.9284 - accuracy: 0.8349 - val_loss: 1.7627 - val_accuracy: 0.8832 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.8401 - accuracy: 0.8452\n",
      "Epoch 5: val_accuracy did not improve from 0.88325\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 1.8401 - accuracy: 0.8452 - val_loss: 1.6902 - val_accuracy: 0.8832 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.7534 - accuracy: 0.8586\n",
      "Epoch 6: val_accuracy improved from 0.88325 to 0.89848, saving model to ./models/best_small_model_fold_5.h5\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 1.7534 - accuracy: 0.8586 - val_loss: 1.6060 - val_accuracy: 0.8985 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.6821 - accuracy: 0.8714\n",
      "Epoch 7: val_accuracy improved from 0.89848 to 0.91371, saving model to ./models/best_small_model_fold_5.h5\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 1.6821 - accuracy: 0.8714 - val_loss: 1.5435 - val_accuracy: 0.9137 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.6112 - accuracy: 0.8746\n",
      "Epoch 8: val_accuracy improved from 0.91371 to 0.92893, saving model to ./models/best_small_model_fold_5.h5\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 1.6112 - accuracy: 0.8746 - val_loss: 1.4796 - val_accuracy: 0.9289 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.5311 - accuracy: 0.8887\n",
      "Epoch 9: val_accuracy improved from 0.92893 to 0.94416, saving model to ./models/best_small_model_fold_5.h5\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 1.5311 - accuracy: 0.8887 - val_loss: 1.3707 - val_accuracy: 0.9442 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.4384 - accuracy: 0.9060\n",
      "Epoch 10: val_accuracy improved from 0.94416 to 0.95431, saving model to ./models/best_small_model_fold_5.h5\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 1.4384 - accuracy: 0.9060 - val_loss: 1.3456 - val_accuracy: 0.9543 - lr: 0.0100\n",
      "Epoch 11/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.4140 - accuracy: 0.9015\n",
      "Epoch 11: val_accuracy did not improve from 0.95431\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 1.4140 - accuracy: 0.9015 - val_loss: 1.3091 - val_accuracy: 0.9315 - lr: 0.0100\n",
      "Epoch 12/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.3437 - accuracy: 0.9117\n",
      "Epoch 12: val_accuracy did not improve from 0.95431\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 1.3437 - accuracy: 0.9117 - val_loss: 1.3160 - val_accuracy: 0.9162 - lr: 0.0100\n",
      "Epoch 13/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.3037 - accuracy: 0.9079\n",
      "Epoch 13: val_accuracy did not improve from 0.95431\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1.3037 - accuracy: 0.9079 - val_loss: 1.1983 - val_accuracy: 0.9391 - lr: 0.0100\n",
      "Epoch 14/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.2543 - accuracy: 0.9066\n",
      "Epoch 14: val_accuracy did not improve from 0.95431\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 1.2543 - accuracy: 0.9066 - val_loss: 1.1632 - val_accuracy: 0.9340 - lr: 0.0100\n",
      "Epoch 15/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.1870 - accuracy: 0.9187\n",
      "Epoch 15: val_accuracy did not improve from 0.95431\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 1.1870 - accuracy: 0.9187 - val_loss: 1.1182 - val_accuracy: 0.9442 - lr: 0.0100\n",
      "Epoch 16/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.1777 - accuracy: 0.9040\n",
      "Epoch 16: val_accuracy did not improve from 0.95431\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 1.1777 - accuracy: 0.9040 - val_loss: 1.1144 - val_accuracy: 0.9162 - lr: 0.0100\n",
      "Epoch 17/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.1012 - accuracy: 0.9245\n",
      "Epoch 17: val_accuracy did not improve from 0.95431\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 1.1012 - accuracy: 0.9245 - val_loss: 1.0457 - val_accuracy: 0.9315 - lr: 0.0100\n",
      "Epoch 18/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.0676 - accuracy: 0.9315\n",
      "Epoch 18: val_accuracy did not improve from 0.95431\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 1.0676 - accuracy: 0.9315 - val_loss: 1.0228 - val_accuracy: 0.9365 - lr: 0.0100\n",
      "Epoch 19/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.0432 - accuracy: 0.9232\n",
      "Epoch 19: val_accuracy did not improve from 0.95431\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 1.0432 - accuracy: 0.9232 - val_loss: 0.9846 - val_accuracy: 0.9391 - lr: 0.0100\n",
      "Epoch 20/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.0117 - accuracy: 0.9213\n",
      "Epoch 20: val_accuracy did not improve from 0.95431\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 1.0117 - accuracy: 0.9213 - val_loss: 0.9252 - val_accuracy: 0.9467 - lr: 0.0100\n",
      "Epoch 21/100\n",
      "97/98 [============================>.] - ETA: 0s - loss: 0.9385 - accuracy: 0.9394\n",
      "Epoch 21: val_accuracy did not improve from 0.95431\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 0.9389 - accuracy: 0.9392 - val_loss: 0.9028 - val_accuracy: 0.9391 - lr: 0.0100\n",
      "Epoch 22/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9445 - accuracy: 0.9296\n",
      "Epoch 22: val_accuracy improved from 0.95431 to 0.96447, saving model to ./models/best_small_model_fold_5.h5\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.9445 - accuracy: 0.9296 - val_loss: 0.8536 - val_accuracy: 0.9645 - lr: 0.0100\n",
      "Epoch 23/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8996 - accuracy: 0.9296\n",
      "Epoch 23: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 87ms/step - loss: 0.8996 - accuracy: 0.9296 - val_loss: 0.8654 - val_accuracy: 0.9340 - lr: 0.0100\n",
      "Epoch 24/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8583 - accuracy: 0.9379\n",
      "Epoch 24: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 0.8583 - accuracy: 0.9379 - val_loss: 0.8249 - val_accuracy: 0.9416 - lr: 0.0100\n",
      "Epoch 25/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8444 - accuracy: 0.9283\n",
      "Epoch 25: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.8444 - accuracy: 0.9283 - val_loss: 0.7773 - val_accuracy: 0.9518 - lr: 0.0100\n",
      "Epoch 26/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8343 - accuracy: 0.9207\n",
      "Epoch 26: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.8343 - accuracy: 0.9207 - val_loss: 0.7704 - val_accuracy: 0.9518 - lr: 0.0100\n",
      "Epoch 27/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7789 - accuracy: 0.9354\n",
      "Epoch 27: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.7789 - accuracy: 0.9354 - val_loss: 0.7319 - val_accuracy: 0.9543 - lr: 0.0100\n",
      "Epoch 28/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7470 - accuracy: 0.9431\n",
      "Epoch 28: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.7470 - accuracy: 0.9431 - val_loss: 0.7070 - val_accuracy: 0.9569 - lr: 0.0100\n",
      "Epoch 29/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7208 - accuracy: 0.9418\n",
      "Epoch 29: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 0.7208 - accuracy: 0.9418 - val_loss: 0.6791 - val_accuracy: 0.9594 - lr: 0.0100\n",
      "Epoch 30/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7005 - accuracy: 0.9431\n",
      "Epoch 30: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.7005 - accuracy: 0.9431 - val_loss: 0.6733 - val_accuracy: 0.9492 - lr: 0.0100\n",
      "Epoch 31/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6863 - accuracy: 0.9405\n",
      "Epoch 31: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.6863 - accuracy: 0.9405 - val_loss: 0.6757 - val_accuracy: 0.9365 - lr: 0.0100\n",
      "Epoch 32/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6608 - accuracy: 0.9386\n",
      "Epoch 32: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.6608 - accuracy: 0.9386 - val_loss: 0.6185 - val_accuracy: 0.9594 - lr: 0.0100\n",
      "Epoch 33/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6477 - accuracy: 0.9379\n",
      "Epoch 33: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.6477 - accuracy: 0.9379 - val_loss: 0.6103 - val_accuracy: 0.9569 - lr: 0.0100\n",
      "Epoch 34/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5847 - accuracy: 0.9591\n",
      "Epoch 34: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.5847 - accuracy: 0.9591 - val_loss: 0.6206 - val_accuracy: 0.9416 - lr: 0.0100\n",
      "Epoch 35/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5897 - accuracy: 0.9482\n",
      "Epoch 35: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.5897 - accuracy: 0.9482 - val_loss: 0.5696 - val_accuracy: 0.9518 - lr: 0.0100\n",
      "Epoch 36/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5641 - accuracy: 0.9488\n",
      "Epoch 36: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.5641 - accuracy: 0.9488 - val_loss: 0.5729 - val_accuracy: 0.9518 - lr: 0.0100\n",
      "Epoch 37/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5558 - accuracy: 0.9559\n",
      "Epoch 37: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.5558 - accuracy: 0.9559 - val_loss: 0.5582 - val_accuracy: 0.9391 - lr: 0.0100\n",
      "Epoch 38/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5293 - accuracy: 0.9450\n",
      "Epoch 38: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.5293 - accuracy: 0.9450 - val_loss: 0.5343 - val_accuracy: 0.9569 - lr: 0.0100\n",
      "Epoch 39/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5044 - accuracy: 0.9578\n",
      "Epoch 39: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 0.5044 - accuracy: 0.9578 - val_loss: 0.5253 - val_accuracy: 0.9543 - lr: 0.0100\n",
      "Epoch 40/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5150 - accuracy: 0.9539\n",
      "Epoch 40: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.5150 - accuracy: 0.9539 - val_loss: 0.4998 - val_accuracy: 0.9518 - lr: 0.0100\n",
      "Epoch 41/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4751 - accuracy: 0.9559\n",
      "Epoch 41: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 0.4751 - accuracy: 0.9559 - val_loss: 0.4766 - val_accuracy: 0.9569 - lr: 0.0100\n",
      "Epoch 42/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4753 - accuracy: 0.9565\n",
      "Epoch 42: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.4753 - accuracy: 0.9565 - val_loss: 0.4478 - val_accuracy: 0.9619 - lr: 0.0100\n",
      "Epoch 43/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4285 - accuracy: 0.9635\n",
      "Epoch 43: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.4285 - accuracy: 0.9635 - val_loss: 0.4672 - val_accuracy: 0.9569 - lr: 0.0100\n",
      "Epoch 44/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4451 - accuracy: 0.9584\n",
      "Epoch 44: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 81ms/step - loss: 0.4451 - accuracy: 0.9584 - val_loss: 0.4333 - val_accuracy: 0.9619 - lr: 0.0100\n",
      "Epoch 45/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4063 - accuracy: 0.9642\n",
      "Epoch 45: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 9s 88ms/step - loss: 0.4063 - accuracy: 0.9642 - val_loss: 0.4317 - val_accuracy: 0.9594 - lr: 0.0100\n",
      "Epoch 46/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4095 - accuracy: 0.9578\n",
      "Epoch 46: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.4095 - accuracy: 0.9578 - val_loss: 0.4101 - val_accuracy: 0.9518 - lr: 0.0100\n",
      "Epoch 47/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3834 - accuracy: 0.9616\n",
      "Epoch 47: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 0.3834 - accuracy: 0.9616 - val_loss: 0.4371 - val_accuracy: 0.9569 - lr: 0.0100\n",
      "Epoch 48/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3921 - accuracy: 0.9584\n",
      "Epoch 48: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.3921 - accuracy: 0.9584 - val_loss: 0.4086 - val_accuracy: 0.9492 - lr: 0.0100\n",
      "Epoch 49/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.9699\n",
      "Epoch 49: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.3495 - accuracy: 0.9699 - val_loss: 0.3974 - val_accuracy: 0.9467 - lr: 0.0100\n",
      "Epoch 50/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3466 - accuracy: 0.9693\n",
      "Epoch 50: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.3466 - accuracy: 0.9693 - val_loss: 0.3830 - val_accuracy: 0.9569 - lr: 0.0100\n",
      "Epoch 51/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3536 - accuracy: 0.9623\n",
      "Epoch 51: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.3536 - accuracy: 0.9623 - val_loss: 0.3695 - val_accuracy: 0.9543 - lr: 0.0100\n",
      "Epoch 52/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3266 - accuracy: 0.9687\n",
      "Epoch 52: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 84ms/step - loss: 0.3266 - accuracy: 0.9687 - val_loss: 0.3834 - val_accuracy: 0.9518 - lr: 0.0100\n",
      "Epoch 53/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3216 - accuracy: 0.9655\n",
      "Epoch 53: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.3216 - accuracy: 0.9655 - val_loss: 0.3822 - val_accuracy: 0.9467 - lr: 0.0100\n",
      "Epoch 54/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3097 - accuracy: 0.9674\n",
      "Epoch 54: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.3097 - accuracy: 0.9674 - val_loss: 0.3253 - val_accuracy: 0.9645 - lr: 0.0100\n",
      "Epoch 55/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3065 - accuracy: 0.9674\n",
      "Epoch 55: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.3065 - accuracy: 0.9674 - val_loss: 0.3498 - val_accuracy: 0.9492 - lr: 0.0100\n",
      "Epoch 56/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.9680\n",
      "Epoch 56: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 0.2932 - accuracy: 0.9680 - val_loss: 0.3289 - val_accuracy: 0.9594 - lr: 0.0100\n",
      "Epoch 57/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.9718\n",
      "Epoch 57: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 82ms/step - loss: 0.2890 - accuracy: 0.9718 - val_loss: 0.3241 - val_accuracy: 0.9442 - lr: 0.0100\n",
      "Epoch 58/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.9744\n",
      "Epoch 58: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.2620 - accuracy: 0.9744 - val_loss: 0.3129 - val_accuracy: 0.9619 - lr: 0.0100\n",
      "Epoch 59/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.9782\n",
      "Epoch 59: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.2486 - accuracy: 0.9782 - val_loss: 0.3271 - val_accuracy: 0.9645 - lr: 0.0100\n",
      "Epoch 60/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2893 - accuracy: 0.9629\n",
      "Epoch 60: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.2893 - accuracy: 0.9629 - val_loss: 0.4201 - val_accuracy: 0.9315 - lr: 0.0100\n",
      "Epoch 61/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2824 - accuracy: 0.9603\n",
      "Epoch 61: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 83ms/step - loss: 0.2824 - accuracy: 0.9603 - val_loss: 0.3128 - val_accuracy: 0.9543 - lr: 0.0100\n",
      "Epoch 62/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2754 - accuracy: 0.9591\n",
      "Epoch 62: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 9s 87ms/step - loss: 0.2754 - accuracy: 0.9591 - val_loss: 0.2669 - val_accuracy: 0.9619 - lr: 0.0100\n",
      "Epoch 63/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2562 - accuracy: 0.9667\n",
      "Epoch 63: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.2562 - accuracy: 0.9667 - val_loss: 0.3167 - val_accuracy: 0.9492 - lr: 0.0100\n",
      "Epoch 64/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.9591\n",
      "Epoch 64: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.2525 - accuracy: 0.9591 - val_loss: 0.2938 - val_accuracy: 0.9594 - lr: 0.0100\n",
      "Epoch 65/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2263 - accuracy: 0.9770\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "\n",
      "Epoch 65: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 9s 89ms/step - loss: 0.2263 - accuracy: 0.9770 - val_loss: 0.2947 - val_accuracy: 0.9518 - lr: 0.0100\n",
      "Epoch 66/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2051 - accuracy: 0.9782\n",
      "Epoch 66: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 86ms/step - loss: 0.2051 - accuracy: 0.9782 - val_loss: 0.2782 - val_accuracy: 0.9543 - lr: 1.0000e-03\n",
      "Epoch 67/100\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2292 - accuracy: 0.9693Restoring model weights from the end of the best epoch: 62.\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.96447\n",
      "98/98 [==============================] - 8s 85ms/step - loss: 0.2292 - accuracy: 0.9693 - val_loss: 0.2700 - val_accuracy: 0.9543 - lr: 1.0000e-03\n",
      "Epoch 67: early stopping\n",
      "25/25 [==============================] - 2s 63ms/step\n",
      "Fold 5 Validation Accuracy: 0.9543\n",
      "Fold 5 Validation F1 Score: 0.9567\n",
      "\n",
      "Average Validation Accuracy across all folds: 0.9397\n",
      "Average Validation F1 Score across all folds: 0.9394\n",
      "\n",
      "Best Fold: 3 with Validation Accuracy: 0.9646\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_index, test_index) in enumerate(kfold.split(image_dataframe)):\n",
    "    print(f\"Training fold {fold_idx + 1}/{num_folds}\")\n",
    "\n",
    "    train_data = image_dataframe.iloc[train_index]\n",
    "    test_data = image_dataframe.iloc[test_index]\n",
    "\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_data,\n",
    "        x_col='file_path',\n",
    "        y_col='label',\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=test_data,\n",
    "        x_col='file_path',\n",
    "        y_col='label',\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # 1. Define the Input layer\n",
    "    input_layer = Input(shape=(img_width, img_height, 3))\n",
    "    \n",
    "    # 2. Load the pre-trained MobileNetV2 model without the top classification layers\n",
    "    base_model = MobileNet(weights='imagenet', include_top=False, input_tensor=input_layer, input_shape=(img_width, img_height, 3), alpha=0.25)\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # 4. Add custom layers on top of the base_model\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # Removed activation from here\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)  # Added Activation layer separately\n",
    "    x = Dropout(0.6)(x)\n",
    "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # 5. Create the Functional API model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    # 6. Compile the model with the SGD optimizer\n",
    "    opt = SGD(learning_rate=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    # 7. Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1),\n",
    "        ModelCheckpoint(\n",
    "            filepath=f'./models/best_small_model_fold_{fold_idx + 1}.h5',\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        TensorBoard(log_dir=f'./logs/fold_{fold_idx + 1}')\n",
    "    ]\n",
    "\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "        epochs=100,\n",
    "        validation_data=test_generator,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    y_true = test_generator.classes\n",
    "    y_pred_probs = model.predict(test_generator, verbose=1)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    fold_accuracies.append(accuracy)\n",
    "    print(f\"Fold {fold_idx + 1} Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    fold_f1_scores.append(f1)\n",
    "    print(f\"Fold {fold_idx + 1} Validation F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # 10. Plot and Save Loss Curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss Curve - Fold {fold_idx + 1}')\n",
    "    plt.legend()\n",
    "    loss_curve_path = f'./plots/loss/MNV2_loss_curve_fold_{fold_idx + 1}.png'\n",
    "    plt.savefig(loss_curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 11. Plot and Save Accuracy Curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy', color='green')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Accuracy Curve - Fold {fold_idx + 1}')\n",
    "    plt.legend()\n",
    "    accuracy_curve_path = f'./plots/accuracy/MNV2_accuracy_curve_fold_{fold_idx + 1}.png'\n",
    "    plt.savefig(accuracy_curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 12. Track the Best Fold\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_fold_idx = fold_idx\n",
    "        best_test_data = test_data.copy()\n",
    "        best_model_path = f'./models/best_overall_small_model_fold_{fold_idx + 1}.h5'\n",
    "        model.save(best_model_path)\n",
    "        print(f\"New best model saved for fold {fold_idx + 1} with accuracy {accuracy:.4f}\")\n",
    "\n",
    "# 13. Final Evaluation Across Folds\n",
    "avg_accuracy = np.mean(fold_accuracies)\n",
    "avg_f1 = np.mean(fold_f1_scores)\n",
    "print(f\"\\nAverage Validation Accuracy across all folds: {avg_accuracy:.4f}\")\n",
    "print(f\"Average Validation F1 Score across all folds: {avg_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Fold: {best_fold_idx + 1} with Validation Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 0.9139240384101868\n",
      "Fold 1 F1 Score: 0.9236447767388417\n",
      "Fold 2 Accuracy: 0.9518987536430359\n",
      "Fold 2 F1 Score: 0.9566658394523077\n",
      "Fold 3 Accuracy: 0.9645569324493408\n",
      "Fold 3 F1 Score: 0.9442377668247889\n",
      "Fold 4 Accuracy: 0.913705587387085\n",
      "Fold 4 F1 Score: 0.9157040729263772\n",
      "Fold 5 Accuracy: 0.9543147087097168\n",
      "Fold 5 F1 Score: 0.956740127073351\n",
      "Average Test accuracy: 0.9396800041198731\n",
      "Standard Deviation of Test accuracy: 0.021542395761374077\n",
      "Average F1 Score: 0.9393985166031333\n"
     ]
    }
   ],
   "source": [
    "avg_test_acc = np.mean(fold_accuracies)\n",
    "std_test_acc = np.std(fold_accuracies)\n",
    "avg_f1_score = np.mean(fold_f1_scores)\n",
    "\n",
    "for fold_idx in range(num_folds):\n",
    "    print(f\"Fold {fold_idx + 1} Accuracy: {fold_accuracies[fold_idx]}\")\n",
    "    print(f\"Fold {fold_idx + 1} F1 Score: {fold_f1_scores[fold_idx]}\")\n",
    "    \n",
    "print(\"Average Test accuracy:\", avg_test_acc)\n",
    "print(\"Standard Deviation of Test accuracy:\", std_test_acc)\n",
    "print(\"Average F1 Score:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Best Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best fold 4 with accuracy 0.9492385983467102 saved as validation data.\n",
      "Best model saved at best_model_fold_4.keras\n"
     ]
    }
   ],
   "source": [
    "# After all folds, save the best test fold to a file\n",
    "if best_test_data is not None:\n",
    "    best_test_data.to_csv(f'best_fold_{best_fold_idx + 1}_validation_data.csv', index=False)\n",
    "    print(f\"Best fold {best_fold_idx + 1} with accuracy {best_accuracy} saved as validation data.\")\n",
    "    print(f\"Best model saved at {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the Keras model to a TFLite model (No Quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('best_model_fold_4.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpc62ih_ay/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpc62ih_ay/assets\n",
      "2024-11-15 20:10:51.314296: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-11-15 20:10:51.314312: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-11-15 20:10:51.314532: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpc62ih_ay\n",
      "2024-11-15 20:10:51.325113: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-11-15 20:10:51.325123: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpc62ih_ay\n",
      "2024-11-15 20:10:51.345569: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n",
      "2024-11-15 20:10:51.354250: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-11-15 20:10:51.570968: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpc62ih_ay\n",
      "2024-11-15 20:10:51.654229: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 339698 microseconds.\n",
      "2024-11-15 20:10:51.726863: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('best_model_fold_4.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing model on exported test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test data from best_fold_4_validation_data.csv\n",
      "                                           file_path   label\n",
      "0  /home/sec_team2/Engagement/Dataset/screen/1651...  screen\n",
      "1  /home/sec_team2/Engagement/Dataset/screen/1814...  screen\n",
      "2  /home/sec_team2/Engagement/Dataset/screen/1657...  screen\n",
      "3  /home/sec_team2/Engagement/Dataset/screen/1644...  screen\n",
      "4  /home/sec_team2/Engagement/Dataset/screen/1646...  screen\n"
     ]
    }
   ],
   "source": [
    "# Load the exported test set\n",
    "test_set_path = f'best_fold_{best_fold_idx + 1}_validation_data.csv'\n",
    "test_df = pd.read_csv(test_set_path)\n",
    "print(f\"Loaded test data from {test_set_path}\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 394 validated image filenames belonging to 3 classes.\n",
      "Loaded best model from: best_model_fold_4.keras\n",
      "25/25 [==============================] - 2s 59ms/step - loss: 0.5363 - accuracy: 0.9416\n",
      "Test Loss: 0.5362608432769775\n",
      "Test Accuracy: 0.9416243433952332\n",
      "25/25 [==============================] - 2s 57ms/step\n",
      "Test F1 Score: 0.954227492317899\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       paper       0.96      0.98      0.97       112\n",
      "      screen       0.94      0.96      0.95       156\n",
      "      wander       0.97      0.93      0.95       126\n",
      "\n",
      "    accuracy                           0.95       394\n",
      "   macro avg       0.96      0.96      0.96       394\n",
      "weighted avg       0.95      0.95      0.95       394\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure the same preprocessing pipeline is applied to the test set\n",
    "test_generator = CustomImageDataGenerator().flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='file_path',\n",
    "    y_col='label',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Don't shuffle for evaluation\n",
    ")\n",
    "\n",
    "# Step 3: Load the best saved model\n",
    "best_model = load_model(f'best_model_fold_{best_fold_idx + 1}.keras')\n",
    "print(f\"Loaded best model from: {best_model_path}\")\n",
    "\n",
    "# best_model_path = f'best_model_fold_{best_fold_idx + 1}.h5'  # Adjust if necessary\n",
    "# best_model = load_model(best_model_path)\n",
    "# print(f\"Loaded best model from: {best_model_path}\")\n",
    "\n",
    "# Step 4: Evaluate the model on the test set\n",
    "results = best_model.evaluate(test_generator, verbose=1)\n",
    "test_loss, test_accuracy = results[0], results[1]\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Step 5: Get predictions\n",
    "y_pred_probs = best_model.predict(test_generator, verbose=1)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Convert probabilities to class predictions\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Step 6: Calculate Metrics\n",
    "# F1 Score\n",
    "test_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"Test F1 Score: {test_f1}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=test_generator.class_indices.keys(),\n",
    "            yticklabels=test_generator.class_indices.keys())\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT training as finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Secondary K-Fold Loop for QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize K-Fold (Same as Primary Loop)\n",
    "kfold_qat = KFold(n_splits=num_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "# 2. Initialize Tracking Variables for QAT\n",
    "qat_fold_accuracies = []\n",
    "qat_fold_f1_scores = []\n",
    "best_qat_accuracy = 0\n",
    "best_qat_fold_idx = -1\n",
    "\n",
    "# 3. Define QAT Parameters\n",
    "QAT_EPOCHS = 20  # Fine-tuning epochs after QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying QAT on fold 1/5\n",
      "Found 1578 validated image filenames belonging to 3 classes.\n",
      "Found 395 validated image filenames belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model for fold 1.\n",
      "Applied QAT to the model for fold 1.\n",
      "Epoch 1/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7658 - accuracy: 0.9469\n",
      "Epoch 1: val_accuracy improved from -inf to 0.90633, saving model to ./quantized_models/qat_small_model_fold_1.h5\n",
      "98/98 [==============================] - 15s 116ms/step - loss: 0.7658 - accuracy: 0.9469 - val_loss: 0.9123 - val_accuracy: 0.9063 - lr: 0.0100\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 0.7596 - accuracy: 0.9379\n",
      "Epoch 2: val_accuracy improved from 0.90633 to 0.90886, saving model to ./quantized_models/qat_small_model_fold_1.h5\n",
      "98/98 [==============================] - 10s 105ms/step - loss: 0.7596 - accuracy: 0.9379 - val_loss: 0.8737 - val_accuracy: 0.9089 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7207 - accuracy: 0.9494\n",
      "Epoch 3: val_accuracy did not improve from 0.90886\n",
      "98/98 [==============================] - 11s 111ms/step - loss: 0.7207 - accuracy: 0.9494 - val_loss: 0.8356 - val_accuracy: 0.9038 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6966 - accuracy: 0.9456\n",
      "Epoch 4: val_accuracy did not improve from 0.90886\n",
      "98/98 [==============================] - 11s 108ms/step - loss: 0.6966 - accuracy: 0.9456 - val_loss: 0.9287 - val_accuracy: 0.9013 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6840 - accuracy: 0.9405\n",
      "Epoch 5: val_accuracy did not improve from 0.90886\n",
      "98/98 [==============================] - 11s 110ms/step - loss: 0.6840 - accuracy: 0.9405 - val_loss: 0.7921 - val_accuracy: 0.8987 - lr: 0.0100\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6524 - accuracy: 0.9481\n",
      "Epoch 6: val_accuracy did not improve from 0.90886\n",
      "98/98 [==============================] - 10s 105ms/step - loss: 0.6524 - accuracy: 0.9481 - val_loss: 0.7601 - val_accuracy: 0.9063 - lr: 0.0100\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6323 - accuracy: 0.9501\n",
      "Epoch 7: val_accuracy improved from 0.90886 to 0.91392, saving model to ./quantized_models/qat_small_model_fold_1.h5\n",
      "98/98 [==============================] - 11s 108ms/step - loss: 0.6323 - accuracy: 0.9501 - val_loss: 0.7339 - val_accuracy: 0.9139 - lr: 0.0100\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6090 - accuracy: 0.9475\n",
      "Epoch 8: val_accuracy did not improve from 0.91392\n",
      "98/98 [==============================] - 11s 114ms/step - loss: 0.6090 - accuracy: 0.9475 - val_loss: 0.7555 - val_accuracy: 0.9139 - lr: 0.0100\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5705 - accuracy: 0.9526\n",
      "Epoch 9: val_accuracy improved from 0.91392 to 0.91646, saving model to ./quantized_models/qat_small_model_fold_1.h5\n",
      "98/98 [==============================] - 11s 107ms/step - loss: 0.5705 - accuracy: 0.9526 - val_loss: 0.6952 - val_accuracy: 0.9165 - lr: 0.0100\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5678 - accuracy: 0.9552\n",
      "Epoch 10: val_accuracy did not improve from 0.91646\n",
      "98/98 [==============================] - 10s 106ms/step - loss: 0.5678 - accuracy: 0.9552 - val_loss: 0.7565 - val_accuracy: 0.9089 - lr: 0.0100\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5745 - accuracy: 0.9430\n",
      "Epoch 11: val_accuracy did not improve from 0.91646\n",
      "98/98 [==============================] - 10s 102ms/step - loss: 0.5745 - accuracy: 0.9430 - val_loss: 0.6517 - val_accuracy: 0.9165 - lr: 0.0100\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5138 - accuracy: 0.9565\n",
      "Epoch 12: val_accuracy did not improve from 0.91646\n",
      "98/98 [==============================] - 11s 117ms/step - loss: 0.5138 - accuracy: 0.9565 - val_loss: 0.6605 - val_accuracy: 0.9114 - lr: 0.0100\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5000 - accuracy: 0.9558\n",
      "Epoch 13: val_accuracy did not improve from 0.91646\n",
      "98/98 [==============================] - 11s 109ms/step - loss: 0.5000 - accuracy: 0.9558 - val_loss: 0.6366 - val_accuracy: 0.9139 - lr: 0.0100\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4906 - accuracy: 0.9552\n",
      "Epoch 14: val_accuracy did not improve from 0.91646\n",
      "98/98 [==============================] - 11s 113ms/step - loss: 0.4906 - accuracy: 0.9552 - val_loss: 0.7093 - val_accuracy: 0.9114 - lr: 0.0100\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4844 - accuracy: 0.9577\n",
      "Epoch 15: val_accuracy did not improve from 0.91646\n",
      "98/98 [==============================] - 11s 110ms/step - loss: 0.4844 - accuracy: 0.9577 - val_loss: 0.6281 - val_accuracy: 0.9063 - lr: 0.0100\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4462 - accuracy: 0.9680\n",
      "Epoch 16: val_accuracy did not improve from 0.91646\n",
      "98/98 [==============================] - 11s 107ms/step - loss: 0.4462 - accuracy: 0.9680 - val_loss: 0.6642 - val_accuracy: 0.9139 - lr: 0.0100\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4307 - accuracy: 0.9686\n",
      "Epoch 17: val_accuracy did not improve from 0.91646\n",
      "98/98 [==============================] - 10s 100ms/step - loss: 0.4307 - accuracy: 0.9686 - val_loss: 0.5949 - val_accuracy: 0.9165 - lr: 0.0100\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4254 - accuracy: 0.9673\n",
      "Epoch 18: val_accuracy did not improve from 0.91646\n",
      "98/98 [==============================] - 12s 120ms/step - loss: 0.4254 - accuracy: 0.9673 - val_loss: 0.6041 - val_accuracy: 0.9139 - lr: 0.0100\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.9629\n",
      "Epoch 19: val_accuracy did not improve from 0.91646\n",
      "98/98 [==============================] - 10s 103ms/step - loss: 0.3998 - accuracy: 0.9629 - val_loss: 0.5355 - val_accuracy: 0.9165 - lr: 0.0100\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4085 - accuracy: 0.9597\n",
      "Epoch 20: val_accuracy improved from 0.91646 to 0.91899, saving model to ./quantized_models/qat_small_model_fold_1.h5\n",
      "98/98 [==============================] - 12s 119ms/step - loss: 0.4085 - accuracy: 0.9597 - val_loss: 0.5926 - val_accuracy: 0.9190 - lr: 0.0100\n",
      "25/25 [==============================] - 2s 74ms/step\n",
      "Fold 1 QAT Validation Accuracy: 0.9190\n",
      "Fold 1 QAT Validation F1 Score: 0.9107\n",
      "New best QAT model saved for fold 1 with accuracy 0.9190\n",
      "\n",
      "Applying QAT on fold 2/5\n",
      "Found 1578 validated image filenames belonging to 3 classes.\n",
      "Found 395 validated image filenames belonging to 3 classes.\n",
      "Loaded pretrained model for fold 2.\n",
      "Applied QAT to the model for fold 2.\n",
      "Epoch 1/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4100 - accuracy: 0.9571\n",
      "Epoch 1: val_accuracy improved from -inf to 0.93165, saving model to ./quantized_models/qat_small_model_fold_2.h5\n",
      "98/98 [==============================] - 15s 121ms/step - loss: 0.4100 - accuracy: 0.9571 - val_loss: 0.5840 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 0.3895 - accuracy: 0.9635\n",
      "Epoch 2: val_accuracy improved from 0.93165 to 0.94177, saving model to ./quantized_models/qat_small_model_fold_2.h5\n",
      "98/98 [==============================] - 11s 114ms/step - loss: 0.3895 - accuracy: 0.9635 - val_loss: 0.4816 - val_accuracy: 0.9418 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3868 - accuracy: 0.9616\n",
      "Epoch 3: val_accuracy improved from 0.94177 to 0.95190, saving model to ./quantized_models/qat_small_model_fold_2.h5\n",
      "98/98 [==============================] - 11s 112ms/step - loss: 0.3868 - accuracy: 0.9616 - val_loss: 0.4558 - val_accuracy: 0.9519 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3681 - accuracy: 0.9629\n",
      "Epoch 4: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 10s 107ms/step - loss: 0.3681 - accuracy: 0.9629 - val_loss: 0.4874 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3383 - accuracy: 0.9738\n",
      "Epoch 5: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 10s 104ms/step - loss: 0.3383 - accuracy: 0.9738 - val_loss: 0.4506 - val_accuracy: 0.9367 - lr: 0.0100\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3608 - accuracy: 0.9577\n",
      "Epoch 6: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 11s 109ms/step - loss: 0.3608 - accuracy: 0.9577 - val_loss: 0.4308 - val_accuracy: 0.9519 - lr: 0.0100\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3521 - accuracy: 0.9597\n",
      "Epoch 7: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 12s 121ms/step - loss: 0.3521 - accuracy: 0.9597 - val_loss: 0.4377 - val_accuracy: 0.9519 - lr: 0.0100\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3302 - accuracy: 0.9603\n",
      "Epoch 8: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 11s 109ms/step - loss: 0.3302 - accuracy: 0.9603 - val_loss: 0.4302 - val_accuracy: 0.9443 - lr: 0.0100\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3241 - accuracy: 0.9635\n",
      "Epoch 9: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 10s 104ms/step - loss: 0.3241 - accuracy: 0.9635 - val_loss: 0.4522 - val_accuracy: 0.9418 - lr: 0.0100\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2987 - accuracy: 0.9686\n",
      "Epoch 10: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 10s 107ms/step - loss: 0.2987 - accuracy: 0.9686 - val_loss: 0.4443 - val_accuracy: 0.9392 - lr: 0.0100\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2971 - accuracy: 0.9661\n",
      "Epoch 11: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 10s 103ms/step - loss: 0.2971 - accuracy: 0.9661 - val_loss: 0.3921 - val_accuracy: 0.9494 - lr: 1.0000e-03\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2816 - accuracy: 0.9725\n",
      "Epoch 12: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 10s 105ms/step - loss: 0.2816 - accuracy: 0.9725 - val_loss: 0.4106 - val_accuracy: 0.9443 - lr: 1.0000e-03\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2845 - accuracy: 0.9757\n",
      "Epoch 13: val_accuracy improved from 0.95190 to 0.95443, saving model to ./quantized_models/qat_small_model_fold_2.h5\n",
      "98/98 [==============================] - 10s 106ms/step - loss: 0.2845 - accuracy: 0.9757 - val_loss: 0.3706 - val_accuracy: 0.9544 - lr: 1.0000e-03\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.9706\n",
      "Epoch 14: val_accuracy improved from 0.95443 to 0.95696, saving model to ./quantized_models/qat_small_model_fold_2.h5\n",
      "98/98 [==============================] - 11s 107ms/step - loss: 0.2850 - accuracy: 0.9706 - val_loss: 0.3811 - val_accuracy: 0.9570 - lr: 1.0000e-03\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.9661\n",
      "Epoch 15: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 11s 111ms/step - loss: 0.2849 - accuracy: 0.9661 - val_loss: 0.3806 - val_accuracy: 0.9544 - lr: 1.0000e-03\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.9718\n",
      "Epoch 16: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 11s 107ms/step - loss: 0.2863 - accuracy: 0.9718 - val_loss: 0.3903 - val_accuracy: 0.9544 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2750 - accuracy: 0.9744\n",
      "Epoch 17: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 11s 116ms/step - loss: 0.2750 - accuracy: 0.9744 - val_loss: 0.3908 - val_accuracy: 0.9519 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 0.9686Restoring model weights from the end of the best epoch: 13.\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 11s 115ms/step - loss: 0.2820 - accuracy: 0.9686 - val_loss: 0.4001 - val_accuracy: 0.9519 - lr: 1.0000e-05\n",
      "Epoch 18: early stopping\n",
      "25/25 [==============================] - 2s 72ms/step\n",
      "Fold 2 QAT Validation Accuracy: 0.9519\n",
      "Fold 2 QAT Validation F1 Score: 0.9669\n",
      "New best QAT model saved for fold 2 with accuracy 0.9519\n",
      "\n",
      "Applying QAT on fold 3/5\n",
      "Found 1578 validated image filenames belonging to 3 classes.\n",
      "Found 395 validated image filenames belonging to 3 classes.\n",
      "Loaded pretrained model for fold 3.\n",
      "Applied QAT to the model for fold 3.\n",
      "Epoch 1/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3151 - accuracy: 0.9603\n",
      "Epoch 1: val_accuracy improved from -inf to 0.94937, saving model to ./quantized_models/qat_small_model_fold_3.h5\n",
      "98/98 [==============================] - 15s 117ms/step - loss: 0.3151 - accuracy: 0.9603 - val_loss: 0.3960 - val_accuracy: 0.9494 - lr: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2981 - accuracy: 0.9680\n",
      "Epoch 2: val_accuracy did not improve from 0.94937\n",
      "98/98 [==============================] - 10s 105ms/step - loss: 0.2981 - accuracy: 0.9680 - val_loss: 0.3774 - val_accuracy: 0.9367 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2801 - accuracy: 0.9693\n",
      "Epoch 3: val_accuracy did not improve from 0.94937\n",
      "98/98 [==============================] - 11s 107ms/step - loss: 0.2801 - accuracy: 0.9693 - val_loss: 0.3988 - val_accuracy: 0.9316 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2839 - accuracy: 0.9673\n",
      "Epoch 4: val_accuracy did not improve from 0.94937\n",
      "98/98 [==============================] - 11s 114ms/step - loss: 0.2839 - accuracy: 0.9673 - val_loss: 0.3945 - val_accuracy: 0.9367 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2731 - accuracy: 0.9635\n",
      "Epoch 5: val_accuracy did not improve from 0.94937\n",
      "98/98 [==============================] - 11s 108ms/step - loss: 0.2731 - accuracy: 0.9635 - val_loss: 0.3721 - val_accuracy: 0.9392 - lr: 1.0000e-03\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.9635\n",
      "Epoch 6: val_accuracy improved from 0.94937 to 0.95190, saving model to ./quantized_models/qat_small_model_fold_3.h5\n",
      "98/98 [==============================] - 10s 107ms/step - loss: 0.2854 - accuracy: 0.9635 - val_loss: 0.3639 - val_accuracy: 0.9519 - lr: 1.0000e-03\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.9673\n",
      "Epoch 7: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 11s 114ms/step - loss: 0.2709 - accuracy: 0.9673 - val_loss: 0.3781 - val_accuracy: 0.9418 - lr: 1.0000e-03\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2591 - accuracy: 0.9738\n",
      "Epoch 8: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 11s 114ms/step - loss: 0.2591 - accuracy: 0.9738 - val_loss: 0.3459 - val_accuracy: 0.9468 - lr: 1.0000e-03\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2527 - accuracy: 0.9776\n",
      "Epoch 9: val_accuracy did not improve from 0.95190\n",
      "98/98 [==============================] - 11s 114ms/step - loss: 0.2527 - accuracy: 0.9776 - val_loss: 0.3772 - val_accuracy: 0.9443 - lr: 1.0000e-03\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2646 - accuracy: 0.9706\n",
      "Epoch 10: val_accuracy improved from 0.95190 to 0.95696, saving model to ./quantized_models/qat_small_model_fold_3.h5\n",
      "98/98 [==============================] - 11s 112ms/step - loss: 0.2646 - accuracy: 0.9706 - val_loss: 0.3476 - val_accuracy: 0.9570 - lr: 1.0000e-03\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 0.9667\n",
      "Epoch 11: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 11s 110ms/step - loss: 0.2658 - accuracy: 0.9667 - val_loss: 0.3618 - val_accuracy: 0.9544 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.9744\n",
      "Epoch 12: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 11s 108ms/step - loss: 0.2489 - accuracy: 0.9744 - val_loss: 0.3538 - val_accuracy: 0.9443 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 0.9750\n",
      "Epoch 13: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 10s 104ms/step - loss: 0.2507 - accuracy: 0.9750 - val_loss: 0.3363 - val_accuracy: 0.9494 - lr: 1.0000e-05\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2710 - accuracy: 0.9667\n",
      "Epoch 14: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 11s 111ms/step - loss: 0.2710 - accuracy: 0.9667 - val_loss: 0.3491 - val_accuracy: 0.9570 - lr: 1.0000e-05\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2443 - accuracy: 0.9789\n",
      "Epoch 15: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 10s 105ms/step - loss: 0.2443 - accuracy: 0.9789 - val_loss: 0.3324 - val_accuracy: 0.9494 - lr: 1.0000e-05\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2613 - accuracy: 0.9744\n",
      "Epoch 16: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 10s 105ms/step - loss: 0.2613 - accuracy: 0.9744 - val_loss: 0.3574 - val_accuracy: 0.9494 - lr: 1.0000e-05\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2470 - accuracy: 0.9757\n",
      "Epoch 17: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 10s 102ms/step - loss: 0.2470 - accuracy: 0.9757 - val_loss: 0.3601 - val_accuracy: 0.9468 - lr: 1.0000e-05\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2441 - accuracy: 0.9808\n",
      "Epoch 18: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 11s 109ms/step - loss: 0.2441 - accuracy: 0.9808 - val_loss: 0.3515 - val_accuracy: 0.9519 - lr: 1.0000e-06\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.9750\n",
      "Epoch 19: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 11s 115ms/step - loss: 0.2509 - accuracy: 0.9750 - val_loss: 0.3520 - val_accuracy: 0.9468 - lr: 1.0000e-06\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9750Restoring model weights from the end of the best epoch: 15.\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.95696\n",
      "98/98 [==============================] - 11s 109ms/step - loss: 0.2506 - accuracy: 0.9750 - val_loss: 0.3370 - val_accuracy: 0.9519 - lr: 1.0000e-07\n",
      "Epoch 20: early stopping\n",
      "25/25 [==============================] - 2s 68ms/step\n",
      "Fold 3 QAT Validation Accuracy: 0.9519\n",
      "Fold 3 QAT Validation F1 Score: 0.9518\n",
      "\n",
      "Applying QAT on fold 4/5\n",
      "Found 1579 validated image filenames belonging to 3 classes.\n",
      "Found 394 validated image filenames belonging to 3 classes.\n",
      "Loaded pretrained model for fold 4.\n",
      "Applied QAT to the model for fold 4.\n",
      "Epoch 1/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5623 - accuracy: 0.9565\n",
      "Epoch 1: val_accuracy improved from -inf to 0.89340, saving model to ./quantized_models/qat_small_model_fold_4.h5\n",
      "98/98 [==============================] - 14s 110ms/step - loss: 0.5623 - accuracy: 0.9565 - val_loss: 0.7407 - val_accuracy: 0.8934 - lr: 0.0100\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 0.5750 - accuracy: 0.9405\n",
      "Epoch 2: val_accuracy improved from 0.89340 to 0.89594, saving model to ./quantized_models/qat_small_model_fold_4.h5\n",
      "98/98 [==============================] - 10s 106ms/step - loss: 0.5750 - accuracy: 0.9405 - val_loss: 0.7656 - val_accuracy: 0.8959 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5243 - accuracy: 0.9616\n",
      "Epoch 3: val_accuracy improved from 0.89594 to 0.91371, saving model to ./quantized_models/qat_small_model_fold_4.h5\n",
      "98/98 [==============================] - 11s 111ms/step - loss: 0.5243 - accuracy: 0.9616 - val_loss: 0.6838 - val_accuracy: 0.9137 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4989 - accuracy: 0.9559\n",
      "Epoch 4: val_accuracy did not improve from 0.91371\n",
      "98/98 [==============================] - 10s 105ms/step - loss: 0.4989 - accuracy: 0.9559 - val_loss: 0.6815 - val_accuracy: 0.8909 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4850 - accuracy: 0.9597\n",
      "Epoch 5: val_accuracy improved from 0.91371 to 0.92640, saving model to ./quantized_models/qat_small_model_fold_4.h5\n",
      "98/98 [==============================] - 11s 111ms/step - loss: 0.4850 - accuracy: 0.9597 - val_loss: 0.6041 - val_accuracy: 0.9264 - lr: 0.0100\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4657 - accuracy: 0.9591\n",
      "Epoch 6: val_accuracy did not improve from 0.92640\n",
      "98/98 [==============================] - 10s 105ms/step - loss: 0.4657 - accuracy: 0.9591 - val_loss: 0.6848 - val_accuracy: 0.9010 - lr: 0.0100\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4660 - accuracy: 0.9578\n",
      "Epoch 7: val_accuracy did not improve from 0.92640\n",
      "98/98 [==============================] - 11s 108ms/step - loss: 0.4660 - accuracy: 0.9578 - val_loss: 0.6271 - val_accuracy: 0.9036 - lr: 0.0100\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4393 - accuracy: 0.9648\n",
      "Epoch 8: val_accuracy did not improve from 0.92640\n",
      "98/98 [==============================] - 10s 101ms/step - loss: 0.4393 - accuracy: 0.9648 - val_loss: 0.6206 - val_accuracy: 0.9061 - lr: 1.0000e-03\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4282 - accuracy: 0.9706\n",
      "Epoch 9: val_accuracy did not improve from 0.92640\n",
      "98/98 [==============================] - 10s 102ms/step - loss: 0.4282 - accuracy: 0.9706 - val_loss: 0.5925 - val_accuracy: 0.9112 - lr: 1.0000e-03\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4309 - accuracy: 0.9648\n",
      "Epoch 10: val_accuracy did not improve from 0.92640\n",
      "98/98 [==============================] - 10s 100ms/step - loss: 0.4309 - accuracy: 0.9648 - val_loss: 0.5998 - val_accuracy: 0.9188 - lr: 1.0000e-03\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4413 - accuracy: 0.9629\n",
      "Epoch 11: val_accuracy did not improve from 0.92640\n",
      "98/98 [==============================] - 11s 113ms/step - loss: 0.4413 - accuracy: 0.9629 - val_loss: 0.5747 - val_accuracy: 0.9188 - lr: 1.0000e-03\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4104 - accuracy: 0.9718\n",
      "Epoch 12: val_accuracy did not improve from 0.92640\n",
      "98/98 [==============================] - 11s 108ms/step - loss: 0.4104 - accuracy: 0.9718 - val_loss: 0.5553 - val_accuracy: 0.9188 - lr: 1.0000e-03\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4230 - accuracy: 0.9699\n",
      "Epoch 13: val_accuracy did not improve from 0.92640\n",
      "98/98 [==============================] - 11s 108ms/step - loss: 0.4230 - accuracy: 0.9699 - val_loss: 0.5868 - val_accuracy: 0.9112 - lr: 1.0000e-03\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4218 - accuracy: 0.9661\n",
      "Epoch 14: val_accuracy did not improve from 0.92640\n",
      "98/98 [==============================] - 11s 112ms/step - loss: 0.4218 - accuracy: 0.9661 - val_loss: 0.5859 - val_accuracy: 0.9264 - lr: 1.0000e-03\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4110 - accuracy: 0.9693\n",
      "Epoch 15: val_accuracy did not improve from 0.92640\n",
      "98/98 [==============================] - 11s 110ms/step - loss: 0.4110 - accuracy: 0.9693 - val_loss: 0.5933 - val_accuracy: 0.9188 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4200 - accuracy: 0.9699\n",
      "Epoch 16: val_accuracy did not improve from 0.92640\n",
      "98/98 [==============================] - 10s 103ms/step - loss: 0.4200 - accuracy: 0.9699 - val_loss: 0.5921 - val_accuracy: 0.9112 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4153 - accuracy: 0.9693Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.92640\n",
      "98/98 [==============================] - 11s 113ms/step - loss: 0.4153 - accuracy: 0.9693 - val_loss: 0.5799 - val_accuracy: 0.9137 - lr: 1.0000e-05\n",
      "Epoch 17: early stopping\n",
      "25/25 [==============================] - 2s 66ms/step\n",
      "Fold 4 QAT Validation Accuracy: 0.9137\n",
      "Fold 4 QAT Validation F1 Score: 0.9136\n",
      "\n",
      "Applying QAT on fold 5/5\n",
      "Found 1579 validated image filenames belonging to 3 classes.\n",
      "Found 394 validated image filenames belonging to 3 classes.\n",
      "Loaded pretrained model for fold 5.\n",
      "Applied QAT to the model for fold 5.\n",
      "Epoch 1/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.9160 - accuracy: 0.9290\n",
      "Epoch 1: val_accuracy improved from -inf to 0.94670, saving model to ./quantized_models/qat_small_model_fold_5.h5\n",
      "98/98 [==============================] - 15s 118ms/step - loss: 0.9160 - accuracy: 0.9290 - val_loss: 0.8601 - val_accuracy: 0.9467 - lr: 0.0100\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 0.8637 - accuracy: 0.9399\n",
      "Epoch 2: val_accuracy did not improve from 0.94670\n",
      "98/98 [==============================] - 10s 103ms/step - loss: 0.8637 - accuracy: 0.9399 - val_loss: 0.8265 - val_accuracy: 0.9340 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8543 - accuracy: 0.9245\n",
      "Epoch 3: val_accuracy did not improve from 0.94670\n",
      "98/98 [==============================] - 11s 116ms/step - loss: 0.8543 - accuracy: 0.9245 - val_loss: 0.7937 - val_accuracy: 0.9340 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.8181 - accuracy: 0.9354\n",
      "Epoch 4: val_accuracy did not improve from 0.94670\n",
      "98/98 [==============================] - 10s 103ms/step - loss: 0.8181 - accuracy: 0.9354 - val_loss: 0.8130 - val_accuracy: 0.9239 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7784 - accuracy: 0.9443\n",
      "Epoch 5: val_accuracy improved from 0.94670 to 0.94924, saving model to ./quantized_models/qat_small_model_fold_5.h5\n",
      "98/98 [==============================] - 11s 113ms/step - loss: 0.7784 - accuracy: 0.9443 - val_loss: 0.7317 - val_accuracy: 0.9492 - lr: 0.0100\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7496 - accuracy: 0.9399\n",
      "Epoch 6: val_accuracy did not improve from 0.94924\n",
      "98/98 [==============================] - 10s 103ms/step - loss: 0.7496 - accuracy: 0.9399 - val_loss: 0.7354 - val_accuracy: 0.9442 - lr: 0.0100\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.7572 - accuracy: 0.9283\n",
      "Epoch 7: val_accuracy did not improve from 0.94924\n",
      "98/98 [==============================] - 11s 110ms/step - loss: 0.7572 - accuracy: 0.9283 - val_loss: 0.7090 - val_accuracy: 0.9340 - lr: 0.0100\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6990 - accuracy: 0.9469\n",
      "Epoch 8: val_accuracy did not improve from 0.94924\n",
      "98/98 [==============================] - 11s 111ms/step - loss: 0.6990 - accuracy: 0.9469 - val_loss: 0.7361 - val_accuracy: 0.8909 - lr: 0.0100\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6999 - accuracy: 0.9335\n",
      "Epoch 9: val_accuracy improved from 0.94924 to 0.95431, saving model to ./quantized_models/qat_small_model_fold_5.h5\n",
      "98/98 [==============================] - 10s 106ms/step - loss: 0.6999 - accuracy: 0.9335 - val_loss: 0.6517 - val_accuracy: 0.9543 - lr: 0.0100\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6594 - accuracy: 0.9463\n",
      "Epoch 10: val_accuracy did not improve from 0.95431\n",
      "98/98 [==============================] - 10s 106ms/step - loss: 0.6594 - accuracy: 0.9463 - val_loss: 0.6532 - val_accuracy: 0.9492 - lr: 0.0100\n",
      "Epoch 11/20\n",
      "97/98 [============================>.] - ETA: 0s - loss: 0.6346 - accuracy: 0.9381\n",
      "Epoch 11: val_accuracy improved from 0.95431 to 0.96701, saving model to ./quantized_models/qat_small_model_fold_5.h5\n",
      "98/98 [==============================] - 10s 101ms/step - loss: 0.6360 - accuracy: 0.9379 - val_loss: 0.6110 - val_accuracy: 0.9670 - lr: 0.0100\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6003 - accuracy: 0.9533\n",
      "Epoch 12: val_accuracy did not improve from 0.96701\n",
      "98/98 [==============================] - 10s 103ms/step - loss: 0.6003 - accuracy: 0.9533 - val_loss: 0.5857 - val_accuracy: 0.9492 - lr: 0.0100\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6057 - accuracy: 0.9399\n",
      "Epoch 13: val_accuracy did not improve from 0.96701\n",
      "98/98 [==============================] - 10s 106ms/step - loss: 0.6057 - accuracy: 0.9399 - val_loss: 0.5838 - val_accuracy: 0.9518 - lr: 0.0100\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5702 - accuracy: 0.9463\n",
      "Epoch 14: val_accuracy did not improve from 0.96701\n",
      "98/98 [==============================] - 10s 103ms/step - loss: 0.5702 - accuracy: 0.9463 - val_loss: 0.5665 - val_accuracy: 0.9467 - lr: 0.0100\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5556 - accuracy: 0.9565\n",
      "Epoch 15: val_accuracy did not improve from 0.96701\n",
      "98/98 [==============================] - 10s 106ms/step - loss: 0.5556 - accuracy: 0.9565 - val_loss: 0.5360 - val_accuracy: 0.9543 - lr: 0.0100\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5399 - accuracy: 0.9488\n",
      "Epoch 16: val_accuracy did not improve from 0.96701\n",
      "98/98 [==============================] - 10s 106ms/step - loss: 0.5399 - accuracy: 0.9488 - val_loss: 0.5562 - val_accuracy: 0.9442 - lr: 0.0100\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5121 - accuracy: 0.9520\n",
      "Epoch 17: val_accuracy did not improve from 0.96701\n",
      "98/98 [==============================] - 11s 111ms/step - loss: 0.5121 - accuracy: 0.9520 - val_loss: 0.5187 - val_accuracy: 0.9594 - lr: 0.0100\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4993 - accuracy: 0.9533\n",
      "Epoch 18: val_accuracy did not improve from 0.96701\n",
      "98/98 [==============================] - 10s 105ms/step - loss: 0.4993 - accuracy: 0.9533 - val_loss: 0.4896 - val_accuracy: 0.9594 - lr: 0.0100\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4601 - accuracy: 0.9571\n",
      "Epoch 19: val_accuracy did not improve from 0.96701\n",
      "98/98 [==============================] - 10s 104ms/step - loss: 0.4601 - accuracy: 0.9571 - val_loss: 0.4762 - val_accuracy: 0.9569 - lr: 0.0100\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4667 - accuracy: 0.9514\n",
      "Epoch 20: val_accuracy did not improve from 0.96701\n",
      "98/98 [==============================] - 10s 103ms/step - loss: 0.4667 - accuracy: 0.9514 - val_loss: 0.4717 - val_accuracy: 0.9543 - lr: 0.0100\n",
      "25/25 [==============================] - 2s 74ms/step\n",
      "Fold 5 QAT Validation Accuracy: 0.9543\n",
      "Fold 5 QAT Validation F1 Score: 0.9720\n",
      "New best QAT model saved for fold 5 with accuracy 0.9543\n"
     ]
    }
   ],
   "source": [
    "# 4. Iterate through each fold for QAT\n",
    "for fold_idx, (train_index, test_index) in enumerate(kfold_qat.split(image_dataframe)):\n",
    "    print(f\"\\nApplying QAT on fold {fold_idx + 1}/{num_folds}\")\n",
    "\n",
    "    train_data = image_dataframe.iloc[train_index]\n",
    "    test_data = image_dataframe.iloc[test_index]\n",
    "\n",
    "    # Data Generators\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_data,\n",
    "        x_col='file_path',\n",
    "        y_col='label',\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,  # Increased batch size\n",
    "        class_mode='categorical',\n",
    "        shuffle=True,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=test_data,\n",
    "        x_col='file_path',\n",
    "        y_col='label',\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Paths\n",
    "    saved_model_path = f'./models/best_small_model_fold_{fold_idx + 1}.h5'\n",
    "    qat_model_path = f'./quantized_models/qat_small_model_fold_{fold_idx + 1}.h5'\n",
    "\n",
    "    # Check if the saved model exists\n",
    "    if not os.path.exists(saved_model_path):\n",
    "        print(f\"Saved model for fold {fold_idx + 1} not found at {saved_model_path}. Skipping QAT for this fold.\")\n",
    "        continue\n",
    "\n",
    "    # 5. Load the Pretrained Model\n",
    "    pretrained_model = load_model(saved_model_path)\n",
    "    print(f\"Loaded pretrained model for fold {fold_idx + 1}.\")\n",
    "\n",
    "    # 6. Apply Quantization-Aware Training (QAT)\n",
    "    qat_model = tfmot.quantization.keras.quantize_model(pretrained_model)\n",
    "    print(f\"Applied QAT to the model for fold {fold_idx + 1}.\")\n",
    "\n",
    "    # 7. Compile the QAT Model\n",
    "    qat_model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=SGD(learning_rate=0.01),  # Lower learning rate for fine-tuning\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # 8. Define QAT Callbacks\n",
    "    qat_callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2),\n",
    "        ModelCheckpoint(\n",
    "            filepath=qat_model_path,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        TensorBoard(log_dir=f'./logs/qat_fold_{fold_idx + 1}')\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    # 10. Fine-Tune the QAT Model\n",
    "    history_qat = qat_model.fit(\n",
    "        train_generator,  # Reuse the train_generator from the primary loop\n",
    "        steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "        epochs=QAT_EPOCHS,\n",
    "        validation_data=test_generator,\n",
    "        callbacks=qat_callbacks,\n",
    "        # class_weight=class_weight_dict  # Reuse class weights\n",
    "    )\n",
    "\n",
    "    # 11. Evaluation\n",
    "    y_true_qat = test_generator.classes\n",
    "    y_pred_probs_qat = qat_model.predict(test_generator, verbose=1)\n",
    "    y_pred_qat = np.argmax(y_pred_probs_qat, axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    qat_accuracy = history_qat.history['val_accuracy'][-1]\n",
    "    qat_fold_accuracies.append(qat_accuracy)\n",
    "    print(f\"Fold {fold_idx + 1} QAT Validation Accuracy: {qat_accuracy:.4f}\")\n",
    "\n",
    "    qat_f1 = f1_score(y_true_qat, y_pred_qat, average='weighted')\n",
    "    qat_fold_f1_scores.append(qat_f1)\n",
    "    print(f\"Fold {fold_idx + 1} QAT Validation F1 Score: {qat_f1:.4f}\")\n",
    "\n",
    "    # 12. Plot and Save QAT Loss Curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history_qat.history['loss'], label='QAT Training Loss', color='blue')\n",
    "    plt.plot(history_qat.history['val_loss'], label='QAT Validation Loss', color='orange')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'QAT Loss Curve - Fold {fold_idx + 1}')\n",
    "    plt.legend()\n",
    "    qat_loss_curve_path = f'./plots/loss/QAT_MNV2_loss_curve_fold_{fold_idx + 1}.png'\n",
    "    plt.savefig(qat_loss_curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 13. Plot and Save QAT Accuracy Curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history_qat.history['accuracy'], label='QAT Training Accuracy', color='green')\n",
    "    plt.plot(history_qat.history['val_accuracy'], label='QAT Validation Accuracy', color='red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'QAT Accuracy Curve - Fold {fold_idx + 1}')\n",
    "    plt.legend()\n",
    "    qat_accuracy_curve_path = f'./plots/accuracy/QAT_MNV2_accuracy_curve_fold_{fold_idx + 1}.png'\n",
    "    plt.savefig(qat_accuracy_curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 14. Track the Best QAT Fold\n",
    "    if qat_accuracy > best_qat_accuracy:\n",
    "        best_qat_accuracy = qat_accuracy\n",
    "        best_qat_fold_idx = fold_idx\n",
    "        print(f\"New best QAT model saved for fold {fold_idx + 1} with accuracy {qat_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 QAT Accuracy: 0.9190\n",
      "Fold 1 QAT F1 Score: 0.9107\n",
      "Fold 2 QAT Accuracy: 0.9519\n",
      "Fold 2 QAT F1 Score: 0.9669\n",
      "Fold 3 QAT Accuracy: 0.9519\n",
      "Fold 3 QAT F1 Score: 0.9518\n",
      "Fold 4 QAT Accuracy: 0.9137\n",
      "Fold 4 QAT F1 Score: 0.9136\n",
      "Fold 5 QAT Accuracy: 0.9543\n",
      "Fold 5 QAT F1 Score: 0.9720\n",
      "Average QAT Validation Accuracy: 0.938161027431488\n",
      "Standard Deviation of QAT Validation Accuracy: 0.017911397142492024\n",
      "Average QAT F1 Score: 0.9430083122680408\n",
      "Standard Deviation of QAT F1 Score: 0.02606519785460401\n",
      "\n",
      "Best QAT Fold: 5 with Validation Accuracy: 0.9543\n"
     ]
    }
   ],
   "source": [
    "# 5. Final Evaluation Across QAT Folds\n",
    "avg_qat_accuracy = np.mean(qat_fold_accuracies)\n",
    "std_qat_accuracy = np.std(qat_fold_accuracies)\n",
    "avg_qat_f1 = np.mean(qat_fold_f1_scores)\n",
    "std_qat_f1 = np.std(qat_fold_f1_scores)  # Added standard deviation for F1 scores\n",
    "\n",
    "for fold_idx in range(num_folds):\n",
    "    print(f\"Fold {fold_idx + 1} QAT Accuracy: {qat_fold_accuracies[fold_idx]:.4f}\")\n",
    "    print(f\"Fold {fold_idx + 1} QAT F1 Score: {qat_fold_f1_scores[fold_idx]:.4f}\")\n",
    "\n",
    "print(\"Average QAT Validation Accuracy:\", avg_qat_accuracy)\n",
    "print(\"Standard Deviation of QAT Validation Accuracy:\", std_qat_accuracy)\n",
    "print(\"Average QAT F1 Score:\", avg_qat_f1)\n",
    "print(\"Standard Deviation of QAT F1 Score:\", std_qat_f1)  # Printed std for F1 scores\n",
    "\n",
    "print(f\"\\nBest QAT Fold: {best_qat_fold_idx + 1} with Validation Accuracy: {best_qat_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded validation set from: best_fold_4_validation_data.csv\n",
      "Validation samples for QAT: 394\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the saved validation set\n",
    "val_set_path = f'best_fold_{best_fold_idx + 1}_validation_data.csv'  # Adjust fold index if needed\n",
    "val_data_qat = pd.read_csv(val_set_path)\n",
    "print(f\"Loaded validation set from: {val_set_path}\")\n",
    "print(f\"Validation samples for QAT: {len(val_data_qat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples for QAT: 1579\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create training set by excluding validation samples\n",
    "train_data_qat = image_dataframe[~image_dataframe['file_path'].isin(val_data_qat['file_path'])]\n",
    "print(f\"Training samples for QAT: {len(train_data_qat)}\")\n",
    "\n",
    "# Step 3: Define image dimensions and batch size\n",
    "img_width, img_height = 128, 128\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize data generators (reuse original augmentations)\n",
    "train_datagen_qat = CustomImageDataGenerator(  # Ensure all augmentations are identical\n",
    "    channel_shift_range=0.1,\n",
    "    # Include all original augmentation parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen_qat = CustomImageDataGenerator()  # No augmentations for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1579 validated image filenames belonging to 3 classes.\n",
      "Found 394 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Create training generator (with augmentations)\n",
    "train_generator_qat = train_datagen_qat.flow_from_dataframe(\n",
    "    dataframe=train_data_qat,\n",
    "    x_col='file_path',\n",
    "    y_col='label',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Step 6: Create validation generator (without augmentations)\n",
    "val_generator_qat = val_datagen_qat.flow_from_dataframe(\n",
    "    dataframe=val_data_qat,\n",
    "    x_col='file_path',\n",
    "    y_col='label',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True  # No shuffle for validation set\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model for QAT fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Load the pretrained model\n",
    "pretrained_model = tf.keras.models.load_model('best_model_fold_4.keras')\n",
    "print(\"Loaded pretrained model for QAT fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 64, 64, 8)         216       \n",
      "                                                                 \n",
      " conv1_bn (BatchNormalizati  (None, 64, 64, 8)         32        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " conv1_relu (ReLU)           (None, 64, 64, 8)         0         \n",
      "                                                                 \n",
      " conv_dw_1 (DepthwiseConv2D  (None, 64, 64, 8)         72        \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_1_bn (BatchNormali  (None, 64, 64, 8)         32        \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_1_relu (ReLU)       (None, 64, 64, 8)         0         \n",
      "                                                                 \n",
      " conv_pw_1 (Conv2D)          (None, 64, 64, 16)        128       \n",
      "                                                                 \n",
      " conv_pw_1_bn (BatchNormali  (None, 64, 64, 16)        64        \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_1_relu (ReLU)       (None, 64, 64, 16)        0         \n",
      "                                                                 \n",
      " conv_pad_2 (ZeroPadding2D)  (None, 65, 65, 16)        0         \n",
      "                                                                 \n",
      " conv_dw_2 (DepthwiseConv2D  (None, 32, 32, 16)        144       \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_2_bn (BatchNormali  (None, 32, 32, 16)        64        \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_2_relu (ReLU)       (None, 32, 32, 16)        0         \n",
      "                                                                 \n",
      " conv_pw_2 (Conv2D)          (None, 32, 32, 32)        512       \n",
      "                                                                 \n",
      " conv_pw_2_bn (BatchNormali  (None, 32, 32, 32)        128       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_2_relu (ReLU)       (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv_dw_3 (DepthwiseConv2D  (None, 32, 32, 32)        288       \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_3_bn (BatchNormali  (None, 32, 32, 32)        128       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_3_relu (ReLU)       (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv_pw_3 (Conv2D)          (None, 32, 32, 32)        1024      \n",
      "                                                                 \n",
      " conv_pw_3_bn (BatchNormali  (None, 32, 32, 32)        128       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_3_relu (ReLU)       (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv_pad_4 (ZeroPadding2D)  (None, 33, 33, 32)        0         \n",
      "                                                                 \n",
      " conv_dw_4 (DepthwiseConv2D  (None, 16, 16, 32)        288       \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_4_bn (BatchNormali  (None, 16, 16, 32)        128       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_4_relu (ReLU)       (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv_pw_4 (Conv2D)          (None, 16, 16, 64)        2048      \n",
      "                                                                 \n",
      " conv_pw_4_bn (BatchNormali  (None, 16, 16, 64)        256       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_4_relu (ReLU)       (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv_dw_5 (DepthwiseConv2D  (None, 16, 16, 64)        576       \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_5_bn (BatchNormali  (None, 16, 16, 64)        256       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_5_relu (ReLU)       (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv_pw_5 (Conv2D)          (None, 16, 16, 64)        4096      \n",
      "                                                                 \n",
      " conv_pw_5_bn (BatchNormali  (None, 16, 16, 64)        256       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_5_relu (ReLU)       (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv_pad_6 (ZeroPadding2D)  (None, 17, 17, 64)        0         \n",
      "                                                                 \n",
      " conv_dw_6 (DepthwiseConv2D  (None, 8, 8, 64)          576       \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_6_bn (BatchNormali  (None, 8, 8, 64)          256       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_6_relu (ReLU)       (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv_pw_6 (Conv2D)          (None, 8, 8, 128)         8192      \n",
      "                                                                 \n",
      " conv_pw_6_bn (BatchNormali  (None, 8, 8, 128)         512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_6_relu (ReLU)       (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_dw_7 (DepthwiseConv2D  (None, 8, 8, 128)         1152      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_7_bn (BatchNormali  (None, 8, 8, 128)         512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_7_relu (ReLU)       (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_pw_7 (Conv2D)          (None, 8, 8, 128)         16384     \n",
      "                                                                 \n",
      " conv_pw_7_bn (BatchNormali  (None, 8, 8, 128)         512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_7_relu (ReLU)       (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_dw_8 (DepthwiseConv2D  (None, 8, 8, 128)         1152      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_8_bn (BatchNormali  (None, 8, 8, 128)         512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_8_relu (ReLU)       (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_pw_8 (Conv2D)          (None, 8, 8, 128)         16384     \n",
      "                                                                 \n",
      " conv_pw_8_bn (BatchNormali  (None, 8, 8, 128)         512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_8_relu (ReLU)       (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_dw_9 (DepthwiseConv2D  (None, 8, 8, 128)         1152      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_9_bn (BatchNormali  (None, 8, 8, 128)         512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_9_relu (ReLU)       (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_pw_9 (Conv2D)          (None, 8, 8, 128)         16384     \n",
      "                                                                 \n",
      " conv_pw_9_bn (BatchNormali  (None, 8, 8, 128)         512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_9_relu (ReLU)       (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_dw_10 (DepthwiseConv2  (None, 8, 8, 128)         1152      \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv_dw_10_bn (BatchNormal  (None, 8, 8, 128)         512       \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_dw_10_relu (ReLU)      (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_pw_10 (Conv2D)         (None, 8, 8, 128)         16384     \n",
      "                                                                 \n",
      " conv_pw_10_bn (BatchNormal  (None, 8, 8, 128)         512       \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_pw_10_relu (ReLU)      (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_dw_11 (DepthwiseConv2  (None, 8, 8, 128)         1152      \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv_dw_11_bn (BatchNormal  (None, 8, 8, 128)         512       \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_dw_11_relu (ReLU)      (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_pw_11 (Conv2D)         (None, 8, 8, 128)         16384     \n",
      "                                                                 \n",
      " conv_pw_11_bn (BatchNormal  (None, 8, 8, 128)         512       \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_pw_11_relu (ReLU)      (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv_pad_12 (ZeroPadding2D  (None, 9, 9, 128)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_12 (DepthwiseConv2  (None, 4, 4, 128)         1152      \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv_dw_12_bn (BatchNormal  (None, 4, 4, 128)         512       \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_dw_12_relu (ReLU)      (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv_pw_12 (Conv2D)         (None, 4, 4, 256)         32768     \n",
      "                                                                 \n",
      " conv_pw_12_bn (BatchNormal  (None, 4, 4, 256)         1024      \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_pw_12_relu (ReLU)      (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv_dw_13 (DepthwiseConv2  (None, 4, 4, 256)         2304      \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv_dw_13_bn (BatchNormal  (None, 4, 4, 256)         1024      \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_dw_13_relu (ReLU)      (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv_pw_13 (Conv2D)         (None, 4, 4, 256)         65536     \n",
      "                                                                 \n",
      " conv_pw_13_bn (BatchNormal  (None, 4, 4, 256)         1024      \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_pw_13_relu (ReLU)      (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_6  (None, 256)               0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252339 (985.70 KB)\n",
      "Trainable params: 246611 (963.32 KB)\n",
      "Non-trainable params: 5728 (22.38 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted model to QAT.\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Apply Quantization-Aware Training (QAT)\n",
    "qat_model = tfmot.quantization.keras.quantize_model(pretrained_model)\n",
    "print(\"Converted model to QAT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Compile the QAT model\n",
    "opt = SGD(learning_rate=0.01)  # Define optimizer\n",
    "qat_model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "98/98 [==============================] - 9s 97ms/step - loss: 0.3004 - accuracy: 0.9738 - val_loss: 0.4328 - val_accuracy: 0.9416 - lr: 1.0000e-03\n",
      "Epoch 2/15\n",
      "98/98 [==============================] - 9s 97ms/step - loss: 0.3065 - accuracy: 0.9699 - val_loss: 0.4851 - val_accuracy: 0.9391 - lr: 1.0000e-03\n",
      "Epoch 3/15\n",
      "98/98 [==============================] - 9s 95ms/step - loss: 0.3137 - accuracy: 0.9712 - val_loss: 0.4491 - val_accuracy: 0.9467 - lr: 1.0000e-03\n",
      "Epoch 4/15\n",
      "98/98 [==============================] - 9s 97ms/step - loss: 0.2963 - accuracy: 0.9782 - val_loss: 0.4831 - val_accuracy: 0.9391 - lr: 1.0000e-04\n",
      "Epoch 5/15\n",
      "98/98 [==============================] - 10s 98ms/step - loss: 0.2984 - accuracy: 0.9770 - val_loss: 0.4428 - val_accuracy: 0.9416 - lr: 1.0000e-04\n",
      "Epoch 6/15\n",
      "98/98 [==============================] - 10s 98ms/step - loss: 0.2814 - accuracy: 0.9859 - val_loss: 0.4764 - val_accuracy: 0.9315 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Define callbacks\n",
    "lr_scheduler_qat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
    "early_stopping_qat = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Step 11: Fine-tune the QAT model\n",
    "qat_history = qat_model.fit(\n",
    "    train_generator_qat,\n",
    "    steps_per_epoch=train_generator_qat.samples // train_generator_qat.batch_size,\n",
    "    epochs=15,  # Adjust fine-tuning epochs as needed\n",
    "    validation_data=val_generator_qat,\n",
    "    callbacks=[lr_scheduler_qat, early_stopping_qat]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAIjCAYAAADvBuGTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACPcklEQVR4nOzdd3zT1f7H8Ve69wDKaCmjBdlQtqBsEK1yAZXlYIjc68BxkasgiIADJxdFr3gREEEUVODqDwQZgizZIIogm1L26F5pkt8faUNDCzSQko738/HIQ/LN9/vN+abHNp+cc94xWCwWCyIiIiIiInJT3FzdABERERERkdJAxZWIiIiIiIgTqLgSERERERFxAhVXIiIiIiIiTqDiSkRERERExAlUXImIiIiIiDiBiisREREREREnUHElIiIiIiLiBCquREREREREnEDFlYjILTR48GBq1KhxQ8eOHz8eg8Hg3AYVM0ePHsVgMPD555/f8uc2GAyMHz/edv/zzz/HYDBw9OjR6x5bo0YNBg8e7NT23ExfERER11BxJSKC9Y11YW5r1qxxdVPLvGeffRaDwcDBgwevus+YMWMwGAz89ttvt7Bljjt58iTjx49n165drm5Kgf78808MBgM+Pj4kJCS4ujkiIsWeiisREWDOnDl2t27duhW4vV69ejf1PNOnT2f//v03dOzYsWNJT0+/qecvDR5++GEA5s2bd9V9vvrqKxo1akTjxo1v+HkeffRR0tPTqV69+g2f43pOnjzJhAkTCiyubqavOMvcuXOpXLkyAN9++61L2yIiUhJ4uLoBIiLFwSOPPGJ3/9dff2XFihX5tl8pLS0NPz+/Qj+Pp6fnDbUPwMPDAw8P/dpu3bo1tWrV4quvvmLcuHH5Ht+0aRNHjhzhrbfeuqnncXd3x93d/abOcTNupq84g8ViYd68eTz00EMcOXKEL7/8kscff9ylbbqa1NRU/P39Xd0MERGNXImIFFbHjh1p2LAh27dvp3379vj5+fHyyy8D8L///Y97772X8PBwvL29iY6O5rXXXsNkMtmd48p1NLlrjN577z3++9//Eh0djbe3Ny1btmTr1q12xxa05spgMDB8+HAWL15Mw4YN8fb2pkGDBixbtixf+9esWUOLFi3w8fEhOjqaTz/9tNDruNatW0efPn2oVq0a3t7eREZG8s9//jPfSNrgwYMJCAggPj6eXr16ERAQQFhYGCNHjsz3WiQkJDB48GCCg4MJCQlh0KBBhZ569vDDD7Nv3z527NiR77F58+ZhMBgYMGAAWVlZjBs3jubNmxMcHIy/vz/t2rXj559/vu5zFLTmymKx8Prrr1O1alX8/Pzo1KkTf/zxR75jL168yMiRI2nUqBEBAQEEBQVxzz33sHv3bts+a9asoWXLlgAMGTLENvU0d71ZQWuuUlNTeeGFF4iMjMTb25s6derw3nvvYbFY7PZzpF9czYYNGzh69Cj9+/enf//+/PLLL5w4cSLffmazmQ8++IBGjRrh4+NDWFgYd999N9u2bbPbb+7cubRq1Qo/Pz9CQ0Np3749P/30k12b8655y3Xlerbcn8vatWt56qmnqFixIlWrVgXg2LFjPPXUU9SpUwdfX1/Kly9Pnz59Clw3l5CQwD//+U9q1KiBt7c3VatWZeDAgZw/f56UlBT8/f157rnn8h134sQJ3N3dmTRpUiFfSREpS/QRqIiIAy5cuMA999xD//79eeSRR6hUqRJgfcMXEBDAiBEjCAgIYPXq1YwbN46kpCTefffd65533rx5JCcn849//AODwcA777zD/fffz+HDh687grF+/XoWLlzIU089RWBgIB9++CEPPPAAx48fp3z58gDs3LmTu+++mypVqjBhwgRMJhMTJ04kLCysUNf9zTffkJaWxpNPPkn58uXZsmULU6dO5cSJE3zzzTd2+5pMJrp3707r1q157733WLlyJe+//z7R0dE8+eSTgLVI6dmzJ+vXr+eJJ56gXr16LFq0iEGDBhWqPQ8//DATJkxg3rx5NGvWzO65FyxYQLt27ahWrRrnz5/ns88+Y8CAAQwbNozk5GRmzJhB9+7d2bJlCzExMYV6vlzjxo3j9ddfJzY2ltjYWHbs2MFdd91FVlaW3X6HDx9m8eLF9OnTh5o1a3LmzBk+/fRTOnTowN69ewkPD6devXpMnDiRcePG8fe//5127doB0LZt2wKf22Kx8Le//Y2ff/6ZoUOHEhMTw/Lly/nXv/5FfHw8//73v+32L0y/uJYvv/yS6OhoWrZsScOGDfHz8+Orr77iX//6l91+Q4cO5fPPP+eee+7h8ccfJzs7m3Xr1vHrr7/SokULACZMmMD48eNp27YtEydOxMvLi82bN7N69WruuuuuQr/+eT311FOEhYUxbtw4UlNTAdi6dSsbN26kf//+VK1alaNHj/LJJ5/QsWNH9u7daxtlTklJoV27dvz555889thjNGvWjPPnz/P9999z4sQJYmJi6N27N/Pnz2fy5Ml2I5hfffUVFovFNj1VRMSORURE8nn66actV/6K7NChgwWwTJs2Ld/+aWlp+bb94x//sPj5+VkyMjJs2wYNGmSpXr267f6RI0csgKV8+fKWixcv2rb/73//swCWH374wbbt1VdfzdcmwOLl5WU5ePCgbdvu3bstgGXq1Km2bT169LD4+flZ4uPjbdsOHDhg8fDwyHfOghR0fZMmTbIYDAbLsWPH7K4PsEycONFu36ZNm1qaN29uu7948WILYHnnnXds27Kzsy3t2rWzAJZZs2Zdt00tW7a0VK1a1WIymWzbli1bZgEsn376qe2cmZmZdsddunTJUqlSJctjjz1mtx2wvPrqq7b7s2bNsgCWI0eOWCwWi+Xs2bMWLy8vy7333msxm822/V5++WULYBk0aJBtW0ZGhl27LBbrz9rb29vutdm6detVr/fKvpL7mr3++ut2+z344IMWg8Fg1wcK2y+uJisry1K+fHnLmDFjbNseeughS5MmTez2W716tQWwPPvss/nOkfsaHThwwOLm5mbp3bt3vtck7+t45eufq3r16navbe7P5c4777RkZ2fb7VtQP920aZMFsHzxxRe2bePGjbMAloULF1613cuXL7cAlh9//NHu8caNG1s6dOiQ7zgREYvFYtG0QBERB3h7ezNkyJB82319fW3/Tk5O5vz587Rr1460tDT27dt33fP269eP0NBQ2/3cUYzDhw9f99iuXbsSHR1tu9+4cWOCgoJsx5pMJlauXEmvXr0IDw+37VerVi3uueee654f7K8vNTWV8+fP07ZtWywWCzt37sy3/xNPPGF3v127dnbXsnTpUjw8PGwjWWBd4/TMM88Uqj1gXSd34sQJfvnlF9u2efPm4eXlRZ8+fWzn9PLyAqzT1y5evEh2djYtWrQocErhtaxcuZKsrCyeeeYZu6mUzz//fL59vb29cXOz/ok1mUxcuHCBgIAA6tSp4/Dz5lq6dCnu7u48++yzdttfeOEFLBYLP/74o9326/WLa/nxxx+5cOECAwYMsG0bMGAAu3fvtpsG+d1332EwGHj11VfznSP3NVq8eDFms5lx48bZXpMr97kRw4YNy7cmLm8/NRqNXLhwgVq1ahESEmL3un/33Xc0adKE3r17X7XdXbt2JTw8nC+//NL22O+//85vv/123bWYIlJ2qbgSEXFARESE7c16Xn/88Qe9e/cmODiYoKAgwsLCbG/AEhMTr3veatWq2d3PLbQuXbrk8LG5x+cee/bsWdLT06lVq1a+/QraVpDjx48zePBgypUrZ1tH1aFDByD/9eWuu7lae8C6NqZKlSoEBATY7VenTp1CtQegf//+uLu721IDMzIyWLRoEffcc49doTp79mwaN26Mj48P5cuXJywsjCVLlhTq55LXsWPHAKhdu7bd9rCwMLvnA2sh9+9//5vatWvj7e1NhQoVCAsL47fffnP4efM+f3h4OIGBgXbbcxMsc9uX63r94lrmzp1LzZo18fb25uDBgxw8eJDo6Gj8/Pzsio1Dhw4RHh5OuXLlrnquQ4cO4ebmRv369a/7vI6oWbNmvm3p6emMGzfOtiYt93VPSEiwe90PHTpEw4YNr3l+Nzc3Hn74YRYvXkxaWhpgnSrp4+NjK95FRK6k4kpExAF5PxnPlZCQQIcOHdi9ezcTJ07khx9+YMWKFbz99tuA9Y329Vwtlc5yRVCBs48tDJPJRLdu3ViyZAkvvfQSixcvZsWKFbbghSuv71Yl7FWsWJFu3brx3XffYTQa+eGHH0hOTrZbCzN37lwGDx5MdHQ0M2bMYNmyZaxYsYLOnTsX6udyo958801GjBhB+/btmTt3LsuXL2fFihU0aNCgSJ83rxvtF0lJSfzwww8cOXKE2rVr227169cnLS2NefPmOa1vFcaVQSi5Cvp/8ZlnnuGNN96gb9++LFiwgJ9++okVK1ZQvnz5G3rdBw4cSEpKCosXL7alJ953330EBwc7fC4RKRsUaCEicpPWrFnDhQsXWLhwIe3bt7dtP3LkiAtbdVnFihXx8fEp8Et3r/VFvLn27NnDX3/9xezZsxk4cKBt+4oVK264TdWrV2fVqlWkpKTYjV45+r1ODz/8MMuWLePHH39k3rx5BAUF0aNHD9vj3377LVFRUSxcuNBuClpB09gK02aAAwcOEBUVZdt+7ty5fKNB3377LZ06dWLGjBl22xMSEqhQoYLtviPT4qpXr87KlStJTk62G73KnXbqrO/jWrhwIRkZGXzyySd2bQXrz2fs2LFs2LCBO++8k+joaJYvX87FixevOnoVHR2N2Wxm79691wwQCQ0NzZcWmZWVxalTpwrd9m+//ZZBgwbx/vvv27ZlZGTkO290dDS///77dc/XsGFDmjZtypdffknVqlU5fvw4U6dOLXR7RKTs0ciViMhNyh0hyPtpflZWFv/5z39c1SQ77u7udO3alcWLF3Py5Enb9oMHD+Zbp3O148H++iwWCx988MENtyk2Npbs7Gw++eQT2zaTyeTwG9devXrh5+fHf/7zH3788Ufuv/9+fHx8rtn2zZs3s2nTJofb3LVrVzw9PZk6dard+aZMmZJvX3d393yjO9988w3x8fF223K/m6kwEfSxsbGYTCY++ugju+3//ve/MRgMhV4/dz1z584lKiqKJ554ggcffNDuNnLkSAICAmxTAx944AEsFgsTJkzId57c6+/Vqxdubm5MnDgx3+hR3tcoOjrabv0cwH//+9+rjlwVpKDXferUqfnO8cADD7B7924WLVp01XbnevTRR/npp5+YMmUK5cuXd9rrLCKlk0auRERuUtu2bQkNDWXQoEE8++yzGAwG5syZc0unTl3P+PHj+emnn7jjjjt48sknbW/SGzZsyK5du655bN26dYmOjmbkyJHEx8cTFBTEd999V6i1O1fTo0cP7rjjDkaNGsXRo0epX78+CxcudHg9UkBAAL169bKtu7oyHvu+++5j4cKF9O7dm3vvvZcjR44wbdo06tevT0pKikPPlft9XZMmTeK+++4jNjaWnTt38uOPP+Yb4bnvvvuYOHEiQ4YMoW3btuzZs4cvv/zSbsQLrAVFSEgI06ZNIzAwEH9/f1q3bl3geqIePXrQqVMnxowZw9GjR2nSpAk//fQT//vf/3j++eftwitu1MmTJ/n555/zhWbk8vb2pnv37nzzzTd8+OGHdOrUiUcffZQPP/yQAwcOcPfdd2M2m1m3bh2dOnVi+PDh1KpVizFjxvDaa6/Rrl077r//fry9vdm6dSvh4eG274t6/PHHeeKJJ3jggQfo1q0bu3fvZvny5fle22u57777mDNnDsHBwdSvX59NmzaxcuXKfNHz//rXv/j222/p06cPjz32GM2bN+fixYt8//33TJs2jSZNmtj2feihh3jxxRdZtGgRTz75pMu/3FlEijeNXImI3KTy5cvzf//3f1SpUoWxY8fy3nvv0a1bN9555x1XN82mefPm/Pjjj4SGhvLKK68wY8YMJk6cSJcuXexGegri6enJDz/8QExMDJMmTWLChAnUrl2bL7744obb4+bmxvfff8/DDz/M3LlzGTNmDBEREcyePdvhc+UWVFWqVKFz5852jw0ePJg333yT3bt38+yzz7J8+XLmzp1r+/4lR73++utMmDCBnTt38q9//YtDhw7x008/2Uagcr388su88MILLF++nOeee44dO3awZMkSIiMj7fbz9PRk9uzZuLu788QTTzBgwADWrl1b4HPnvmbPP/88//d//8fzzz/P3r17effdd5k8efINXc+Vvv76a8xms93Uyiv16NGDCxcu2EY9Z82axbvvvsuRI0f417/+xZtvvkl6errd93VNnDiRmTNnkp6ezpgxYxg3bhzHjh2jS5cutn2GDRvGSy+9xC+//MILL7zAkSNHWLFiRb7X9lo++OADBg4cyJdffskLL7zAqVOnWLlyZb7glICAANatW8eTTz7J0qVLefbZZ/nPf/5DnTp1bF9InKtSpUq27+J69NFHC90WESmbDJbi9NGqiIjcUr169eKPP/7gwIEDrm6KSLHVu3dv9uzZU6g1iiJStmnkSkSkjEhPT7e7f+DAAZYuXUrHjh1d0yCREuDUqVMsWbJEo1YiUigauRIRKSOqVKnC4MGDiYqK4tixY3zyySdkZmayc+fOfN/dJFLWHTlyhA0bNvDZZ5+xdetWDh06ROXKlV3dLBEp5hRoISJSRtx999189dVXnD59Gm9vb9q0acObb76pwkqkAGvXrmXIkCFUq1aN2bNnq7ASkULRyJWIiIiIiIgTaM2ViIiIiIiIE6i4EhERERERcQKtuSqA2Wzm5MmTBAYGYjAYXN0cERERERFxEYvFQnJyMuHh4bi5XXtsSsVVAU6ePJnvix5FRERERKTsiouLy/dF41dScVWAwMBAwPoCBgUFubQtRqORn376ibvuugtPT0+XtkVKBvUZcZT6jDhKfUYcpT4jjipOfSYpKYnIyEhbjXAtKq4KkDsVMCgoqFgUV35+fgQFBbm8Y0nJoD4jjlKfEUepz4ij1GfEUcWxzxRmuZACLURERERERJxAxZWIiIiIiIgTqLgSERERERFxAq25ukEWi4Xs7GxMJlORPo/RaMTDw4OMjIwify4pHZzRZ9zd3fHw8NBXEYiIiIg4QMXVDcjKyuLUqVOkpaUV+XNZLBYqV65MXFyc3uhKoTirz/j5+VGlShW8vLyc2DoRERGR0kvFlYPMZjNHjhzB3d2d8PBwvLy8irToMZvNpKSkEBAQcN0vLROBm+8zFouFrKwszp07x5EjR6hdu7b6noiIiEghqLhyUFZWFmazmcjISPz8/Ir8+cxmM1lZWfj4+OgNrhSKM/qMr68vnp6eHDt2zHYuEREREbk2vVu/QSp0pLRTHxcRERFxjN49iYiIiIiIOIGKKxERERERESdQcSU3pUaNGkyZMqXQ+69ZswaDwUBCQkKRtUlERERExBVUXJURBoPhmrfx48ff0Hm3bt3K3//+90Lv37ZtW06dOkVwcPANPd+NqFu3Lt7e3pw+ffqWPaeIiIiIlD0qrsqIU6dO2W5TpkwhKCjIbtvIkSNt++Z+QXJhhIWFOZSa6OXlReXKlW/Zd3atX7+e9PR0HnzwQWbPnn1LnvNajEajq5sgIiIiIkVExZUTWCwW0rKyi+yWnmUqcLvFYil0GytXrmy7BQcHYzAYbPf37dtHYGAgP/74I82bN8fb25v169dz6NAhevbsSaVKlQgICKBly5asXLnS7rxXTgs0GAx89tln9O7dGz8/P2rXrs33339ve/zKaYGff/45ISEhLF++nHr16hEQEMDdd9/NqVOnbMdkZ2fz7LPPEhISQvny5XnppZcYNGgQvXr1uu51z5gxg4ceeohHH32UmTNn5nv8xIkTDBgwgHLlyuHv70+LFi3YvHmz7fEffviBli1b4uPjQ4UKFejdu7fdtS5evNjufCEhIXz++ecAHD16FIPBwPz58+nQoQM+Pj58+eWXXLhwgQEDBhAREYGfnx+NGjXiq6++sjuP2WzmnXfeoVatWnh7e1OtWjXeeOMNADp37szw4cPt9j937hxeXl6sWrXquq+JiIiIiBQNfc+VE6QbTdQft/yWP+/eid3x83Lej3DUqFG89957REVFERoaSlxcHLGxsbzxxht4e3vzxRdf0KNHD/bv30+1atWuep4JEybwzjvv8O677zJ16lQefvhhjh07Rrly5QrcPy0tjffee485c+bg5ubGI488wsiRI/nyyy8BePvtt/nyyy+ZNWsW9erV44MPPmDx4sV06tTpmteTnJzMN998w+bNm6lbty6JiYmsW7eOdu3aAZCSkkKHDh2IiIjg+++/p3LlyuzYsQOz2QzAkiVL6N27N2PGjOGLL74gKyuLpUuX3tDr+v7779O0aVN8fHzIyMigefPmvPTSSwQFBbFkyRIeffRRoqOjadWqFQCjR49m+vTp/Pvf/+bOO+/k1KlT7Nu3D4DHH3+c4cOH8/777+Pt7Q3A3LlziYiIoHPnzg4V3SIiIiLiPCquxGbixIl069bNdr9cuXI0adLEdv+1115j0aJFfP/99/lGTvIaPHgwAwYMAODNN9/kww8/ZMuWLdx9990F7m80Gpk2bRrR0dEADB8+nIkTJ9oenzp1KqNHj7aNGn300UeFKnK+/vprateuTYMGDQDo378/M2bMsBVX8+bN49y5c2zdutVW+NWqVct2/BtvvEH//v2ZMGGCbVve16Ownn/+ee6//367bXmnYT7zzDMsX76cBQsW0KpVK5KTk/nggw/46KOPGDRoEADR0dHceeedANx///0MHz6c//3vf/Tt2xewjgAOHjwYg8Gg4kpERETERVRcOYGvpzt7J3YvknObzWaSk5IJDArM96Wuvp7uTn2uFi1a2N1PSUlh/PjxLFmyhFOnTpGdnU16ejrHjx+/5nkaN25s+7e/vz9BQUGcPXv2qvv7+fnZCiuAKlWq2PZPTEzkzJkzthEdAHd3d5o3b24bYbqamTNn8sgjj9juP/LII3To0IGpU6cSGBjIrl27aNq06VVH1Hbt2sWwYcOu+RyFceXrajKZePPNN1mwYAHx8fFkZWWRmZlpW7v2559/kpmZSZcuXQo8n4+Pj22aY9++fdmxYwe///673fRLERERkZJs27FL7DhvINbVDXGQiisnMBgMTp2el5fZbCbbyx0/L498xZWz+fv7290fOXIkK1as4L333qNWrVr4+vry4IMPkpWVdc3zeHp62t03GAzXLIQK2v9mR1/27t3Lr7/+ypYtW3jppZds200mE19//TXDhg3D19f3mue43uMFtbOgwIorX9d3332XDz74gClTptCoUSP8/f15/vnnba/r9Z4XrFMDY2JiOHHiBLNmzaJz585Ur179useJiIiIFFcWi4VfD1/kw1UH2HT4An4ebjyfmU3oFe8VizMFWshVbdiwgcGDB9O7d28aNWpE5cqVOXr06C1tQ3BwMJUqVWLr1q22bSaTiR07dlzzuBkzZtC+fXt2797Nrl27bLcRI0YwY8YMwDrCtmvXLi5evFjgORo3bnzNgIiwsDC74I0DBw6QlpZ23WvasGEDPXv25JFHHqFJkyZERUXx119/2R6vXbs2vr6+13zuRo0a0aJFC6ZPn868efN47LHHrvu8IiIiIsWRxWLhl7/O0ffTTQyY/iubDl/A091Ak3IWMrOvPVOpuNHIlVxV7dq1WbhwIT169MBgMPDKK69cdypeUXjmmWeYNGkStWrVom7dukydOpVLly5dNc7daDQyZ84cJk6cSMOGDe0ee/zxx5k8eTJ//PEHAwYM4M0336RXr15MmjSJKlWqsHPnTsLDw2nTpg2vvvoqXbp0ITo6mv79+5Odnc3SpUttI2GdO3fmo48+ok2bNphMJl566aV8o3AFqV27Nt9++y0bN24kNDSUyZMnc+bMGerXrw9Yp/299NJLvPjii3h5eXHHHXdw7tw5/vjjD4YOHWp3LcOHD8ff398uxVBERESkJLBYLPy8/ywfrjrIrrgEALzc3ejfKpKhbauxa+PPlPf3cm0jHaSRK7mqyZMnExoaStu2benRowfdu3enWbNmt7wdL730EgMGDGDgwIG0adOGgIAAunfvjo+PT4H7f//991y4cKHAgqNevXrUq1ePGTNm4OXlxU8//UTFihWJjY2lUaNGvPXWW7i7W9eydezYkW+++Ybvv/+emJgYOnfuzJYtW2znev/994mMjKRdu3Y89NBDjBw5slDf+TV27FiaNWtG9+7d6dixI5UrV84XK//KK6/wwgsvMG7cOOrVq0e/fv3yrVsbMGAAHh4eDBgw4KqvhYiIiEhxYzZbWP7HaXp8tJ7HPt/GrrgEvD3ceOyOmqx7qRMTezYkPOT6yySKI4NF0WL5JCUlERwcTGJiIkFBQXaPZWRkcOTIEWrWrHlL3tCazWaSkpIICgoq8jVXJYXZbKZevXr07duX1157zdXNcZmjR48SHR3N1q1b7YpeZ/WZW93XxXWMRiNLly4lNja2UKOvIuoz4ij1GQFrUfXj76eZuvoA+04nA9aAtkfbVOfxdjWpGHj5/UZx6jPXqg2upGmBUuwdO3aMn376iQ4dOpCZmclHH33EkSNHeOihh1zdNJcwGo1cuHCBsWPHcvvtt7tkNFFERESksExmC//320k+Wn2QA2dTAAjw9mBgm+oMvbMm5QO8XdxC51FxJcWem5sbn3/+OSNHjsRisdCwYUNWrlxJvXr1XN00l9iwYQOdOnXitttu49tvv3V1c0REREQKlG0y879dJ/n454McPp8KQKCPB0PuqMljd9QgxK9kracqDBVXUuxFRkayYcMGVzej2OjYsaO+KFhERESKraxsM4t2nuDjnw9x/KI1STnEz5Ohd9Rk0B01CPIpvVNDVVyJiIiIiMhNy8w2sWDbCaatOUR8QjoA5f29eLxdFI+2qU6Ad+kvPUr/FYqIiIiISJHJMJr4astxPl17mNNJGQCEBXrzj/ZRPNS6Gn5eZafkKDtXKiIiIiIiTpOWlc2Xvx7n018Ocz4lE4DKQT480SGK/q2q4ePp7uIW3noqrkREREREpNBSMrOZs+kY09cd5mJqFgARIb482TGaPi2q4u1R9oqqXCquRERERETkupIyjMzecJQZG46QkGYEoFo5P57uFE3vplXx8tB3sqq4EhERERGRq0pIy2LmhqPM2nCE5IxsAKIq+PN0p1r0jAnHw11FVS69EuKQjh078vzzz9vu16hRgylTplzzGIPBwOLFi2/6uZ11HhERERG5voupWbyzbB93vv0zH646QHJGNrUrBvBB/xhWjOjAA82rqrC6gkauyogePXpgNBpZtmxZvsfWrVtH+/bt2b17N40bN3bovFu3bsXf399ZzQRg/PjxLF68mF27dtltP3XqFKGhoU59rqtJT08nIiICNzc34uPj8fYuPd8cLiIiInItZ5Mz+GzdEeZsOka60QRA3cqBPNulNnc3qIybm8HFLSy+VFyVEUOHDuWBBx7gxIkTVK1a1e6xWbNm0aJFC4cLK4CwsDBnNfG6KleufMue67vvvqNBgwZYLBYWL15Mv379btlzX8lisWAymfDw0P+uIiIiUnROJ2bw6S+HmLf5OJnZZgAaRgTxbOfadK1XSUVVIWgczxksFshKLbqbMa3g7RZLoZt43333ERYWxueff263PSUlhW+++YahQ4dy4cIFBgwYQEREBH5+fjRq1Iivvvrqmue9clrggQMHaN++PT4+PtSvX58VK1bkO+all17itttuw8/Pj6ioKF555RWMRuuiyM8//5wJEyawe/duDAYDBoPB1uYrpwXu2bOHzp074+vrS/ny5fn73/9OSkqK7fHBgwfTq1cv3nvvPapUqUL58uV5+umnbc91LTNmzOCRRx7hkUceYcaMGfke/+OPP7jvvvsICgoiMDCQdu3acejQIdvjM2fOpEGDBnh7e1OlShWGDx8OwNGjRzEYDHajcgkJCRgMBtasWQPAmjVrMBgM/PjjjzRv3hxvb2/Wr1/PoUOH6NmzJ5UqVSIgIICWLVuycuVKu3ZlZmYyatQoGjRogK+vL7Vq1WLGjBlYLBZq1arFe++9Z7f/rl27MBgMHDx48LqviYiIiJRO8QnpvLL4d9q/8zOzNhwlM9tMTGQIswa35Ifhd3KXRqsKTR+FO4MxDd4ML5JTuwEhV3vw5ZPgVbgpeR4eHgwcOJDPP/+cMWPGYDBY/wf55ptvMJlMDBgwgJSUFJo3b85LL71EUFAQS5Ys4dFHHyU6OppWrVpd9znMZjP3338/lSpVYvPmzSQmJtqtz8oVGBjI559/Tnh4OHv27GHYsGEEBgby4osv0q9fP37//XeWLVtmKxyCg4PznSM1NZXu3bvTpk0btm7dytmzZ3n88ccZPny4XQH5888/U6VKFX7++WcOHjxIv379iImJYdiwYVe9jkOHDrFp0yYWLlyIxWLhn//8J8eOHaN69eoAxMfH0759ezp27Mjq1asJCgpiw4YNZGdbF3h+8sknjBgxgrfeeot77rmHxMRENmzYcN3X70qjRo3ivffeIyoqitDQUOLi4oiNjeWNN97A29ubL774gh49erB//36qVasGwMCBA9m0aRNvv/02t99+O8eOHeP8+fMYDAYee+wxZs2axciRI23PMWvWLNq3b0+tWrUcbp+IiIiUbHEX0/jPmoN8u/0ERpP1Q/uWNUJ5tktt7qxVwfZ+UQpPxVUZ8thjj/Huu++ydu1aOnbsCFjfXD/wwAMEBwcTHBxs98b7mWeeYfny5SxYsKBQxdXKlSvZt28fy5cvJzzcWmy++eab3HPPPXb7jR071vbvGjVqMHLkSL7++mtefPFFfH19CQgIwMPD45rTAOfNm0dGRgZffPGFbc3XRx99RI8ePXj77bepVKkSAKGhoXz00Ue4u7tTt25d7r33XlatWnXN4mrmzJncc889tvVd3bt3Z9asWYwfPx6Ajz/+mODgYL7++ms8PT0BuO2222zHv/7667zwwgs899xztm0tW7a87ut3pYkTJ9KtWzfb/XLlytGkSRPb/ddee41Fixbx/fffM3z4cP766y8WLFjA8uXLadWqFUFBQXZF0+DBgxk3bhxbtmyhVatWGI1G5s2bl280S0REREq3I+dT+fjngyzaGY/JbC2q2kSV59kutbk9qpyKqpug4soZPP2so0hFwGw2k5ScTFBgIG5uV8zi9PRz6Fx169albdu2zJw5k44dO3Lw4EHWrVvHxIkTATCZTLz55pssWLCA+Ph4srKyyMzMxM+vcM/z559/EhkZaSusANq0aZNvv/nz5/Phhx9y6NAhUlJSyM7OJigoyKFr+fPPP2nSpIldmMYdd9yB2Wxm//79tuKqQYMGuLtf/iK7KlWqsGfPnque12QyMXv2bD744APbtkceeYSRI0cybtw43Nzc2LVrF+3atbMVVnmdPXuWkydP0qVLF4eupyAtWrSwu5+SksL48eNZsmQJp06dIjs7m/T0dI4fPw5Yp/i5u7vToUMH0tPT850vPDyce++9l5kzZ9KqVSt++OEHMjMz6dOnz023VURERIq/g2eT+Wj1Qb7ffZKcmop2tSvwbJfatKxRzrWNKyVUXDmDwVDo6XkOM5vB02Q9/5XF1Q0YOnQozzzzDB9//DGzZs0iOjqaDh06APDuu+/ywQcfMGXKFBo1aoS/vz/PP/88WVlZN/28uTZt2sTDDz/MhAkT6N69u20E6P3333fac+R1ZQFkMBgwm81X3X/58uXEx8fnC7AwmUysWrWKbt264evre9Xjr/UYYCuQLXnWy11tDdiVKYwjR45kxYoVvPfee9SqVQtfX18efPBB28/nes8N8Pjjj/Poo4/y73//m1mzZtGvX79CF88iIiJSMu07ncTU1QdZuueUbcl+57oVeaZzLZpWuzVJzGWFAi3KmL59++Lm5sa8efP44osveOyxx2xDvxs2bKBnz5488sgjNGnShKioKP76669Cn7tevXrExcVx6tQp27Zff/3Vbp+NGzdSvXp1xowZQ4sWLahduzbHjh2z28fLywuTyXTd59q9ezepqam2bRs2bMDNzY06deoUus1XmjFjBv3792fXrl12t/79+9uCLRo3bsy6desKLIoCAwOpUaMGq1atKvD8uemKeV+jKyPnr2bDhg0MHjyY3r1706hRIypXrszRo0dtjzdq1Aiz2czatWuveo7Y2Fj8/f355JNPWLZsGY899lihnltERERKnt/jE/nHnG3cPWUdS36zFlZ31a/ED8PvZObgliqsioCKqzImICCAfv36MXr0aE6dOsXgwYNtj9WuXZsVK1awceNG/vzzT/7xj39w5syZQp+7a9eu3HbbbQwaNIjdu3ezbt06xowZY7dP7dq1OX78OF9//TWHDh3iww8/ZNGiRXb71KhRgyNHjrBr1y7Onz9PZmZmvud6+OGH8fHxYdCgQfz+++/8/PPPPPPMMzz66KO2KYGOOnfuHD/88AODBg2iYcOGdreBAweyePFiLl68yPDhw0lKSqJ///5s27aNAwcOMGfOHPbv3w9Yv6fr/fff58MPP+TAgQPs2LGDqVOnAtbRpdtvv5233nqLP//8k7Vr19qtQbuW2rVrs3DhQnbt2sXu3bt56KGH7EbhatSowaBBg3j88cdZsmQJR44cYc2aNSxYsMC2j7u7O4MHD2b06NHUrl27wGmbIiIiUrLtiktg6OdbuW/qepb/cQaDAe5tVIUfn2vHfwe2oFHV/GFh4hwqrsqgoUOHcunSJbp37263Pmrs2LE0a9aM7t2707FjRypXrkyvXr0KfV43NzcWLVpEeno6rVq14vHHH+eNN96w2+dvf/sb//znPxk+fDgxMTFs3LiRV155xW6fBx54gLvvvptOnToRFhZWYBy8n58fy5cv5+LFi7Rs2ZIHH3yQLl268NFHHzn2YuSRG45R0HqpLl264Ovry9y5cylfvjyrV68mJSWFDh060Lx5c6ZPn26bgjho0CCmTJnCf/7zHxo0aMB9993HgQMHbOeaOXMm2dnZNG/enOeff57XX3+9UO2bPHkyoaGhtG3blh49etC9e3eaNWtmt88nn3zCAw88wMiRI6lfvz7Dhg2zG90D688/KyuLIUOGOPoSiYiISDG27ehFBs7cQq+PN7Bq31ncDNAzJpyfnm/Pxw83o14Vx9a4i+MMFosDX5ZURiQlJREcHExiYmK+oIWMjAyOHDlCzZo18fHxKfK2mM1mkpKSCAoKyh9oIVKA6/WZdevW0aVLF+Li4q45yner+7q4jtFoZOnSpcTGxhYY1CJyJfUZcZT6TNH69fAFPlx1gI2HLgDg7magV0wET3eKJioswMWtuzHFqc9cqza4kgItRMqIzMxMzp07x/jx4+nTp88NT58UERER17NYLGw4aC2qthy9CICHm4EHm1flqY61qFZegVWuoOJKpIz46quvGDp0KDExMXzxxReubo6IiIjcAIvFwpq/zvHhqgPsPJ4AgJe7G31bVuWJDtFUDVVR5UoqrkTKiMGDB9sFmIiIiEjJYbFYWPnnWaauPsBvJxIB8PZwY0CrajzRIZrKwZrCXxyouBIRERERKabMZgvL/jjN1NUH+fNUEgC+nu48cns1hrWPomKgiqriRMXVDVIOiJR26uMiIiKuYzJb+L/fTvLxzwf560wKAP5e7gxsW4PH76xJ+QBvF7dQCqLiykG5aSVpaWn4+vq6uDUiRSctLQ3A5Qk9IiIiZUm2ycz3u0/y0c8HOXzO+nUqgd4eDLmjBkPuqEmov5eLWyjXouLKQe7u7oSEhHD27FnA+n1LBoOhyJ7PbDaTlZVFRkaGotilUG62z1gsFtLS0jh79iwhISG4u7sXQStFREQkL6PJzKId8Xy85iDHLlg/4Az29WTonTUZ1LYGwb76sLMkUHF1AypXrgxgK7CKksViIT09HV9f3yIt4qT0cFafCQkJsfV1ERERKRqZ2Sa+3X6C//x8iPiEdADK+XvxeLuaPHp7dQJ9VFSVJCquboDBYKBKlSpUrFgRo9FYpM9lNBr55ZdfaN++vaZnSaE4o894enpqxEpERKQIZRhNzN8ax7S1hziVmAFAhQBv/tE+iodvr4afl96ml0T6qd0Ed3f3In8D6u7uTnZ2Nj4+PiqupFDUZ0RERIqv9CwTX24+xqe/HOZcciYAlYK8eaJDNANaVcPHUx9ulmQqrkREREREilhKZjZzfz3G9F8OcyE1C4DwYB+e7FSLPs2rqqgqJVRciYiIiIgUkaQMI19sPMpn64+QkGZdThJZzpenO9bi/mZV8fJQYFlpouJKRERERMTJEtOMzNxwhFkbjpCUkQ1AzQr+PN2pFj1jwvF0V1FVGqm4EhERERFxkoupWcxYf5jZG4+RkmktqmpVDOCZzrW4t1EVPFRUlWoqrkREREREbtK55Ew+W3eYOb8eIy3LBEDdyoEM71yLexpWwd1NX6lTFqi4EhERERG5QWeSMvh07WHmbTlGhtEMQIPwIJ7pXJu76lfCTUVVmaLiSkRERETEQScT0pm29hBfb40jK9taVDWJDOG5LrXoVKciBoOKqrLI5ZM+P/74Y2rUqIGPjw+tW7dmy5YtV93XaDQyceJEoqOj8fHxoUmTJixbtsxuH5PJxCuvvELNmjXx9fUlOjqa1157DYvFUtSXIiIiIiKlXNzFNEYv3EOHd3/mi03HyMo206J6KF881orFT7Wlc91KKqzKMJeOXM2fP58RI0Ywbdo0WrduzZQpU+jevTv79++nYsWK+fYfO3Ysc+fOZfr06dStW5fly5fTu3dvNm7cSNOmTQF4++23+eSTT5g9ezYNGjRg27ZtDBkyhODgYJ599tlbfYkiIiIiUgocPZ/Kxz8fZOHOeExm64f2t0eV49kutWkTVV4FlQAuLq4mT57MsGHDGDJkCADTpk1jyZIlzJw5k1GjRuXbf86cOYwZM4bY2FgAnnzySVauXMn777/P3LlzAdi4cSM9e/bk3nvvBaBGjRp89dVX1xwRy8zMJDMz03Y/KSkJsI6UGY1G51zsDcp9fle3Q0oO9RlxlPqMOEp9RhxVkvvMoXOpfLL2MD/8doqcmoo7osvzdMcoWtYIBSA7O9uFLSydilOfcaQNLiuusrKy2L59O6NHj7Ztc3Nzo2vXrmzatKnAYzIzM/Hx8bHb5uvry/r1623327Zty3//+1/++usvbrvtNnbv3s369euZPHnyVdsyadIkJkyYkG/7Tz/9hJ+fn6OXViRWrFjh6iZICaM+I45SnxFHqc+Io0pSnzmZBj+dcGPXBQMWrKNS9UPMdK9qpkbgGc7tPcPSvS5uZBlQHPpMWlpaofd1WXF1/vx5TCYTlSpVstteqVIl9u3bV+Ax3bt3Z/LkybRv357o6GhWrVrFwoULMZlMtn1GjRpFUlISdevWxd3dHZPJxBtvvMHDDz981baMHj2aESNG2O4nJSURGRnJXXfdRVBQ0E1e6c0xGo2sWLGCbt264enp6dK2SMmgPiOOUp8RR6nPiKNKUp/ZeyqJj9cc5qe9Z23butYN46mOUTSKCHZhy8qW4tRncme1FUaJSgv84IMPGDZsGHXr1sVgMBAdHc2QIUOYOXOmbZ8FCxbw5ZdfMm/ePBo0aMCuXbt4/vnnCQ8PZ9CgQQWe19vbG29v73zbPT09Xf7DzFWc2iIlg/qMOEp9RhylPiOOKs59ZndcAlNXH2Dln5eLqthGlRneqTb1w137YXtZVhz6jCPP77LiqkKFCri7u3PmzBm77WfOnKFy5coFHhMWFsbixYvJyMjgwoULhIeHM2rUKKKiomz7/Otf/2LUqFH0798fgEaNGnHs2DEmTZp01eJKRERERMqm7ccu8eGqA6z96xwABgP0aBzO8M61uK1SoItbJyWNy4orLy8vmjdvzqpVq+jVqxcAZrOZVatWMXz48Gse6+PjQ0REBEajke+++46+ffvaHktLS8PNzT5h3t3dHbPZ7PRrEBEREZGSafPhC3y4+gAbDl4AwN3NQM+YcJ7uVIvosAAXt05KKpdOCxwxYgSDBg2iRYsWtGrViilTppCammpLDxw4cCARERFMmjQJgM2bNxMfH09MTAzx8fGMHz8es9nMiy++aDtnjx49eOONN6hWrRoNGjRg586dTJ48mccee8wl1ygiIiIixYPFYmHjoQt8sOoAW45cBMDDzcADzaryVKdoqpf3d3ELpaRzaXHVr18/zp07x7hx4zh9+jQxMTEsW7bMFnJx/Phxu1GojIwMxo4dy+HDhwkICCA2NpY5c+YQEhJi22fq1Km88sorPPXUU5w9e5bw8HD+8Y9/MG7cuFt9eSIiIiJSDFgsFtb+dY4PVx1gx/EEADzdDfRtEckTHaKJLFc80qGl5HN5oMXw4cOvOg1wzZo1dvc7dOjA3r3XzrwMDAxkypQpTJkyxUktFBEREZGSyGKxsOrPs3y4+gC/nUgEwMvDjYdaVeMfHaKoEuzr4hZKaePy4kpERERExJnMZgs/7T3Nh6sOsveUNUbbx9ONR1pX5+/to6gY5HOdM4jcGBVXIiIiIlIqmMwWlu45xUerD7L/TDIAfl7uPNqmOsPaRVEhIP9X74g4k4orERERESnRsk1mfvjtJB+tPsihc6kABHp7MKhtDR67sybl/L1c3EIpK1RciYiIiEiJZDSZWbQznv/8fJCjF9IACPLx4LE7azKkbU2C/YrnFxZL6aXiSkRERBySbTKTarT+11PvXcUFMrNNfLc9nv+sOciJS+kAhPp58ni7KAa2qU6gjzqmuIaKKxERESmUfaeTWLD1BIt2nuBSmgcvb1tJoI8HIX6ehPh6EeLnSbCvZ7771m1eOds9CfbzxNvD3dWXIyVQhtHEgm1xfLLmEKcSMwCoEODF39tH8XDr6vh7662tuJZ6oIiIiFxVUoaRH3afZMHWOHbnRFnnlZyRTXJGNnGkO3ReX0/3gouxnH9f3p6zzc+LEF9P/LzcMRgMzro8KSHSs0zM23KcT9ce4mxyJgAVA735R4doHmpVDV8vFetSPKi4EhERETsWi4VfD1/km21xLP39FBlGMwAebga61qvEA82qkPjXVu7s1JVUo4WENCNJ6UYS0rNISDOSkGYkMd1IQloWCen29xPTjZgtkG40kZ5oso0+FJanu4Fg38ujYNYCzX5UzDZSlqdwC/TxwM1NRVlJk5qZzdxfjzF93WHOp2QBUCXYhyc7RtO3RSQ+niqqpHhRcSUiIiIAnE7M4LsdJ1iwLY5jOeEAALUrBtCvZSS9m0ZQPsAbo9HI0oNQ3t+Lyg4uujKbLSRnZpOYZi3GEnOKr4R0I4lpWbZ/WwuyrDyPGckymTGaLJxPyeR8SqZDz2swYC26fD0Jtiu8cqYu5t2Wp2AL9vXE093NoeeSm5ecYeSLTcf4bN1hLqUZAaga6stTHWvxQPMITSuVYkvFlYiISBmWlW1m9b4zzN8ax9q/zmG2WLcHeHvQo0kV+raIJCYyxGlT8dzcDLZ1WNXwK/RxFouFdKPJbmQsb/FlV4zlLdbSjaRlmbBYsD1GnsKxMAK8PS5PU8ydtpg7Qnbl6FmeKY4aVXFcYpqRWRuPMHP9EZIysgGoUd6PpzrVonfTCBW6UuypuBIRESmD/jqTzIKtcSzaGc+F1Czb9lY1ytG3ZSSxjSrj51V83iYYDAb8vDzw8/IgPMTXoWMzs00kpudMXcxTfOVOUyzwflqW7c19SmY2KZnZxCc4tq7M28PNrhi7XIhZpy0WFP4R4udJgLdHmVtXdik1ixnrjzB741GSM62ve3SYP8M716JH43A8VFRJCVF8fmuKiIhIkUrOMPJ/v51i/tY4dsUl2LZXDPTmgeZV6dO8KlFhAa5rYBHx9nCnYqA7FQN9HDrOZLbkrCWzXzN2uUDLypneeHl9We59k9lCZraZM0mZnElybAqju5vhcpBHnvVjQXmmMob4eRX4uHsJW1d2PiWT6esOM3fTMVKzTADcVimAZzrXJrZRlRJ3PSIqrkREREoxi8XCliMXWbDtBEv3nCLdaH0D6+FmoHPdivRrGUmH28I0MlAAdzcDof5ehPp7OXScxWIhJTPbVohdHh3Lsg/7yLOeLPexzGwzJrOFC6lZdiOKhRXk42GLvb8cg3/l6Fj+AJBbvYbpbFIGn/5ymC83H7MFptSvEsSzXWpxV/3KCh+REkvFlYiISCl0JskaTvHNthMcOZ9q2x4d5p8TTlGVsEBvF7aw9DIYDAT6eBLo40nVUMeOzTCa7KYmXll85bufU6yl5EylS8rIJikjm+MXHXtePy93W9hHsK9Hvmh8u1j8POvLfD0di8Y/lZjBjA37+WprHFnZ1qKqSdVgnulcmy71Kpa56ZBS+qi4EhERKSWMJjOr951lwdY41vx1DlNOOoW/lzv3NQ6nb8tImlVzXjiFOJ+Ppzs+nu5UCnJsCqPRZLZNYbxWuMeV95NyovHTskykZZk46WA0vpe7m216YvCV4R659/288POA+YfdGLllHUaTtV82qxbCs11q0+G2MPVJKTVUXImIiJRwB88ms2DbCRbuOGH7LiCAFtVD6dsyknsbVcHfW3/ySzNPdzfKB3hTPsCx0cgro/ELE4ufe99ospBlMnMuOZNzyYVZV+YGWGhdsxzPdqlN2+jyKqqk1NFvWhERkRIoJTObJb+dZP7WOHYcT7BtrxDgzQPNI+jbIpLoUhhOIc7lrGh8+3CPPPdzi7bULNwyk3j5/lbceVulIrwiEddScSUiIlJCWCwWth+7xPytcSzZc4q0nHQ1dzcDnepYwyk61gnTdwFJkXM0Gt9oNLJ06VJa1yx3C1on4joqrkRERIq5s8kZLNwRz4JtcRw+dzmcIqqCP31bRnJ/0wgqOrhGR0REnE/FlYiISDFkNJlZs/8c87fG8fP+s7ZwCj8vd+5tVIV+LSNpXj1Ua1ZERIoRFVciIiLFyKFzKSzYFsfCHfF2IQHNqoXQr2Uk9zYOJ0DhFCIixZJ+O4uIiLhYamY2S/acYsHWOLYdu2TbXiHAi/ubVaVvi6rUqhjowhaKiEhhqLgSERFxAYvFwo7jl1iw9QT/99tJUnPCKdwM0KlORfq2jKRz3YoKpxARKUFUXImIiNxC55IzWbTzBAu2neDg2RTb9poV/OnToioPNKvq8BfIiohI8aDiSkREpIhlm8ys/csaTrF631myc8IpfD3dic0Jp2hZQ+EUIiIlnYorERGRInL4XArfbD/Bd9tPcDZPOEVMpDWc4r7GVQj08XRhC0VExJlUXImIiDhRWlY2S/ecZsHWOLYcvWjbXs7fi/ubRtC3ZSS3VVI4hYhIaaTiSkRE5CZZLBZ2xSWwYFscP+w+RUpmNmANp+hwWxj9WkbSuW4lvDwUTiEiUpqpuBIREblBF1IyWbQznvlb4ziQJ5yienk/+raI5P5mEVQJ9nVhC0VE5FZScSUiIuKAbJOZdQfOM39rHCv/PGMLp/DxdCO2YRX6toykVY1yuLkpnEJEpKxRcSUiIlIIxy6ksmBbHN9uP8GZpMvhFE2qBtO3ZSQ9moQTpHAKEZEyTcWViIjIVaRnmfjx91PM3xrH5iOXwylC/Tzp3bQqfVtWpW7lIBe2UEREihMVVyIiInlYLBZ2n0i0hlPsOklyTjiFwQDta1vDKbrUq4i3h7uLWyoiIsWNiisRERHgYmoWi3bGs2BrHPvPJNu2R5bzpW/zSB5oXpXwEIVTiIjI1am4EhGRMstktrDuwDkWbItjxd4zGE3WcApvDzfuaViZvi0iuT2qvMIpRESkUFRciYhImXP8QhrfbLeGU5xKzLBtbxRhDaf4W5Nwgn0VTiEiIo5RcSUiImVChtHEst9PM39rHJsOX7BtD/HzpFdMBH1bRFI/XOEUIiJy41RciYhIqWWxWPg9Pon5247zv10nSc64HE5xZ60K9GsZSdd6lfDxVDiFiIjcPBVXIiJS6lxKzWLxrnjmb41j3+nL4RQRIb70bRHJA80jqBrq58IWiohIaaTiqphbvOsk/3fUDdNvp2heozzVyvlhMGhhtYjIlUxmCxsOnmf+tjhW/HGGLJMZAC8PN+5uYA2naButcAoRESk6Kq6KuWV/nGHtKTfWfrMHsK4NaFw1hJiqwTSJDKFx1RDCAr1d3EoREdeJu5jGN9tP8N32E8QnpNu2NwgPol9OOEWIn5cLWygiImWFiqti7v6m4WQlnCHJI5Q/TyWTkGbkl7/O8ctf52z7RIT40iQymCZVQ2gSGUKjiGD8vfWjFZHSK8NoYvkfp1mwLY4NBy+HUwT7etIrJpw+LSJpGBHswhaKiEhZpHfgxdxd9SuRfdRMbGxrLAZ39p1OYndcArviEvntRAIHz6UQn5BOfEI6S/ecBsDNALUrBtI4Z3QrJjKEOpUD8XR3c/HViIjcnN/jE1mwLY7FO+NJygmnAGs4RZ8WVeneoLLCKURExGVUXJUgXh5uNK5qnQr4aBvrtuQMI3viE9kdl8juuAR+O5HAycQM9p9JZv+ZZL7ZfgKwfiFmg/Ag65TCSOsIV43yWr8lIsVfQloW/9t1kvlb49h7Ksm2PSLElwebV+XB5lWJLKdwChERcT0VVyVcoI8nbaMr0Da6gm3b2aQMdp+wFlu7TySwOy6BpIxsdhxPYMfxBNt+wb6e1tGtnOmETSKDqRjo44KrEBGxZzZb2HjoAvO3xbH8j9NkZeeEU7i7cVeDSvRtEckdtSrgrnAKEREpRlRclUIVg3zoVt+HbvUrAdbveTl6IS1nOqF1dOv3k0kkphtZd+A86w6ctx0bHuxD4zzFVqOIYAJ9PF11KSJSxpy4lMa320/wzTb7cIp6VYLo16IqPWMiCPVXOIWIiBRPKq7KAIPBQM0K/tSs4E+vphEAZGWb+etMMrviEnKmEyby19lkTiZmcDLxNMv+OJ1zLNQKC7AWWzlruOpWDsLLQ+u3RMQ5MowmVuw9w4Jtcaw/eB6Lxbo90MeDXjER9G0RScOIIE1jFhGRYk/FVRnl5eFGw4hgGkYE88jt1QFIyczm9/i80wkTiU9I58DZFA6cTeHbnPVbXu5u1A8PshVbTSJDqFneX98dIyIO2XsyyRpOsSuehDSjbXvb6PL0bRHJ3Q0VTiEiIiWLiiuxCfD24Pao8tweVd627VxyJr/lrNvadcKaUJiQZmRXzhRDNh0DrJ8wN6kaYpdQWClI67dExF5iupHvd8Uzf1scv8dfDqeoEuzDg82r0qd5JNXKK5xCRERKJhVXck1hgd50qVeJLvUur986fjEtZzphIrtPJPB7fCLJGdmsP3ie9Qcvr9+qHORDk8hgW0Jho6rBBGn9lkiZYzZb+PWwNZxi2e+nycwJp/B0N3BX/cr0aVGVdrXDFE4hIiIlnoorcYjBYKB6eX+ql/enZ4x1/ZbRZF2/lRsHv/tEAn+dSeZ0Ugan/8hg+R9nbMdHh/nnrN+yTiesVyUQbw9N+xEpjU4mpFvDKbbHEXfxcjhFnUqB9G0ZSe+mEZRTOIWIiJQiKq7kpnm6u9EgPJgG4cE81LoaAGlZ2fwen/OFxznTCk9cSufQuVQOnUtl4Y74nGMN1K8SZEsojIkMJqpCgNZviZRQmdkmVu49y/xtcaw7cO5yOIW3B3+LCadvi0gaVw1WOIWIiJRKKq6kSPh5edCqZjla1Sxn23YhJZPfTiRapxSesCYUXkzNsn4n14lE5vyas37L24NGVXOnE1rXcFUO8tGbMZFibN/pJOZvjWPxzngu5QmnuD2qHH1bRHJPwyr4emmUWkRESjcVV3LLlA/wplPdinSqWxGwrt86cSndFgdvXb+VRHJmNhsPXWDjoQu2YysGetvFwTeOCCHYT+u3RFwpKcPI97tO8s22OHafSLRtrxTkbQunqFHB34UtFBERubVUXInLGAwGIsv5EVnOjx5NwgHINpk5cDbFVmztikvkrzPJnE3OZMXeM6zYe3n9VlQF6/qt3ITC+lWCFNssUsTMZgubj1xkwbY4lu45ZRdO0bVeJfq2iKT9bQqnEBGRsknFlRQrHu5u1KsSRL0qQfRvZV2/lZ5l4o+TudMJraEZxy+mcfh8KofPp7Jop3X9loebgXpVguwSCqPDAvQmT8QJTidm8O32OBZsO8Hxi2m27bUrBtAvJ5yifIC3C1soIiLieiqupNjz9XKnRY1ytKhxef3WxdSsnO/fSsz5wuMELqRmsSc+kT3xicBxAPy93GmU+2XHOaEZ4cFavyVSGFnZZlb9eYb52+L45a9zmHPCKQK8PejRJJy+LaoSExmi/59ERERyqLiSEqmcvxcd61SkY53L67fiE9JtxdauOOv3b6Vmmfj18EV+PXzRdmyFAG9icka3ctdxhfgpDlok119nkpm/NY5FO+O5mJpl296qpjWcIrZRZfy89OdDRETkSvrrKKWCwWCgaqgfVUP9uLdxFQBMZgsHc9Zv5cbB7zudzPmUTFb+eZaVf561HV+jvF/O+i1rQmGD8GCt35IyJSMbvt56gm93nmR3XIJte8VAbx5oXpW+LSKpqXAKERGRa1JxJaWWu5uBOpUDqVPZ+oWlABlGE3+cTLIFZuyOS+DohTTb7X+7TgLW9Vt1KgfaJRTWrhio9VtSYpjMFpLSjSSkG0lIyyIh3UhimvXfienZJKRnWe+nG7mUmsnv8e4YzXsBa//vXLci/VpG0uG2MDzc3Vx8NSIiIiWDiispU3w83WlePZTm1UNt2xLSsvgtJygjN6HwfEomf5xM4o+TSczbbN3Pz8udhhHBxOQmFFYNoWqor9abSJHKzDaRmFsYpRtJsBVIRhJz76df3pb7eFJGtoPPZCCqgj/9W0XSu2lVwgIVTiEiIuIoFVdS5oX4edH+tjDa3xYGWNdvnUrMsJtOuOeEdf3WliMX2XLk8vqt8v5etrCMxpHWgqucv9ZviT2LxUK60ZRT+BjtRo1yC6LE9Kw8jxtJzBltSssy3dRzB3h7EOzrSYhfzs3Xi2A/T0J8L98P8HLjyB/bGPZgW7y81H9FRERulIorkSsYDAbCQ3wJD/HlnkaX128dPpeSEwdvTSn881QSF1KzWL3vLKv3XV6/Va2cn910wobhwfh6af1WaWA2W0jOzM4pjLLsCiH7UaQ8xVLOqFOWyXzDz2swYC2QfD0J9vPKUxhdcd/Pk2BfL1sxFezriWchpvQZjUaWHkWjsCIiIjdJxZVIIbi7GahdKZDalQLp0+Ly+q0/T+Wu37JOKzx8PpXjF9M4fjGNH3aftB17W6XAywmFVUO4rVKA1rG4ULbJbC2GChg1ujzdLivP45en3eXGkd8IT3cDwb5etsIotxiyu5+nWLIWVF4E+njgpvV+IiIixZ6KK5Eb5OPpTtNqoTStdnn9VmK6kT0nLsfB74pL4FxyJn+eSuLPU0l8tSUu51g3GkUE50wnDCGmagiR5bR+y1EZRpPdOiNbaEO6/UhS3pGmxDQjyZmOrkey5+vpfrn4yZlaZy2MLv87xNf6eLCfJyE5BZOfl7t+xiIiIqWYiisRJwr29eTO2hW4s3YFwLrW5nRSht2XHf92IpGUzGy2Hr3E1qOXbMeG+nnm+bJja+FVPqD0hwpYLBZSs0zW4igtb0iD9X7SFffzPp5hvPGpdgCBPh72xdGVxZJv7rbLBVOQr6di+kVERKRAxaK4+vjjj3n33Xc5ffo0TZo0YerUqbRq1arAfY1GI5MmTWL27NnEx8dTp04d3n77be6++27bPjVq1ODYsWP5jn3qqaf4+OOPi+w6RK5kMBioEuxLlWBf7m5YGbCu2zl8PsWu4Np7KolLaUbW7D/Hmv3nbMdXDfWlSc7IVuOqwTSMCMbfu1j8b5uPyWwhOeNq6XU5RVGeIIe8j2ffxFw7NwO2kaHLQQ1eeQqlyyNKeR8P8vHQ1EwRERFxKpe/S5s/fz4jRoxg2rRptG7dmilTptC9e3f2799PxYoV8+0/duxY5s6dy/Tp06lbty7Lly+nd+/ebNy4kaZNmwKwdetWTKbLCVu///473bp1o0+fPrfsukSuxs3NQK2KgdSqGMgDzasC1rjtP08l81vOdMLdcQkcOpfKiUvpnLiUzpLfTlmPNcBtlQLt0gnrVA4sVGhBYWVlm3PWHdmn1xUU/52YZ11SUoYRy02sR/Jyd7t6op2fF0G+9gl3udPwAry0HklERESKB5cXV5MnT2bYsGEMGTIEgGnTprFkyRJmzpzJqFGj8u0/Z84cxowZQ2xsLABPPvkkK1eu5P3332fu3LkAhIWF2R3z1ltvER0dTYcOHYr4akRujLeHOzGRIcREhjCwjXVbUoaR308k2uLgd8clcjopg32nk9l3Opn52+JyjnWjYc76rSaRwTSoHIDFAulZJs6nZecZOcrCPs0uf5BDQloWqTcZ/e3v5W4/cpQntCE4T4FkC3LIKZZ8PN20HklERERKNJcWV1lZWWzfvp3Ro0fbtrm5udG1a1c2bdpU4DGZmZn4+PjYbfP19WX9+vVXfY65c+cyYsSIq75xy8zMJDMz03Y/KSkJsE5BNBqNDl2Ts+U+v6vbIbeerzu0rB5My+rBQHUAziRlsCc+yfqlx/GJ7IlPIjkjm+3HLrH92OX1W264Y/511Q0/t8EAQT4el+O/fS+vRwryyS2O7B/PfczL40ZG0cxkZ9/c+im5cfo9I45SnxFHqc+Io4pTn3GkDS4trs6fP4/JZKJSpUp22ytVqsS+ffsKPKZ79+5MnjyZ9u3bEx0dzapVq1i4cKHdNMC8Fi9eTEJCAoMHD75qOyZNmsSECRPybf/pp5/w8/Mr/AUVoRUrVri6CVKM1AXqVoQ+YXAuA46nGDieYuBYioETqWCyWD9IcDNY8PMAfw/w8wA/Dwt+7nn+nbPd/4r7vh7gZsgGMvI/eVbOLQEygTM5Nyn59HtGHKU+I45SnxFHFYc+k5aWVuh9XT4t0FEffPABw4YNo27duhgMBqKjoxkyZAgzZ84scP8ZM2Zwzz33EB4eftVzjh49mhEjRtjuJyUlERkZyV133UVQUJDTr8ERRqORFStW0K1bNzw9PV3aFikZUjMyWfzjKu7t1olgfx9NtZPr0u8ZcZT6jDhKfUYcVZz6TO6stsJwaXFVoUIF3N3dOXPG/nPvM2fOULly5QKPCQsLY/HixWRkZHDhwgXCw8MZNWoUUVFR+fY9duwYK1euZOHChddsh7e3N97e+SOvPT09Xf7DzFWc2iLFmz8Q6g0hAb7qM+IQ/Z4RR6nPiKPUZ8RRxaHPOPL8Ls0h9vLyonnz5qxadXltiNlsZtWqVbRp0+aax/r4+BAREUF2djbfffcdPXv2zLfPrFmzqFixIvfee6/T2y4iIiIiIpKXy6cFjhgxgkGDBtGiRQtatWrFlClTSE1NtaUHDhw4kIiICCZNmgTA5s2biY+PJyYmhvj4eMaPH4/ZbObFF1+0O6/ZbGbWrFkMGjQIDw+XX6aIiIiIiJRyLq86+vXrx7lz5xg3bhynT58mJiaGZcuW2UIujh8/jpvb5QG2jIwMxo4dy+HDhwkICCA2NpY5c+YQEhJid96VK1dy/PhxHnvssVt5OSIiIiIiUka5vLgCGD58OMOHDy/wsTVr1tjd79ChA3v37r3uOe+66y4sN/ONpiIiIiIiIg5w6ZorERERERGR0kLFlYiIiIiIiBOouBIREREREXECFVciIiIiIiJOoOJKRERERETECVRciYiIiIiIOIGKKxERERERESdQcSUiIiIiIuIEKq5EREREREScQMWViIiIiIiIE6i4EhERERERcQIVVyIiIiIiIk6g4kpERERERMQJVFyJiIiIiIg4gYorERERERERJ1BxJSIiIiIi4gQqrkRERERERJxAxZWIiIiIiIgTqLgSERERERFxAhVXIiIiIiIiTqDiSkRERERExAlUXImIiIiIiDiBiisREREREREnUHElIiIiIiLiBCquREREREREnEDFlYiIiIiIiBOouBIREREREXECFVciIiIiIiJOoOJKRERERETECVRciYiIiIiIOIGKKxERERERESdQcSUiIiIiIuIEKq5EREREREScQMWViIiIiIiIE6i4EhERERERcQIVVyIiIiIiIk6g4kpERERERMQJVFyJiIiIiIg4gYorERERERERJ1BxJSIiIiIi4gQqrkRERERERJxAxZWIiIiIiIgTqLgSERERERFxAhVXIiIiIiIiTqDiSkRERERExAlUXImIiIiIiDiBiisREREREREnUHElIiIiIiLiBCquREREREREnEDFlYiIiIiIiBOouBIREREREXECh4urGjVqMHHiRI4fP14U7RERERERESmRHC6unn/+eRYuXEhUVBTdunXj66+/JjMzsyjaJiIiIiIiUmLcUHG1a9cutmzZQr169XjmmWeoUqUKw4cPZ8eOHUXRRhERERERkWLvhtdcNWvWjA8//JCTJ0/y6quv8tlnn9GyZUtiYmKYOXMmFovFme0UEREREREp1jxu9ECj0ciiRYuYNWsWK1as4Pbbb2fo0KGcOHGCl19+mZUrVzJv3jxntlVERERERKTYcri42rFjB7NmzeKrr77Czc2NgQMH8u9//5u6deva9unduzctW7Z0akNFRERERESKM4eLq5YtW9KtWzc++eQTevXqhaenZ759atasSf/+/Z3SQBERERERkZLA4eLq8OHDVK9e/Zr7+Pv7M2vWrBtulIiIiIiISEnjcKDF2bNn2bx5c77tmzdvZtu2bU5plIiIiIiISEnjcHH19NNPExcXl297fHw8Tz/9tFMaJSIiIiIiUtI4XFzt3buXZs2a5dvetGlT9u7d65RGiYiIiIiIlDQOF1fe3t6cOXMm3/ZTp07h4XHDye4iIiIiIiIlmsPF1V133cXo0aNJTEy0bUtISODll1+mW7duTm2ciIiIiIhISeHwUNN7771H+/btqV69Ok2bNgVg165dVKpUiTlz5ji9gSIiIiIiIiWBw8VVREQEv/32G19++SW7d+/G19eXIUOGMGDAgAK/80pERERERKQscHhaIFi/x+rvf/87H3/8Me+99x4DBw684cLq448/pkaNGvj4+NC6dWu2bNly1X2NRiMTJ04kOjoaHx8fmjRpwrJly/LtFx8fzyOPPEL58uXx9fWlUaNGiokXEREREZEidcMJFHv37uX48eNkZWXZbf/b3/5W6HPMnz+fESNGMG3aNFq3bs2UKVPo3r07+/fvp2LFivn2Hzt2LHPnzmX69OnUrVuX5cuX07t3bzZu3Gibonjp0iXuuOMOOnXqxI8//khYWBgHDhwgNDT0Ri9VRERERETkuhwurg4fPkzv3r3Zs2cPBoMBi8UCgMFgAMBkMhX6XJMnT2bYsGEMGTIEgGnTprFkyRJmzpzJqFGj8u0/Z84cxowZQ2xsLABPPvkkK1eu5P3332fu3LkAvP3220RGRjJr1izbcTVr1nT0MkVERERERBzicHH13HPPUbNmTVatWkXNmjXZsmULFy5c4IUXXuC9994r9HmysrLYvn07o0ePtm1zc3Oja9eubNq0qcBjMjMz8fHxsdvm6+vL+vXrbfe///57unfvTp8+fVi7di0RERE89dRTDBs27KptyczMJDMz03Y/KSkJsE5DNBqNhb6mopD7/K5uh5Qc6jPiKPUZcZT6jDhKfUYcVZz6jCNtMFhyh54KqUKFCqxevZrGjRsTHBzMli1bqFOnDqtXr+aFF15g586dhTrPyZMniYiIYOPGjbRp08a2/cUXX2Tt2rVs3rw53zEPPfQQu3fvZvHixURHR7Nq1Sp69uyJyWSyFUe5xdeIESPo06cPW7du5bnnnmPatGkMGjSowLaMHz+eCRMm5Ns+b948/Pz8CnU9IiIiIiJS+qSlpfHQQw+RmJhIUFDQNfd1eOTKZDIRGBgIWAutkydPUqdOHapXr87+/ftvrMWF9MEHHzBs2DDq1q2LwWAgOjqaIUOGMHPmTNs+ZrOZFi1a8OabbwLQtGlTfv/992sWV6NHj2bEiBG2+0lJSURGRnLXXXdd9wUsakajkRUrVtCtWzelMUqhqM+Io9RnxFHqM+Io9RlxVHHqM7mz2grD4eKqYcOG7N69m5o1a9K6dWveeecdvLy8+O9//0tUVFShz1OhQgXc3d05c+aM3fYzZ85QuXLlAo8JCwtj8eLFZGRkcOHCBcLDwxk1apTd81apUoX69evbHVevXj2+++67q7bF29sbb2/vfNs9PT1d/sPMVZzaIiWD+ow4Sn1GHKU+I45SnxFHFYc+48jzOxzFPnbsWMxmMwATJ07kyJEjtGvXjqVLl/Lhhx8W+jxeXl40b96cVatW2baZzWZWrVplN02wID4+PkRERJCdnc13331Hz549bY/dcccd+UbQ/vrrL6pXr17otomIiIiIiDjK4ZGr7t272/5dq1Yt9u3bx8WLFwkNDbUlBhbWiBEjGDRoEC1atKBVq1ZMmTKF1NRUW3rgwIEDiYiIYNKkSQBs3ryZ+Ph4YmJiiI+PZ/z48ZjNZl588UXbOf/5z3/Stm1b3nzzTfr27cuWLVv473//y3//+19HL1VERERERKTQHCqujEYjvr6+7Nq1i4YNG9q2lytX7oaevF+/fpw7d45x48Zx+vRpYmJiWLZsGZUqVQLg+PHjuLldHlzLyMhg7NixHD58mICAAGJjY5kzZw4hISG2fVq2bMmiRYsYPXo0EydOpGbNmkyZMoWHH374htooIiIiIiJSGA4VV56enlSrVs2h77K6nuHDhzN8+PACH1uzZo3d/Q4dOrB3797rnvO+++7jvvvuc0bzRERERERECsXhNVdjxozh5Zdf5uLFi0XRHhERERERkRLJ4TVXH330EQcPHiQ8PJzq1avj7+9v9/iOHTuc1jgREREREZGSwuHiqlevXkXQDBERERERkZLN4eLq1VdfLYp2iIiIiIiIlGgOr7kSERERERGR/BweuXJzc7vm91k5M0lQRERERESkpHC4uFq0aJHdfaPRyM6dO5k9ezYTJkxwWsNERERERERKEoeLq549e+bb9uCDD9KgQQPmz5/P0KFDndIwERERERGRksRpa65uv/12Vq1a5azTiYiIiIiIlChOKa7S09P58MMPiYiIcMbpREREREREShyHpwWGhobaBVpYLBaSk5Px8/Nj7ty5Tm2ciIiIiIhISeFwcfXvf//brrhyc3MjLCyM1q1bExoa6tTGiYiIiIiIlBQOF1eDBw8ugmaIiIiIiIiUbA6vuZo1axbffPNNvu3ffPMNs2fPdkqjREREREREShqHi6tJkyZRoUKFfNsrVqzIm2++6ZRGiYiIiIiIlDQOF1fHjx+nZs2a+bZXr16d48ePO6VRIiIiIiIiJY3DxVXFihX57bff8m3fvXs35cuXd0qjREREREREShqHi6sBAwbw7LPP8vPPP2MymTCZTKxevZrnnnuO/v37F0UbRUREREREij2H0wJfe+01jh49SpcuXfDwsB5uNpsZOHCg1lyJiIiIiEiZ5XBx5eXlxfz583n99dfZtWsXvr6+NGrUiOrVqxdF+0REREREREoEh4urXLVr16Z27drObIuIiIiIiEiJ5fCaqwceeIC333473/Z33nmHPn36OKVRIiIiIiIiJY3DxdUvv/xCbGxsvu333HMPv/zyi1MaJSIiIiIiUtI4XFylpKTg5eWVb7unpydJSUlOaZSIiIiIiEhJ43Bx1ahRI+bPn59v+9dff039+vWd0igREREREZGSxuFAi1deeYX777+fQ4cO0blzZwBWrVrFvHnz+Pbbb53eQBERERERkZLA4eKqR48eLF68mDfffJNvv/0WX19fmjRpwurVqylXrlxRtFFERERERKTYu6Eo9nvvvZd7770XgKSkJL766itGjhzJ9u3bMZlMTm2giIiIiIhISeDwmqtcv/zyC4MGDSI8PJz333+fzp078+uvvzqzbSIiIiIiIiWGQyNXp0+f5vPPP2fGjBkkJSXRt29fMjMzWbx4scIsRERERESkTCv0yFWPHj2oU6cOv/32G1OmTOHkyZNMnTq1KNsmIiIiIiJSYhR65OrHH3/k2Wef5cknn6R27dpF2SYREREREZESp9AjV+vXryc5OZnmzZvTunVrPvroI86fP1+UbRMRERERESkxCl1c3X777UyfPp1Tp07xj3/8g6+//prw8HDMZjMrVqwgOTm5KNspIiIiIiJSrDmcFujv789jjz3G+vXr2bNnDy+88AJvvfUWFStW5G9/+1tRtFFERERERKTYu+EodoA6derwzjvvcOLECb766itntUlERERERKTEuaniKpe7uzu9evXi+++/d8bpREREREREShynFFciIiIiIiJlnYorERERERERJ1BxJSIiIiIi4gQqrkRERERERJxAxZWIiIiIiIgTqLgSERERERFxAhVXIiIiIiIiTqDiSkRERERExAlUXImIiIiIiDiBiisREREREREnUHElIiIiIiLiBCquREREREREnEDFlYiIiIiIiBOouBIREREREXECFVciIiIiIiJOoOJKRERERETECVRciYiIiIiIOIGKKxERERERESdQcSUiIiIiIuIEKq5EREREREScQMWViIiIiIiIE6i4EhERERERcQIVVyIiIiIiIk6g4kpERERERMQJVFyJiIiIiIg4gYorERERERERJ1BxJSIiIiIi4gQqrkRERERERJxAxZWIiIiIiIgTFIvi6uOPP6ZGjRr4+PjQunVrtmzZctV9jUYjEydOJDo6Gh8fH5o0acKyZcvs9hk/fjwGg8HuVrdu3aK+DBERERERKcNcXlzNnz+fESNG8Oqrr7Jjxw6aNGlC9+7dOXv2bIH7jx07lk8//ZSpU6eyd+9ennjiCXr37s3OnTvt9mvQoAGnTp2y3davX38rLkdERERERMoolxdXkydPZtiwYQwZMoT69eszbdo0/Pz8mDlzZoH7z5kzh5dffpnY2FiioqJ48skniY2N5f3337fbz8PDg8qVK9tuFSpUuBWXIyIiIiIiZZSHK588KyuL7du3M3r0aNs2Nzc3unbtyqZNmwo8JjMzEx8fH7ttvr6++UamDhw4QHh4OD4+PrRp04ZJkyZRrVq1q54zMzPTdj8pKQmwTkE0Go03dG3Okvv8rm6HlBzqM+Io9RlxlPqMOEp9RhxVnPqMI20wWCwWSxG25ZpOnjxJREQEGzdupE2bNrbtL774ImvXrmXz5s35jnnooYfYvXs3ixcvJjo6mlWrVtGzZ09MJpOtQPrxxx9JSUmhTp06nDp1igkTJhAfH8/vv/9OYGBgvnOOHz+eCRMm5Ns+b948/Pz8nHjFIiIiIiJSkqSlpfHQQw+RmJhIUFDQNfctccXVuXPnGDZsGD/88AMGg4Ho6Gi6du3KzJkzSU9PL/B5EhISqF69OpMnT2bo0KH5Hi9o5CoyMpLz589f9wUsakajkRUrVtCtWzc8PT1d2hYpGdRnxFHqM+Io9RlxlPqMOKo49ZmkpCQqVKhQqOLKpdMCK1SogLu7O2fOnLHbfubMGSpXrlzgMWFhYSxevJiMjAwuXLhAeHg4o0aNIioq6qrPExISwm233cbBgwcLfNzb2xtvb+982z09PV3+w8xVnNoiJYP6jDhKfUYcpT4jjlKfEUcVhz7jyPO7NNDCy8uL5s2bs2rVKts2s9nMqlWr7EayCuLj40NERATZ2dl899139OzZ86r7pqSkcOjQIapUqeK0touIiIiIiOTl8rTAESNGMH36dGbPns2ff/7Jk08+SWpqKkOGDAFg4MCBdoEXmzdvZuHChRw+fJh169Zx9913YzabefHFF237jBw5krVr13L06FE2btxI7969cXd3Z8CAAbf8+kREREREpGxw6bRAgH79+nHu3DnGjRvH6dOniYmJYdmyZVSqVAmA48eP4+Z2uQbMyMhg7NixHD58mICAAGJjY5kzZw4hISG2fU6cOMGAAQO4cOECYWFh3Hnnnfz666+EhYXd6ssTEREREZEywuXFFcDw4cMZPnx4gY+tWbPG7n6HDh3Yu3fvNc/39ddfO6tpIiIiIiIiheLyaYEiIiIiIiKlgYorERERERERJ1BxJSIiIiIi4gQqrkRERERERJxAxZWIiIiIiIgTqLgSERERERFxAhVXIiIiIiIiTqDiSkRERERExAlUXImIiIiIiDiBiisREREREREnUHElIiIiIiLiBCquREREREREnEDFlYiIiIiIiBOouBIREREREXECFVciIiIiIiJOoOJKRERERETECVRciYiIiIiIOIGHqxsgIiIudHYfbls+o8WRXRj+zIYGfwN3T1e3SkREpERScSUiUtaYjLBvCWz9DI6uwx2IAFi4FVZUgeZDoPkgCKzs4oaKiIiULCquRETKiuTTsP1z6y35lHWbwQ3zbfdw6BLUSt2CIfkUrHkTfnkH6v0NWj4O1duCweDKlouIiJQIKq5EREoziwWObbCOUv35A5izrdv9w6DZIGgxBJNfJfYuXUqNu/6L54EfrfvG/Qp/LLTeKjaAlkOhcV/wDnTt9YiIiBRjKq5EREqjzGTY/TVsnQHn/ry8PfJ2aDXMOirl4WXdZjRa/+vhDY37WG+nfrMWWXu+gbN/wJIRsOJViBlgHc0Kq3Prr0lERKSYU3ElIlKanN1nLYp2fw1ZydZtnn7WUaeWj0PlRoU7T5XG8LcPodtE2P2V9ZwXDsKW/1pvNdpZi7Q694K7/pSIiIiAiisRkZLvioAKm/K1rAVVkwHgG3Jj5/YNgdufhFb/gCNrrc+xf6n1eY6ug8BwaD5YARgiIiKouBIRKbmST8P22bB9ll1ABXVirUVVVEfnBVG4uUF0J+stIc4airFjNiSftA/AaDUMqrVRAIaIiJRJKq5EREoSiwWObYSt068aUEFw1aJtQ0gkdHkFOrwIe7+3tiVucwEBGP3AO6Bo2yIiIlKMqLgSESkJMpPht/nWgIqzey9vj7zdOkpV/2/WQIpbSQEYIiIidlRciYgUZ9cKqGgx1Bo8URxcLwCjZntrkaUADBERKcX0F05EpLgxGa2hEVumOz+goqjZBWCssY607V8KR36x3gLDrVMXmw2CwEqubq2IiIhTqbgSESkurhdQUbODNViiJHBzg+jO1ltCnPWatucEYPz8Bqx9WwEYIiJS6qi4EhFxpesFVDQfbA2QKMlCIqHLOOjwkgIwRESkVFNxJSLiClcNqGgNLYe5JqCiqOULwJgOv+UJwFg53jrlseVQBWCIiEiJpOJKRORWOrffGvaw6yv7gIpGfaxT/4pLQEVRq9IY/jYVur0Gu+ZZX5OLh2DLp9ZbzfbWIrNOrAIwRESkxNBfLBGRombKhv1LSmZARVHzDYE2T0HrJ6wBGFs+g79+VACGiIiUSCquRESKii2g4nNrkAPkCagYCjU7lpyAiqJ23QCMd6xTJVs+rgAMEREptlRciYg4ky2g4jP48/vLARV+FaD5IGg+pOQHVBQ1uwCM/1lfy7jN8Pt31lvFBtDqcWjUVwEYIiJSrKi4EhFxhsyUPAEVf1zeXpoDKoqah7f1y5Ib94VTu61FVm4Axv/9E1a8mhOA8TiE3ebq1oqIiKi4EhG5KQqouDWqNMkJwJhofa3zBWB0sL7eCsAQEREX0l8gERFH5QZUbP3MGrqQq1y09Q1+zENlN6CiqPmGXiUAY631FhRhnXrZbKACMERE5JZTcSUiUljJZ2DHbNg2yz6g4rZ7rGuAanZUQMWtYheAcdz6M9nxBSTFw8+vw9q3cwIwhkG12xWAISIit4SKKxGRa7FY4Pgma4y6AiqKp5Bq0PVV6DjKGoCxZTqc2HI5AKNSQ2s6owIwRESkiKm4EhEpyDUDKh6H+j0VUFHcXC0A48zvCsAQEZFbQsWViEhe5/ZbC6rdX0FmknWbhy807mOdYqaAipKhMAEYrYZZp3QqAENERJxEf1FEREzZsH8pbJ2ugIrSJm8AxuGfrUXWX8vyB2A0HwQBFV3dWhERKeFUXIlI2aWAirLDzQ1qdbHeFIAhIiJFRMWViJQtuQEVWz+Dvd+D2Wjd7lfBGt/dYog1IEFKr7wBGH8stvYFBWCIiIgTqLgSkbIhMwX2LLB+L1LegIqqraxrbxRQUfZ4eEOTftbbyV2wbUb+AIyYh6xTQyvUdnVrRUSkBFBxJSKl27m/rCMTBQZUPG4NPhAJj8kTgDEvJwDjMGyeZr0pAENERApBfyFEpPSxBVR8Zg0tyGULqBhgDToQuZJvKLR5Glo/qQAMERFxmIorESk9ks9YQwq2z7IGFUBOQMXd1qIqqpMCKqRw8gVgzCwgAKOntV8pAENERHKouBKRks1igeO/WmPUFVAhRSGkGnQdDx1H5wRgTIcTW+H3b623Sg2tRVbjvuDl7+rWioiIC6m4EpGSKTegYusMawBBLgVUSFG5MgBj62ew59ucAIznYcU4BWCIiJRxKq5EpGRRQIUUB+Ex0PMjuOu1ggMwojpa+6MCMEREyhT9xheR4s+UDX/9CFumXxFQEZUTUPGQAirENewCMFZbR1L/WgaH11hvQVWhxWBopgAMEZGyQMWViBRfKWdh+2wFVEjx5+YGtbpab5eOWfvsji8g6QSsfh3W5ARgtBoGka0VgCEiUkqpuBKR4sUWUPEZ7P2fAiqk5Amtbg3A6DDK2ofzBWA0gpZDFYAhIlIKqbgq5ty2z6TpsR9wW78PKkRbp0GVq6kpUFL6XCugouXj0KCXAiqkZPH0uUoAxp6cAIxXcwIwhioAQ0qf7CxIOAYXj8DFw7hdOEStM4mQ2Q48y7m6dSJFRsVVMWc4/DPVLq6DtevsH/ANtRZaoTUvF1y59wMqasqJlBznD1jfdO6aZx9Q0ehBa1EVHuPS5ok4Rd4AjJ1fwrYZOQEYn1hvUR2h5TDrlFcFYEhJkZVqLZ4uWQuo3EKKi0esU2ItZtuu7kADwPLJz9DpZWj6qPq6lErq1cWcudlg9if7USfMC7eEo9ZfWilnIP0SxG+33q7k6Z9TcNXIX4AFRYCb+62+DBF7CqiQsso3FNoOh9ufsgZgbPlMARhSvKVfylM45S2kct6PXIunX877kBqYgqqSvnsRAalnrCO3m6dBt9egdjd9ICylioqrYs4S3YW/9mdSKzYWN09P68bMFLh0NP8nRZeOQOIJMKZap52c2ZP/hO5eEFqj4BGvkGrg4XUrL0/KmpSzsGM2bPvc+qkm5AmoGApRnRVQIWWDAjCkuLBYrL+bc99H5BZOue8tMhKufbxPSM77iSveU5SLsptJYzYaWZ3ZinsrncZ93btwbh/M62Mdte32GlRpXNRXKnJLqLgqibwDoHJD6+1K2ZmQcNy+4Mr9RXnpGJiy4Pxf1tuVDG4QXPXyL8m8BVhoTfDyK/prk9LHYoG4zdZRKruAivLWgIrmQ6wBACJllV0AxmLrNNkrAzBaPQ6N+igAQ26M2WRNXLWbunfY+kHtxSPWD2WvJaDyFYVTzcvvDfwKv37K4uaBueXfcW/6EKx7HzZ/ah2x/bS9dcZC57EQFH5TlyriaiquShsPb+vC6IIWR5tN1pGtfCNeR63/NaZZC7OE49ZfdlcKqJznk6krPp3yDSniC5MSJysVfltgfaNoF1DR0rq2RAEVIvY8faBJf+vt5C5rymBuAMYPz8FP43ICMB6HCrVc3VopbrKzcj5cLWAEKiHnw9Wryf1wtaBZLaE1rB/qOpNvKNz1urUvr5oIv38Hu76E3xdC22fgjmfBO9C5zylyi6i4Kkvc3K2fkIZWtw7D52WxWOdO5xvxyjMtIOW09XZ8Y/5z5wvYyPPL2T9MU1rKEgVUiNy88Bjo+bF1utSuedb/py4dyROA0cn6/5MCMMqWrNTLH4jajUDlLAvIEyCRj5untVC68sPRclGuWxYQWgMenGldg7h8DMT9Cr+8A9s/V+iFlFjqsWJlMEBgZeutepv8j6ddzCm4juQvwBwN2Mj7Sz0oQmtsSoPcgIqtn9mPepaLghZDoenDCqgQuRF+5S4HYBxabf1/7K9lcPhn600BGKVP+qUr/s7m+bAz5fS1j80TIJFvBCq4avENtKraAh5bBn/+ACtftV7v/z1vnTZ412vWtYn6kFZKCBVXUjh+5ay3iOb5H8sN2Mg34nUEEuNuLGCjXBQERypgo7grKKACg/XT9FaPK6BCxFnc3KB2V+vt0jHYNjN/AEaDXtYpt5Gt9Ea0OMsNkMgXX57zNzT90rWPtwVIFDACVZK/isVggPp/s/792DYD1r4N5/6ELx+0zra563Wo3MjVrRS5LhVXcvMKFbBxOP+IV6ECNiIL/gMSWkMBG66igAoR1wqtDt0mQMfR1gCMLdMhfhvs+cZ6q9zIOmVQARiuYwuQKGiqfWECJCoV8F2WjgdIlEgeXnD7k9a1h7+8B1v+a50RMa0dxDwMncco9EKKtWJRXH388ce8++67nD59miZNmjB16lRatWpV4L5Go5FJkyYxe/Zs4uPjqVOnDm+//TZ33313gfu/9dZbjB49mueee44pU6YU4VVIgQoTsFHQH55LR3ICNo5ZbwUFbARWyfOHp4YCNoqaLaBihv0oZG5ARf2e1gX5InJr2AVg7LROGdzzLZxWAMYtkRsgUdAIVGECJIKq2gdEFWWAREnkGwrd37gcevHHQtg11xp+odALKcZcXlzNnz+fESNGMG3aNFq3bs2UKVPo3r07+/fvp2LF/PPHx44dy9y5c5k+fTp169Zl+fLl9O7dm40bN9K0aVO7fbdu3cqnn35K48b67oRiKW/ABp3sH7syYOPK9KOMREg+Zb0VGLBRroARLwVs3JDzB6wF1a55kJlo3ebhkxNQMUwBFSLFQXjTPAEYX1r/n1UAxs3LSrti3VOev0WFCpCoXnDYU0g1paUWVrma0GeWdd3hT2OsMydyQy86j4GYR9SnpVhxeW+cPHkyw4YNY8iQIQBMmzaNJUuWMHPmTEaNGpVv/zlz5jBmzBhiY2MBePLJJ1m5ciXvv/8+c+fOte2XkpLCww8/zPTp03n99ddvzcWI8zgUsHHFlMOUM5B+EeIvFhyw4RVg/z0dCtjIz5RtXTS/dbr9qGFoTesbtJiHSv/UFJGSyK+c9VP925/OCcCYDn8tvyIAY0hOAEaYq1tbPKQnFDyD4uLhwgVIhOYdfSohARIlUWRLeGw5/Pk9rHjV+vP64Tn4dZpCL6RYcWlxlZWVxfbt2xk9erRtm5ubG127dmXTpk0FHpOZmYmPj/3UI19fX9avX2+37emnn+bee++la9eu1y2uMjMzyczMtN1PSrLGRxuNRoxGo0PX5Gy5z+/qdhQ7noFQsbH1dqWsFLh0DMOlIxguWb/Hy/rvo5B4AkNWylUDNizu3hBSDUtoTSyh1vntltAaWMrVhOBq4O5Z9Nd2k26qz6ScxW3XXNx2zsaQFA+ABQOW2ndhbj4US1RH63QW6xM4qcXiavo9U0rV6GC9JRzHbcfnuO2aiyHpBKx+Dcuat7DU+xvmFkOxRLR0+E1pieozFguknsNw6UjO34PDef59BMN1AiQsPsF5/h5EYcmJM7eE1LCujbraa2cyW28COLHP1I6FqK64bZ+F2/r3MOSEXphrdsDUZQJUKmD9t5RIxen3jCNtMFgsFksRtuWaTp48SUREBBs3bqRNm8ujEy+++CJr165l8+bN+Y556KGH2L17N4sXLyY6OppVq1bRs2dPTCaTrUD6+uuveeONN9i6dSs+Pj507NiRmJiYq665Gj9+PBMmTMi3fd68efj5KTShNHEzG/HLOo9/5hn8M8/in3XG9m+/rHO4WUxXPdaCgTSvCqR6VyLVuyKpXjn/9a5EmncYJrcSOsXDYiE09SA1z68kImGL7TXI9AjkWPkOHC3fiXRvfcItUtK5mbMIT9hKzXMrKZd2yLY9wbcaRyt04URoW0zuJfX3mBlf40Xr7/Xc3++ZZ/DPOot/5lk8zBnXPDzDI/jy73bvSqR6VbTdN3po/VNx5Zmdym1nvqfmuRW4W7KxYOB4uTvZV+UBMrw0u0KcJy0tjYceeojExESCgoKuuW+JK67OnTvHsGHD+OGHHzAYDERHR9O1a1dmzpxJeno6cXFxtGjRghUrVtjWWl2vuCpo5CoyMpLz589f9wUsakajkRUrVtCtWzc8PYv/qEmJlpPuZB3lsoZqGHI+2eTiEQzZ6dc83BJQ2TrCFVITS7mcEa+cTzvxCb5FF+FAn8lKxfDHd7hvn4UhzyieOby59dPsen+zrq2SUk+/Z8qgU7us/+//8R2GbGvhYfEOwtxkAOZmQ6D8tQMwXNJnTFmQGIfhYs5MhJzZCYZLRyHhGIZrBEhYMEBw1ZxZCTVyRqFyZyhUt04XlyJVpH3m0lHc17yO297FAFg8/TC3fgpzm+H62ZZgxelvU1JSEhUqVChUceXSaYEVKlTA3d2dM2fO2G0/c+YMlStXLvCYsLAwFi9eTEZGBhcuXCA8PJxRo0YRFRUFwPbt2zl79izNmjWzHWMymfjll1/46KOPyMzMxN3dfg60t7c33t75P63z9PR0+Q8zV3FqS+nlCWHR1tuVbAEbBUTK5wRsGFJOY0g5DccLmNLqW+4q30tSs8gCNq7aZ84ftKaKFRhQ8Thu4U3RqrOySb9nypBqLa237q/bAjAMl47gvuVT3Ld8ag3AaDUMane/ZliA0/tMVloB35t4+PL3JjocIGH9ryEnQEIrclyvSH7PVKwNfWdD3Fb4aQyGuM24r38P951fKPSiFCgOf5sceX6X9jQvLy+aN2/OqlWr6NWrFwBms5lVq1YxfPjwax7r4+NDREQERqOR7777jr59+wLQpUsX9uyxX0szZMgQ6taty0svvZSvsBIpFLuAjbb5H0+7eDlC/spI3tSzeQI2tuU/1i5g44oCzFkBG7aAis+si9pzhdaElkOt3x2igAqRsud6ARjBkdB8sHMDMNITCg6PuHTEmgB7LXYBEld8WKUACVHohRQDLi/jR4wYwaBBg2jRogWtWrViypQppKam2tIDBw4cSEREBJMmTQJg8+bNxMfHExMTQ3x8POPHj8dsNvPiiy8CEBgYSMOG9osZ/f39KV++fL7tIk7jV856q9o8/2OZKfbJhrY3FUetn8ReI2ADd+9rR/leL2Aj5RzsmA3bZkHSiZyNBmscc8vHIbqz0hFFxPp7oHZX6+3SUevvjB1fWH9HrX4N1r4N9XtZf29EFvw9lDY5ARIFjvJfPGL9sOlafILtv/cp74dO1wqQEAFr/6jfE267x/qB4tq3ISf0gqhO1iKrciNXt1JKMZcXV/369ePcuXOMGzeO06dPExMTw7Jly6hUqRIAx48fxy3Pm7+MjAzGjh3L4cOHCQgIIDY2ljlz5hASEuKiKxC5Du8A6y/ygn6ZZ2fCpWP5R7wuHbFuN2XC+b+stysZ3K2f1OZ941EuCgIjKZeyH/fF/7N+emfOSbjxLQfNBkKLx3K+W0xEpAChNaDbBOg4Gv5YZH2DGr8N9iyw3io3wtDsMfwyTRiOroOk4/Zfi3HpiPVDo2sJqJS/cCqXs0ZVo+jiDB5e0OYpiBkAv7wHW/5rHY2d1s46W6PzGAgKd3UrpRRyaaBFcZWUlERwcHChFq0VNaPRyNKlS4mNjXX5fFO5xUzZ1tEmu09/84yAXSdgwyaihXXtRP1e4KmACslPv2fkuk7uhC2fwe/fQva1k/esDNYpheVq5B95D61h/dBJyhSX/565eARWTbB+YADWKaZtn4G2z6o/FlMu7zN5OFIbuHzkSkSuwt3D+iYktAZEd7J/zC5gw37Ey3LxMKbMdNwaPYjb7X+H8KauaL2IlCbhTaHXx9YpVbu+xLJ1BpaE4xhCa2CwTVfOMwqVEyAhUmyUqwl9Pofbn4KfxkLcZuuUwe2fQ6eXoemjWrMnTqHiSqQkukbARrbRyNIlS4i9917cNAohIs6UE4CR3fIJli75P2Lvvc/lnyiLOCSylTX0Yu//YOWr1jWGPzwHmz+Fbq9BrS5a1yc3RSvZRUoj/WEQkaJm0FsIKaEMBmjQC57eAt0ngU8InN0LXz4Ac3rD6QICpkQKSb8ZRURERKTs8fC2hl48twvaDLd+V1pu6MX/noak63w1gEgBVFyJiIiISNnlGwrd34DhW6FBb8ACO+fC1Gbw85vWr1QRKSQVVyIiIiIiuaEXQ1dA1VZgTLOGXkxtBttng9nk6hZKCaDiSkREREQkV2QrGPoT9JltTexNOQM/PAvT7oQDK13dOinmVFyJiIiIiORlF3rxZgGhF7+7uoVSTKm4EhEREREpiIc3tHkant15OfTi0GrrKJZCL6QAKq5ERERERK7Fr1xO6MUWqN8L+9CLSQq9EBsVVyIiIiIihVEuCvrOviL04i2FXoiNiisREREREUfYQi8+zx96cVChF2WZiisREREREUcZDNbvxboy9GKuQi/KMhVXIiIiIiI3Km/oxe1PK/SijFNxJSIiIiJys/zKwd1vKvSijFNxJSIiIiLiLLmhF4/9BFVb5gm9aA47vlDoRSmn4kpERERExNmqtbamCvb5HEKqQ8pp+P4ZhV6UciquRERERESKQm7oxfCtBYdenPnD1S0UJ1NxJSIiIiJSlK4ZejFcoReliIorEREREZFbwS70oidYzLBzjkIvShEVVyIiIiIit1K5KOj7hUIvSiEVVyIiIiIirnDV0It2Cr0ooVRciYiIiIi4St7Qi7veAJ9gOPtHTujF/Qq9KGFUXImIiIiIuJqHN7QdDs/uyhN6sUqhFyWMiisRERERkeIiN/Ti6c35Qy/WvAVZqa5uoVyDiisRERERkeKmfHT+0Is1k+DDZgq9KMZUXImIiIiIFFe5oRcPziog9GKVq1snV1BxJSIiIiJSnBkM0PD+AkIv7lfoRTGj4kpEREREpCSwC714Kn/oRfJpV7ewzFNxJSIiIiJSkviVg7snWUMv6v3tcujFh00VeuFiKq5EREREREqi8tHQbw48thwiWlwRejFHoRcuoOJKRERERKQkq3Y7PL7yitCL4Qq9cAEVVyIiIiIiJZ1d6MXrBYRe7HV1C8sEFVciIiIiIqWFhze0faaA0Is7rBHuCr0oUiquRERERERKm4JCL3Z8YV2PpdCLIqPiSkRERESktMoXepGq0IsipOJKRERERKS0s4VezISQapdDLz5tD4dWu7p1pYaKKxERERGRssBggIYPwPBtl0MvzvwOc3rD3AcUeuEEKq5ERERERMqSvKEXrZ8ENw84uDIn9OJZhV7cBBVXIiIiIiJlkV85uOcteHpLntCL2TmhF28r9OIGqLgSERERESnLCgy9eFOhFzdAxZWIiIiIiCj0wglUXImIiIiIiFXe0Itur4G3Qi8coeJKRERERETseXjDHc/Cc7uuEnpxxtUtLJZUXImIiIiISMHsQi965Am9aKrQiwKouBIRERERkWsrHw395sKQZRDR/HLoxdTmsHOuQi9yqLgSEREREZHCqd4GHl8FD8yA4GqQfAr+97RCL3KouBIRERERkcIzGKDRgzB8awGhFw+W6dALFVciIiIiIuI4T588oRdP5IRerCjToRcqrkRERERE5Mb5lYN73i449GLtO2Uq9ELFlYiIiIiI3Dxb6MWPEN7MGnrx8xtlKvRCxZWIiIiIiDhP9bZXCb3oAId+dnXripSKKxERERERcS43tzyhFxNzQi/2wJxe1tCLs3+6uoVFQsWViIiIiIgUDU8fuOM5eHanfejFJ23hh+dKXeiFiisRERERESla/uXzh15s/7zUhV6ouBIRERERkVvjmqEXX5b40AsVVyIiIiIicmsVGHrxVIkPvVBxJSIiIiIit941Qi/cv+5PYPoJV7fQYSquRERERETEdfKGXrT6B7h54HZoJZ32jYGLh13dOoeouBIREREREdfzLw+x78BTmzHXuZfTwc2gXJSrW+UQFVciIiIiIlJ8VKiF6cHZbK3xtKtb4jAVVyIiIiIiUuxY3Dxc3QSHqbgSERERERFxAhVXIiIiIiIiTqDiSkRERERExAlUXImIiIiIiDhBsSiuPv74Y2rUqIGPjw+tW7dmy5YtV93XaDQyceJEoqOj8fHxoUmTJixbtsxun08++YTGjRsTFBREUFAQbdq04ccffyzqyxARERERkTLM5cXV/PnzGTFiBK+++io7duygSZMmdO/enbNnzxa4/9ixY/n000+ZOnUqe/fu5YknnqB3797s3LnTtk/VqlV566232L59O9u2baNz58707NmTP/7441ZdloiIiIiIlDEuL64mT57MsGHDGDJkCPXr12fatGn4+fkxc+bMAvefM2cOL7/8MrGxsURFRfHkk08SGxvL+++/b9unR48exMbGUrt2bW677TbeeOMNAgIC+PXXX2/VZYmIiIiISBnj0vD4rKwstm/fzujRo23b3Nzc6Nq1K5s2bSrwmMzMTHx8fOy2+fr6sn79+gL3N5lMfPPNN6SmptKmTZurnjMzM9N2PykpCbBOQTQajQ5dk7PlPr+r2yElh/qMOEp9RhylPiOOUp8RRxWnPuNIG1xaXJ0/fx6TyUSlSpXstleqVIl9+/YVeEz37t2ZPHky7du3Jzo6mlWrVrFw4UJMJpPdfnv27KFNmzZkZGQQEBDAokWLqF+/foHnnDRpEhMmTMi3/aeffsLPz+8Gr865VqxY4eomSAmjPiOOUp8RR6nPiKPUZ8RRxaHPpKWlFXrfEve1xx988AHDhg2jbt26GAwGoqOjGTJkSL5phHXq1GHXrl0kJiby7bffMmjQINauXVtggTV69GhGjBhhu5+UlERkZCR33XUXQUFBRX5N12I0GlmxYgXdunXD09PTpW2RkkF9RhylPiOOUp8RR6nPiKOKU5/JndVWGC4tripUqIC7uztnzpyx237mzBkqV65c4DFhYWEsXryYjIwMLly4QHh4OKNGjSIqKspuPy8vL2rVqgVA8+bN2bp1Kx988AGffvppvnN6e3vj7e2db7unp6fLf5i5ilNbpGRQnxFHqc+Io9RnxFHqM+Ko4tBnHHl+lwZaeHl50bx5c1atWmXbZjabWbVq1VXXR+Xy8fEhIiKC7OxsvvvuO3r27HnN/c1ms926KhEREREREWdy+bTAESNGMGjQIFq0aEGrVq2YMmUKqampDBkyBICBAwcSERHBpEmTANi8eTPx8fHExMQQHx/P+PHjMZvNvPjii7Zzjh49mnvuuYdq1aqRnJzMvHnzWLNmDcuXL3fJNYqIiIiISOnn8uKqX79+nDt3jnHjxnH69GliYmJYtmyZLeTi+PHjuLldHmDLyMhg7NixHD58mICAAGJjY5kzZw4hISG2fc6ePcvAgQM5deoUwcHBNG7cmOXLl9OtW7dbfXn/3979x1RV/3Ecfx0w4IIXEvkhBGpOI6WB40dEaqWYQo2i0Sp3V+DanHVhOuZWuhJcbrnVijb1zpo//jCjdIOcExnRhOViIuwaJrpq/aABgtUUbpMcl+8frbvd6degbp1z6fnYznbv51wuL9j7D16cHxcAAADAf4Tp5UqSKioqVFFRcdN9J0+e9Hv+4IMP6vz587d8v7179wYqGgAAAABMiOkfIgwAAAAAUwHlCgAAAAACwBKnBVrN+Pi4pMnd0/6fcv36df3666+6evWq6behRHBgZjBZzAwmi5nBZDEzmCwrzcwfneCPjnArlKubGB4eliSlpqaanAQAAACAFQwPDysmJuaWrzHGJ1LB/mO8Xq/6+vpkt9tlGIapWa5evarU1FT19vYqOjra1CwIDswMJouZwWQxM5gsZgaTZaWZGR8f1/DwsJKTk/3uYn4zHLm6iZCQEKWkpJgdw090dLTpg4XgwsxgspgZTBYzg8liZjBZVpmZPzti9QduaAEAAAAAAUC5AgAAAIAAoFxZXHh4uKqrqxUeHm52FAQJZgaTxcxgspgZTBYzg8kK1pnhhhYAAAAAEAAcuQIAAACAAKBcAQAAAEAAUK4AAAAAIAAoVwAAAAAQAJQri9u1a5fmzp2riIgI5eXl6fTp02ZHgkW1tbWpuLhYycnJMgxDDQ0NZkeCxb3++uvKzc2V3W5XQkKCSkpKdPHiRbNjwcJcLpcyMjJ8H+qZn5+vxsZGs2MhSOzYsUOGYWjjxo1mR4GF1dTUyDAMv+3uu+82O9aEUa4s7MMPP1RVVZWqq6vV1dWlzMxMrV69WoODg2ZHgwV5PB5lZmZq165dZkdBkGhtbZXT6VR7e7uam5t1/fp1rVq1Sh6Px+xosKiUlBTt2LFDnZ2dOnPmjFasWKHHH39cX375pdnRYHEdHR3as2ePMjIyzI6CIJCenq7+/n7f9tlnn5kdacK4FbuF5eXlKTc3Vzt37pQkeb1epaamqrKyUi+//LLJ6WBlhmGovr5eJSUlZkdBEBkaGlJCQoJaW1v1wAMPmB0HQSI2NlZvvPGGnn/+ebOjwKJGRkaUlZWl3bt3a/v27Vq8eLFqa2vNjgWLqqmpUUNDg9xut9lR/hKOXFnUb7/9ps7OTq1cudK3FhISopUrV+rzzz83MRmAqerKlSuSfv9jGfgzY2Njqqurk8fjUX5+vtlxYGFOp1OPPvqo3980wK189dVXSk5O1rx58+RwOPTDDz+YHWnCppkdADd3+fJljY2NKTEx0W89MTFRFy5cMCkVgKnK6/Vq48aNWrJkie655x6z48DCuru7lZ+fr2vXrmn69Omqr6/XokWLzI4Fi6qrq1NXV5c6OjrMjoIgkZeXpwMHDigtLU39/f3atm2bli1bpnPnzslut5sd709RrgAAcjqdOnfuXFCd1w5zpKWlye1268qVKzpy5IjKysrU2tpKwcINent7tWHDBjU3NysiIsLsOAgSRUVFvscZGRnKy8vTnDlz9NFHHwXF6ceUK4uKi4tTaGioLl265Ld+6dIlzZo1y6RUAKaiiooKHTt2TG1tbUpJSTE7DiwuLCxM8+fPlyRlZ2ero6ND77zzjvbs2WNyMlhNZ2enBgcHlZWV5VsbGxtTW1ubdu7cqdHRUYWGhpqYEMHg9ttv11133aWvv/7a7CgTwjVXFhUWFqbs7Gy1tLT41rxer1paWji3HUBAjI+Pq6KiQvX19fr000915513mh0JQcjr9Wp0dNTsGLCggoICdXd3y+12+7acnBw5HA653W6KFSZkZGRE33zzjZKSksyOMiEcubKwqqoqlZWVKScnR/fee69qa2vl8Xi0du1as6PBgkZGRvz+q/Ptt9/K7XYrNjZWs2fPNjEZrMrpdOrQoUP6+OOPZbfbNTAwIEmKiYmRzWYzOR2saPPmzSoqKtLs2bM1PDysQ4cO6eTJk2pqajI7GizIbrffcA1nVFSUZs6cybWd+L82bdqk4uJizZkzR319faqurlZoaKjWrFljdrQJoVxZ2NNPP62hoSFt3bpVAwMDWrx4sU6cOHHDTS4ASTpz5oyWL1/ue15VVSVJKisr04EDB0xKBStzuVySpIceeshvff/+/SovL//3A8HyBgcH9dxzz6m/v18xMTHKyMhQU1OTHn74YbOjAZgifvzxR61Zs0Y//fST4uPjtXTpUrW3tys+Pt7saBPC51wBAAAAQABwzRUAAAAABADlCgAAAAACgHIFAAAAAAFAuQIAAACAAKBcAQAAAEAAUK4AAAAAIAAoVwAAAAAQAJQrAAAAAAgAyhUAAH+TYRhqaGgwOwYAwGSUKwBAUCsvL5dhGDdshYWFZkcDAPzHTDM7AAAAf1dhYaH279/vtxYeHm5SGgDAfxVHrgAAQS88PFyzZs3y22bMmCHp91P2XC6XioqKZLPZNG/ePB05csTv67u7u7VixQrZbDbNnDlT69at08jIiN9r9u3bp/T0dIWHhyspKUkVFRV++y9fvqwnnnhCkZGRWrBggY4ePerb98svv8jhcCg+Pl42m00LFiy4oQwCAIIf5QoAMOW9+uqrKi0t1dmzZ+VwOPTMM8+op6dHkuTxeLR69WrNmDFDHR0dOnz4sD755BO/8uRyueR0OrVu3Tp1d3fr6NGjmj9/vt/32LZtm5566il98cUXeuSRR+RwOPTzzz/7vv/58+fV2Nionp4euVwuxcXF/Xu/AADAv8IYHx8fNzsEAAB/VXl5uQ4ePKiIiAi/9S1btmjLli0yDEPr16+Xy+Xy7bvvvvuUlZWl3bt367333tNLL72k3t5eRUVFSZKOHz+u4uJi9fX1KTExUXfccYfWrl2r7du33zSDYRh65ZVX9Nprr0n6vbBNnz5djY2NKiws1GOPPaa4uDjt27fvH/otAACsgGuuAABBb/ny5X7lSZJiY2N9j/Pz8/325efny+12S5J6enqUmZnpK1aStGTJEnm9Xl28eFGGYaivr08FBQW3zJCRkeF7HBUVpejoaA0ODkqSXnjhBZWWlqqrq0urVq1SSUmJ7r///r/0swIArItyBQAIelFRUTecphcoNpttQq+77bbb/J4bhiGv1ytJKioq0vfff6/jx4+rublZBQUFcjqdevPNNwOeFwBgHq65AgBMee3t7Tc8X7hwoSRp4cKFOnv2rDwej2//qVOnFBISorS0NNntds2dO1ctLS1/K0N8fLzKysp08OBB1dbW6t133/1b7wcAsB6OXAEAgt7o6KgGBgb81qZNm+a7acThw4eVk5OjpUuX6v3339fp06e1d+9eSZLD4VB1dbXKyspUU1OjoaEhVVZW6tlnn1ViYqIkqaamRuvXr1dCQoKKioo0PDysU6dOqbKyckL5tm7dquzsbKWnp2t0dFTHjh3zlTsAwNRBuQIABL0TJ04oKSnJby0tLU0XLlyQ9Pud/Orq6vTiiy8qKSlJH3zwgRYtWiRJioyMVFNTkzZs2KDc3FxFRkaqtLRUb731lu+9ysrKdO3aNb399tvatGmT4uLi9OSTT044X1hYmDZv3qzvvvtONptNy5YtU11dXQB+cgCAlXC3QADAlGYYhurr61VSUmJ2FADAFMc1VwAAAAAQAJQrAAAAAAgArrkCAExpnP0OAPi3cOQKAAAAAAKAcgUAAAAAAUC5AgAAAIAAoFwBAAAAQABQrgAAAAAgAChXAAAAABAAlCsAAAAACADKFQAAAAAEwP8AxawI8p9StsUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(qat_history.history['loss'], label='Training Loss')\n",
    "plt.plot(qat_history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(qat_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(qat_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide a representative dataset for better quantization accuracy\n",
    "def representative_data_gen():\n",
    "    # Number of samples you want to use for calibration\n",
    "    num_samples = 300\n",
    "    count = 0\n",
    "    \n",
    "    for input_value, _ in train_generator_qat:\n",
    "        yield [input_value]\n",
    "        count += 1\n",
    "        if count >= num_samples:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnp0quxu_/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnp0quxu_/assets\n",
      "/home/sec_team2/anaconda3/envs/sony/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:947: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2024-11-15 20:46:53.302746: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-11-15 20:46:53.302764: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-11-15 20:46:53.302928: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpnp0quxu_\n",
      "2024-11-15 20:46:53.324921: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-11-15 20:46:53.324933: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpnp0quxu_\n",
      "2024-11-15 20:46:53.394512: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-11-15 20:46:53.847996: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpnp0quxu_\n",
      "2024-11-15 20:46:54.011445: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 708518 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\n",
    "\n",
    "# Enable full integer quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# Convert and save the model\n",
    "tflite_model = converter.convert()\n",
    "with open(\"fully_quantized_small_model_int8.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded validation set from: best_fold_4_validation_data.csv\n",
      "Validation samples: 394\n",
      "Found 394 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the saved validation set\n",
    "val_set_path = f'best_fold_{best_fold_idx + 1}_validation_data.csv'  # Adjust fold index if needed\n",
    "val_data_qat = pd.read_csv(val_set_path)\n",
    "print(f\"Loaded validation set from: {val_set_path}\")\n",
    "print(f\"Validation samples: {len(val_data_qat)}\")\n",
    "\n",
    "# Step 2: Create a validation generator\n",
    "img_width, img_height = 128, 128\n",
    "batch_size = 1\n",
    "\n",
    "# Define the test data generator (no augmentation)\n",
    "val_datagen_qat = CustomImageDataGenerator()  # No augmentations for validation\n",
    "\n",
    "val_generator_qat = val_datagen_qat.flow_from_dataframe(\n",
    "    dataframe=val_data_qat,\n",
    "    x_col='file_path',\n",
    "    y_col='label',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Ensure order matches for true labels and predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorFlow Lite model\n",
    "tflite_model_path = \"fully_quantized_small_model_int8.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate tensors\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Details with Quantization Parameters:\n",
      "  Name: serving_default_input_7:0\n",
      "  Shape: [  1 128 128   3]\n",
      "  Data Type: <class 'numpy.int8'>\n",
      "  Quantization Parameters: (0.007843137718737125, 0)\n",
      "  Quantization Scale: [0.00784314]\n",
      "  Quantization Zero Points: [0]\n",
      "\n",
      "\n",
      "Output Details with Quantization Parameters:\n",
      "  Name: StatefulPartitionedCall:0\n",
      "  Shape: [1 3]\n",
      "  Data Type: <class 'numpy.int8'>\n",
      "  Quantization Parameters: (0.00390625, -128)\n",
      "  Quantization Scale: [0.00390625]\n",
      "  Quantization Zero Points: [-128]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display input details including quantization parameters\n",
    "print(\"Input Details with Quantization Parameters:\")\n",
    "for input_detail in input_details:\n",
    "    print(f\"  Name: {input_detail['name']}\")\n",
    "    print(f\"  Shape: {input_detail['shape']}\")\n",
    "    print(f\"  Data Type: {input_detail['dtype']}\")\n",
    "    print(f\"  Quantization Parameters: {input_detail['quantization']}\")\n",
    "    print(f\"  Quantization Scale: {input_detail['quantization_parameters']['scales']}\")\n",
    "    print(f\"  Quantization Zero Points: {input_detail['quantization_parameters']['zero_points']}\")\n",
    "    print()\n",
    "\n",
    "# Display output details including quantization parameters\n",
    "print(\"\\nOutput Details with Quantization Parameters:\")\n",
    "for output_detail in output_details:\n",
    "    print(f\"  Name: {output_detail['name']}\")\n",
    "    print(f\"  Shape: {output_detail['shape']}\")\n",
    "    print(f\"  Data Type: {output_detail['dtype']}\")\n",
    "    print(f\"  Quantization Parameters: {output_detail['quantization']}\")\n",
    "    print(f\"  Quantization Scale: {output_detail['quantization_parameters']['scales']}\")\n",
    "    print(f\"  Quantization Zero Points: {output_detail['quantization_parameters']['zero_points']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9365\n",
      "Precision: 0.9375\n",
      "Recall: 0.9395\n",
      "F1-Score: 0.9384\n"
     ]
    }
   ],
   "source": [
    "# Function to run inference using TFLite model\n",
    "def run_inference_tflite(interpreter, input_data):\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output\n",
    "\n",
    "# Step 4: Collect predictions\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for i in range(len(val_generator_qat)):\n",
    "    X_batch, y_batch = val_generator_qat[i]\n",
    "    \n",
    "    # Convert to required input type (int8 if the model expects it)\n",
    "    input_data = X_batch.astype(np.float32)  # Change dtype if necessary\n",
    "\n",
    "    # If model expects int8 inputs\n",
    "    if input_details[0]['dtype'] == np.int8:\n",
    "        input_scale, input_zero_point = input_details[0]['quantization']\n",
    "        input_data = (input_data / input_scale + input_zero_point).astype(np.int8)\n",
    "    \n",
    "    # Run inference\n",
    "    predictions = run_inference_tflite(interpreter, input_data)\n",
    "    \n",
    "    # If output is quantized, dequantize it\n",
    "    if output_details[0]['dtype'] == np.int8:\n",
    "        output_scale, output_zero_point = output_details[0]['quantization']\n",
    "        predictions = (predictions.astype(np.float32) - output_zero_point) * output_scale\n",
    "    \n",
    "    y_pred.append(predictions)\n",
    "    y_true.append(y_batch)\n",
    "\n",
    "# Step 5: Evaluate Performance\n",
    "# Concatenate all batches\n",
    "y_pred = np.concatenate(y_pred, axis=0)\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "# Convert predictions and true labels to class indices\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_true, axis=1)\n",
    "\n",
    "# Compute Accuracy, Precision, Recall, and F1-Score\n",
    "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "precision = precision_score(y_true_classes, y_pred_classes, average='macro')\n",
    "recall = recall_score(y_true_classes, y_pred_classes, average='macro')\n",
    "f1 = f1_score(y_true_classes, y_pred_classes, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert to c header file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_c_array(bytes) -> str:\n",
    "    hexstr = binascii.hexlify(bytes).decode(\"UTF-8\")\n",
    "    hexstr = hexstr.upper()\n",
    "    array = [\"0x\" + hexstr[i:i + 2] for i in range(0, len(hexstr), 2)]\n",
    "    array = [array[i:i+10] for i in range(0, len(array), 10)]\n",
    "    return \",\\n  \".join([\", \".join(e) for e in array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_binary = open('fully_quantized_small_model_int8.tflite', 'rb').read()\n",
    "ascii_bytes = convert_to_c_array(tflite_binary)\n",
    "header_file = \"const unsigned char model_tflite[] = {\\n  \" + ascii_bytes + \"\\n};\\nunsigned int model_tflite_len = \" + str(len(tflite_binary)) + \";\"\n",
    "# print(c_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fully_quantized_small_model_int8.h\", \"w\") as f:\n",
    "    f.write(header_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sony",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
